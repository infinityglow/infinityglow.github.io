[{"title":"关于钱的两个经典问题","url":"/study/algorithm/dynamic-programming/classic-problems/","content":"\n## 动态规划\n\n在介绍**动态规划**（**dynamic programming**）前，咱们首先来清楚一个概念，重叠子问题（overlapping subproblem）。前面我们学到的分治法是将一个大问题分成若干个子问题，然后逐一解决子问题，然后合并子问题的解，从而得到原问题的解。这里的子问题之间都是相互独立的，不存在任何交集，就像归并排序一样，子序列 a 和子序列 b 的合并不会跟子序列 c 和子序列 d 的合并有任何重叠，反过来也一样。而动态规划就是专门为了解决这类子问题存在重叠的问题（子问题 1 包含了子问题 2 的解，子问题 2 又包含了子问题 3 的解）而设计的。你一定会觉得前面的描述太抽象了，我们来看一个简单的例子。\n\n斐波那契数列是数学中最常见的数列，它的每一项都等于前两项之和，其中第一项和第二项分别为 0 和 1。根据定义我们很快就能得到下面的式子：\n\n![relation](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/relation0.png)\n\n假设要计算 F(5)，我们就要先计算 F(4) 和 F(3)，而计算 F(4) 又要计算 F(3) 和 F(2)，F(3) 又是由 F(2) 和 F(1) 组成等等。善于观察的你一定发现了 F(3) 被计算了两次，从而造成了不必要的重复，影响了效率。\n\n![树](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/tree.png)\n\n上图中就很好的反映出了重叠子问题这一概念，其中打红圈的部分表示重复计算的部分，而真正需要计算的仅是从左下部分的 F(3) 到顶部的 F(5) 。动态规划的核心则是将那些本该需要重复计算的部分用一张表暂时存储起来，以便下一次计算的时候直接取用，而非再次计算。\n\n既然我们知道 F(1) = 0, F(2) = 1，所以我们便可以计算出 F(3) 的值，并保存在数组中，计算完 F(3) 后，F(4) 又可以根据已有的 F(2) 和 F(3) 被计算出来，这样以此类推，直到计算出 F(n)。\n\n![表](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/table0.png)\n\n用递归的方式求解斐波那契数列是非常低效的，时间复杂度达到了 Θ(2<sup>n</sup>)。而如果牺牲空间复杂度为 Θ(n) 来换取时间复杂度的话，我们可以将时间复杂度将降为 Θ(n)。\n\n## 硬币问题\n\n硬币问题是一道经典的动态规划问题，问题的描述为，在一行面值不同的硬币中选择金额最大的组合，同时保证两个相邻的硬币不能被同时选择。\n\n![硬币](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/coin0.png)\n\n例如上面的图中，我们可以这样选择：\n\n![硬币](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/coin1.png)\n\n但不能这样选择：\n\n![硬币](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/coin2.png)\n\n我们首先来讨论一下用暴力求解的方式。很简单，从 n 枚硬币中选择最大金额的问题 F(n) 可以分为两个子问题，取决于我们选不选第 n 枚硬币，如果不选，子问题变为从剩下 n - 1 枚硬币中选择最大金额 F(n-1)；如果选了，子问题变为从剩下 n - 2 枚硬币中选择最大金额并加上第 n 枚硬币的面值 F(n-2) + C<sub>n</sub> ，最后两者中取其较大者，所以 F(n) = max(F(n-1), F(n-2)+C<sub>n</sub>)。因此我们写出暴力求解的代码\n\n```\ndef coin_row_bf(coins, n):\n    if n == 0:\n        return 0\n    if n == 1:\n        return coins[n-1]  # 选择第 n 枚硬币\n    return max(coin_row_bf(coins, n-2), coin_row_bf(coins, n-1) + coins[n-1])\n```\n\n这样做会造成很多子问题被重复地求解，非常低效，时间复杂度达到了指数级 Θ(2<sup>n</sup>)。下面来看看动态规划是怎样来做的，我们首先从只有一枚硬币说起，毫无疑问，我们要选择它。\n\n![硬币](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/coin3.png)\n\n如果有两枚呢？我们就会用两种选择，保留第一枚，放弃第二枚，或者保留第二枚，放弃第一枚，在下面的例子中我们选择了第二枚，选择第二枚硬币后，第一枚就不能再选了，因此问题变为了从 0 枚硬币中选择最大的组合，显然没有硬币的时候最大组合为 0，故 F(0) = 0。\n\n![硬币](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/coin4.png)\n\n如果再加入第三枚面值为 50 的硬币，我们又怎么办呢？前面我们已经得到了 F(n) = max(F(n-1), F(n-2)+C<sub>n</sub>) 的关系，所以 F(3) = max(F(2), F(1)+50)。为了不重复计算，我们需要用一个长度为 n 的数组来记录前 i 枚硬币的最优解，这样我们便不用再次计算 F(2) 和 F(1) 的值了。因此，我们用一个循环把从 2 到 n 的所有最优解都求出来，最后返回 F(n)。\n\n![硬币](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/coin5.png)\n\n```\ndef coin_row_dp(coins, n):\n    optima = [-1 for i in range(n+1)]\n    optima[0] = 0; optima[1] = coins[0]\n    global combination\n    for i in range(2, n+1):\n        # max(F(n-2), F(n-1)+Cn)\n        if optima[i-1] < optima[i-2] + coins[i-1]:\n            optima[i] = optima[i-2] + coins[i-1]\n        else:\n            optima[i] = optima[i-1]\n    # 回溯\n    while n > 0:\n        if optima[n] > optima[n-1]:\n            combination[n-1] = 1\n            n -= 2\n        else:\n            n -= 1\n    return optima[-1]\n```\n\n最后得到其最优解为 130。\n\n![硬币](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/coin6.png)\n\n因为只用了一个循环，所以时间复杂度就为 Θ(n)。而由于用到了长度为 n 的数组，所以空间复杂度也为 Θ(n)。\n\n## 找零钱问题\n\n在生活中，找零钱的场景是再熟悉不过了，给定一些面值的钱币，需要凑成一定金额的零钱，怎样凑才能让用到的钱币数最少。\n\n![零钱](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/change.png)\n\n一般情况下，如果需要找的零钱金额为 m，所有钱币的面值为 {d<sub>1</sub>, d<sub>2</sub>, ..., d<sub>m</sub>}，原问题 F(n) 就可以分为多个子问题：{F(n - d<sub>i</sub>) | 1 ≤ i ≤ m ∧ n - d<sub>i</sub> ≥ 0}，每一个子问题又可以再分为几个子问题，直到 n = 0。每递归一次，所需钱币数就 +1，最后我们从子问题集 {F(n - d<sub>i</sub>) | 1 ≤ i ≤ m ∧ n - d<sub>i</sub> ≥ 0} 中寻找最小的组合。因此，F(n) = min({F(n - d<sub>i</sub>) | 1 ≤ i ≤ m ∧ n - d<sub>i</sub> ≥ 0})，所以我们可以写出如下递归关系式：\n\n![relation](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/relation1.png)\n\n用代码来实现：\n\n```\ndef change_making_bf(money, amount):\n    if amount == 0:\n        return 0\n    cnt = inf\n    for m in money:\n        if m <= amount:\n            cnt = min(cnt, change_making_bf(money, amount-m) + 1)\n    return cnt\n```\n\n这样做还是会造成很多重复性的求解，因此它的时间复杂度还是指数级的 Θ(m<sup>n</sup>)。\n\n来看看动态规划是怎么来做的吧。首先还是需要创建一个表来暂时存放最优解，然后我们计算从 1 到 n 的最优解。举个例子，假设 d = {1, 5, 10}，凑齐 0 元我们只需要 0 张钱币，所以 F(0) = 0，而凑齐 1 元我们只有 1 元的面值可用，剩下的面值都大于 1，所以当拿走 1 元后凑齐 1 元的问题就变为了凑齐 0 元的问题，而 F(0) 我们先前计算过，所以 F(1) = 1 + F(0)。后面的 2 到 n 也像前面的那样操作，只是越到后面，我们就越需要从 F(i - 1), F(i - 5), F(i - 10) 中做比较，然后选择最小值。 \n\n![表](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/images/table1.png)\n\n代码实现：\n\n```\ndef change_making_dp(money, amount):\n    optima = [0 for i in range(amount+1)]\n    global combination\n    for i in range(1, amount+1):\n        temp = inf\n        j = 0\n        # min({F(n-di)|1<=i<=m}) if n-di > 0\n        while j < len(money) and i >= money[j]:\n            temp = min(temp, optima[i-money[j]])\n            j += 1\n        optima[i] = temp + 1\n    # 回溯\n    while amount > 0:\n        temp = inf; idx = 0\n        for k in range(len(money)):\n            if amount-money[k] < 0:\n                break\n            if optima[amount-money[k]] < temp:\n                temp = optima[amount-money[k]]\n                idx = k\n        combination.append(money[idx])\n        amount -= money[idx]\n    return optima[-1]\n```\n\n因为`for`循环内每循环一次就需要执行 m 次的比较操作，来选择最小的 F(n - di)，所以用动态规划来解决找零钱问题的时间复杂度为 Θ(mn)，而空间复杂度为 Θ(n)。\n\n我们可以看到，动态规划能够大大提高算法的效率，当然这也离不开要以牺牲内存空间来作为代价。所以，动态规划可以看做“带有缓存的分治法”（来源于[知乎](https://www.zhihu.com/question/23995189)）。"},{"title":"哈希","url":"/study/algorithm/time-space-tradeoff/hashing/","content":"\n## 介绍\n\n这一节我们会介绍一种新的概念：**哈希**（**hashing**）。一提到它，你可能就想到了在比特币交易中的区块链技术，不管你是挖矿还是单纯的交易，都离不开哈希算法。事实上，哈希算法还广泛地运用在银行、通讯、商业等领域，目的只有一个，那就是加密，并且加密后几乎不能被破解。\n\n![比特币](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/hashing/images/bitcoin.jpg)\n\n除了加密，哈希还被运用在快速查找的领域，因为它的高效性，一些主流的编程语言都采用了这种方式来做数据之间的映射。比如 Python 中的字典类型`dict`，以及 Java 中的`HashMap`。它们都能够在很短的时间内从上百万条记录中找到我们所需要的内容。这么厉害～下面我们就来探究一下它的原理吧。\n\n## 哈希表\n\n我们知道，查找可以通过两种数据结构来实现：数组和链表。两者的主要区别在于数据是否存储在连续的地址空间中，但无论是哪一种，我们从外部访问的时候，都是访问的第一个元素或节点的地址，这就造成了如果需要查找一个值或记录，我们要从头开始，遍历整个数组或链表，直到找到该值或查找失败。因此无论是数组还是链表，查找的平均时间复杂度都为 Θ(n)。\n\n![数组 链表](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/hashing/images/array-linked-list.png)\n\n既然查找一个元素都需要先获取该数据结构头部的地址，那么我们可不可以直接获取一个元素对应的存储地址呢？答案肯定的，这里需要用到一种特殊的数据结构：哈希表。\n\n**哈希表**（**hash table**）也叫散列表。它是用来存储我们经过哈希操作后的数据，每一个数据都对应一个地址，供外部程序来访问。如果你想查找一个值`key`，我们需要用一个函数`H(key)`将`key`映射到一个特定的地址`address`上，然后比较该地址上的值来判断是否查找成功。\n\n![哈希表](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/hashing/images/hash-table.png)\n\n## 哈希函数\n\n要想实现`key`到`address`之间的映射关系，我们需要借助一个特殊的函数，**哈希函数**（**hash function**）。哈希函数多种多样，但它们的功能只有一个，那就是将不同的`key`均匀地分布在哈希表中，以达到便于查找的目的。最常见的，也是最简单的哈希函数就是取余操作，即`H(key) = key mod m`，其中`m`表示哈希表的长度。\n\n举个例子，假设哈希函数`H(key) = key mod 7`，当`key`为 32 时，计算出的结果等于 4，这里的 4 就代表由哈希函数计算出的哈希地址`address`，当然这里的地址不是计算机的物理地址，只是为了方便而举的例子而已。在哈希表中我们就在索引为 4 的位置记录 32。下次查找的时候，我们就可以通过哈希函数计算出的地址然后直接访问表里的值了。\n\n![哈希函数](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/hashing/images/hash-function0.png)\n\n但是，如果其他值`key'`通过哈希函数计算得到了相同的地址，这就会造成一种现象叫做**冲突**（**collision**），比如 25 mod 7 也等于 4，造成了 25 不能够直接存储在索引为 4 的位置。为了解决冲突的问题，我们需要采用一些机制来保证那些冲突的元素能够存储在正确的位置，下面就来分别介绍一下几种主要的处理冲突的方法。\n\n![哈希函数](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/hashing/images/hash-function1.png)\n\n## 冲突处理\n\n我们将用三种处理冲突的方法来构建哈希表。元素从数组`[58, 19, 75, 38, 29, 4, 60, 94, 84]`中获取，哈希表的长度为 13，哈希函数为`H(key) = key mod 13`。\n\n```\nclass HashTable(object):\n    def __init__(self, size):\n        self.size = size\n        self.table = [None for i in range(size)]\n    def function(self, key):\n        return key % self.size\n```\n\n### 链地址法\n\n**链地址法**（**separate chaining**）是将冲突的元素以链表的形式储存起来，具体来讲，具有相同的哈希地址的元素用节点的方式保存在链表的尾部。\n\n将上面的数组按照顺序放入到哈希表中：\n\nH(58) = 58 mod 13 = 6，索引为 6 的位置为空，所以节点 58 直接放入到哈希表中。\n\nH(19) = 19 mod 13 = 6，索引为 6 的位置不为空，所以需要将节点 19 连到节点 58 的后面。\n\nH(75) = 75 mod 13 = 10，索引为 10 的位置为空，所以节点 75 直接放入到哈希表中。\n\nH(38) = 38 mod 13 = 12，索引为 12 的位置为空，所以节点 38 直接放入到哈希表中。\n\nH(29) = 29 mod 13 = 3，索引为 3 的位置为空，所以节点 29 直接放入到哈希表中。\n\nH(4) = 4 mod 13 = 4，索引为 4 的位置为空，所以节点 4 直接放入到哈希表中。\n\nH(60) = 60 mod 13 = 8，索引为 8 的位置为空，所以节点 60 直接放入到哈希表中。\n\nH(94) = 94 mod 13 = 3，索引为 3 的位置不为空，所以需要将节点 94 连到节点 29 的后面。\n\nH(84) = 84 mod 13 = 6，索引为 6 的位置不为空，所以需要将节点 84 连到节点 19 的后面。\n\n![链地址法](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/hashing/images/separate-chaining.png)\n\n查找的时候，我们先计算查找元素的哈希值，然后从哈希表中找到对应的元素的值，如果值为空（None），那么就查找失败，如果不为空，那么就从链表里查找该元素。\n\n假设查找的值为 84，计算出哈希值为 6，由于索引 6 的位置不为空，所以我们从链表中查找，最后查找成功。如果查找元素 22，计算出哈希值为 9，发现索引 9 的位置为空，因此查找失败。\n\n代码实现：\n\n```\nclass Node(object):\n    def __init__(self, address):\n        self.address = address\n        self.next = None\n```\n\n```\ndef put(self, key):\n    address = self.function(key)\n    node = Node(key)\n    # 表中对应值为 None\n    if not self.table[address]:\n        self.table[address] = node\n    else:\n        p = self.table[address]\n        while p.next:\n            p = p.next\n        p.next = node\ndef get(self, key):\n    address = self.function(key)\n    p = self.table[address]\n    while p:\n        if p.address == key:\n            return True\n        p = p.next\n    return False\n```\n\n### 线性探测法\n\n**线性探测法**（**linear probing**）是将冲突元素放在产生冲突地址的下一个位置单元的方式来处理冲突，如再冲突，就再探测下一个位置单元是否冲突，直到不冲突为止（如果最后一个索引值是冲突，那么就探测第一个索引值是否冲突）。比如，索引 6 的位置发生了冲突，那么就探测索引 7 是否也冲突，如果不冲突就把元素存储在索引 7 的位置上；如果继续冲突就探测索引 8，以此类推，直到不冲突为止。\n\n将上面的数组按照顺序放入到哈希表中：\n\nH(58) = 58 mod 13 = 6，索引为 6 的位置为空，所以元素 58 直接放入到哈希表中。\n\nH(19) = 19 mod 13 = 6，索引为 6 的位置不为空，探测索引 7，发现位置为空，所以元素 19 就放入到索引 7 的位置。\n\nH(75) = 75 mod 13 = 10，索引为 10 的位置为空，所以元素 75 直接放入到哈希表中。\n\nH(38) = 38 mod 13 = 12，索引为 12 的位置为空，所以元素 38 直接放入到哈希表中。\n\nH(29) = 29 mod 13 = 3，索引为 3 的位置为空，所以元素 29 直接放入到哈希表中。\n\nH(4) = 4 mod 13 = 4，索引为 4 的位置为空，所以元素 4 直接放入到哈希表中。\n\nH(60) = 60 mod 13 = 8，索引为 8 的位置为空，所以元素 60 直接放入到哈希表中。\n\nH(94) = 94 mod 13 = 3，索引为 3 的位置不为空，探测索引 4，发现还是不为空，继续探测索引 5，发现位置为空，所以元素 94 就放入到索引 5 的位置。\n\nH(84) = 84 mod 13 = 6，索引为 6 的位置不为空，探测索引 7，发现还是不为空，继续探测索引 8，也不为空，继续探测索引 9，发现位置为空，所以元素 84 就放入到索引 9 的位置。\n\n![线性探测法](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/hashing/images/linear-probing.png)\n\n查找跟链地址法相似，也是先计算哈希值，找到对应元素，如果值相等，就查找成功；如果不相等，就按照线性探测的方式看下一个索引值对应的元素的值是否相等，这样一直探测下去，直到探测到的值为 None，即查找失败。\n\n假设还是查找 84 和 22，查找 84 时，先计算出哈希值为 6，而索引 6 对应元素的值为 58，不等于 84，所以继续探测下一个索引值，发现还是不相等，直到最后我们在索引 9 的位置探测到 84 = 84，故查找成功。如果查找值为 22，其哈希值计算得到为 9，发现 22 不等于对应元素的值 89，所以要继续探测，探测两轮后发现索引 11 为空，所以查找失败。\n\n代码实现：\n\n```\ndef put(self, key):\n    address = self.function(key)\n    # 表中对应值为 None\n    while self.table[address]:\n        address = (address + 1) % self.size\n    self.table[address] = key\ndef get(self, key):\n    address = self.function(key)\n    while self.table[address] is not None:\n        if self.table[address] == key:\n            return True\n        address = (address + 1) % self.size\n    return False\n```\n\n### 再哈希法\n\n**再哈希法**（**rehashing**）是通过用另外一个哈希函数的方式来处理冲突。具体来讲，我们将原哈希值`l = H(key)`加上另外一个哈希函数的值`s(key)`，再把相加后的结果放到原哈希函数里计算哈希值`H(l + s(key))`，然后看是否冲突，如不冲突就把元素存储到对应索引值的下方；如果还是冲突，就计算`H(l + 2s(key))`的值然后再判断，这样以此类推，直到不冲突为止。\n\n还是来看具体的例子，这里`s(key) = m - 2 - key mod (m - 2) = 11 - key mod 11`。\n\nH(58) = 58 mod 13 = 6，索引为 6 的位置为空，所以元素 58 直接放入到哈希表中。\n\nH(19) = 19 mod 13 = 6，索引为 6 的位置不为空，再哈希后 H(l + s(key)) = 9，发现位置为空，所以元素 19 就放入到索引 9 的位置。\n\nH(75) = 75 mod 13 = 10，索引为 10 的位置为空，所以元素 75 直接放入到哈希表中。\n\nH(38) = 38 mod 13 = 12，索引为 12 的位置为空，所以元素 38 直接放入到哈希表中。\n\nH(29) = 29 mod 13 = 3，索引为 3 的位置为空，所以元素 29 直接放入到哈希表中。\n\nH(4) = 4 mod 13 = 4，索引为 4 的位置为空，所以元素 4 直接放入到哈希表中。\n\nH(60) = 60 mod 13 = 8，索引为 8 的位置为空，所以元素 60 直接放入到哈希表中。\n\nH(94) = 94 mod 13 = 3，索引为 3 的位置不为空，再哈希后 H(l + s(key)) = 8，发现还是不为空，继续再哈希， H(l + 2s(key)) = 0，发现位置为空，所以元素 94 就放入到索引 0 的位置。\n\nH(84) = 84 mod 13 = 6，索引为 6 的位置不为空，再哈希后 H(l + s(key)) = 10，发现还是不为空，继续再哈希，H(l + 2s(key)) = 1，发现位置为空，所以元素 84 就放入到索引 1 的位置。\n\n![再哈希法](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/hashing/images/double-hashing.png)\n\n查找也跟线性探测法相似，先计算哈希值，如果对应元素的值相等就查找成功，否则就再哈希，直到对应元素的值为 None，即查找失败。例如要查找 22，计算得到哈希值为 9，发现 22 不等于对应元素的值 89，于是我们再哈希，得到的哈希值为 7，对应值为 None，所以查找失败。\n\n```\ndef rehash(self, key):\n    m = self.size\n    return m - 2 - key % (m - 2)\ndef put(self, key):\n    address = self.function(key)\n    # 表中对应值为 None\n    while self.table[address]:\n        address = self.function(address+self.rehash(key))\n    self.table[address] = key\ndef get(self, key):\n    value = self.function(key)\n    while self.table[value] is not None:\n        if self.table[value] == key:\n            return True\n        value = self.function(value+self.rehash(key))\n    return False\n```\n\n## 复杂度分析\n\n由于哈希表能够直接访问查找元素的地址，所以它的时间复杂度为常数的复杂度 Θ(1)。而每一个`key`到`address`的映射关系需要记录下来，假设哈希表有 n 个元素，那么就需要 n 条记录，故空间复杂度为 Θ(n)。\n\n至于处理冲突的三种方式，它们各自都有优缺点，下面来分别列举一下：\n\n链地址法\n\n- 优点：处理冲突的方式简单，查找效率高\n- 缺点：内存空间占用较大（需要存放下一个节点的指针）\n\n线性探测法\n\n- 优点：处理冲突的方式简单，不需要额外计算\n- 缺点：容易成簇聚集，造成查找效率降低\n\n再哈希法\n\n- 优点：再哈希后的值分散不聚集\n- 缺点：需要做额外计算，如选择不好再哈希的函数，可能会造成一直冲突。\n\n为了客观地反应这三者的查找效率，我在这节的[代码](https://github.com/infinityglow/Algorithm-and-Complexity/tree/master/Time-Space%20Tradeoff/Hashing)中分别模拟了查找成功和查找失败的情况下的平均查找次数，最后的结论是：链地址法 > 再哈希法 > 线性探测法。当然，这个结果只是作为参考，真实的情况比这个要复杂得多，所以三种处理冲突的方法都是有它的用武之地的。\n\n```\n>>> python \"separate chaining.py\"\n>>> 平均查找次数（查找成功）= 1.44\n    平均查找次数（查找失败）= 1.62\n\n>>> python \"linear probing.py\"\n>>> 平均查找次数（查找成功）= 1.67\n    平均查找次数（查找失败）= 3.77\n\n>>> python \"double hashing.py\"\n>>> 平均查找次数（查找成功）= 1.56\n    平均查找次数（查找失败）= 2.77\n```"},{"title":"字符串匹配（TST）","url":"/study/algorithm/time-space-tradeoff/string-matching/","content":"\n## 回顾\n\n这一节我们再来讨论一下字符串匹配问题。在前面的学习中我们已经知道了字符串匹配就是在文本字符串中找到模式字符串，以及它第一次出现的位置。如使用暴力求解的方法，我们将文本字符串和模式字符串对齐，然后从左往右依次比对，如果全部比对成功就返回模式字符串出现的位置；如果失败就向右移动一个单位，然后再进行比较，如果超出文本字符串的右边界就返回匹配失败。这种方式在最坏的情况下复杂度为 Θ(mn)，其中 m 和 n 分别代表模式字符串和文本字符串的长度，而在一般情况下则是线性的复杂度 Θ(n)。那么有没有更高效的算法能够替代它呢？下面我们就来一探究竟吧。\n\n## 一种猜想\n\n暴力求解虽然能够实现这一过程，但是每次比对失败后模式字符串只向右移动了一个单位，并造成了一些重复的比对。我们可以尝试从右往左进行比对，如果比较失败，我们就移动整个模式字符串的长度。听起来这将会大大缩短比较的次数，可是事实上真是如此吗？我们还是以 DNA 配对来作为我们的例子。\n\n首先，我们需要将 DNA 链和探针的第一个碱基对齐，然后从右往左依次比对，最后发现比对失败。\n\n![DNA](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/string-matching/images/DNA-0.png)\n\n于是我们将探针向右移动跟它长度一样的单位，在这里就是移动 6 个单位。\n\n![DNA](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/string-matching/images/DNA-1.png)\n\n移动后发现，我们已经错过了 DNA 链和探针的匹配的位置。因此，直接移动一整个探针长度的方法并不可行。\n\n## Horspool 算法\n\n既然移动整个探针长度可能会直接错过与 DNA 链的匹配，那么我们可以根据探针的最后一个碱基匹配到的不同碱基，来判断我们的探针到底要往右移动多少个单位。\n\n举个例子，下图中探针的最后一个碱基匹配到的为 C，如果前面的碱基匹配失败，由于探针中没有与 C 匹配的碱基，所以我们移动整个探针的长度是安全的，因此在这种情况下我们的探针需要向右移动 6 个单位。\n\n![DNA](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/string-matching/images/DNA-2.png)\n\n如果探针中存在与之匹配的碱基，那么在移动的过程中，我们要使探针的与之相匹配的最右侧碱基与之前探针的最后一个碱基所对应的碱基对齐。听起来很拗口，让我们来举一个具体的例子，假设探针的最后一个碱基匹配到的为 A，由于探针中存在两个与 A 配对的 T，根据规则，我们需要将最右侧的 T 与 A 对齐，因此探针需要向右移动 3 个单位。\n\n![DNA](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/string-matching/images/DNA-3.png)\n\n同理，如果匹配到的为 T，探针中存在两个 A，取其最右者，因而这种情况下探针需要向右移动 1 个单位。\n\n![DNA](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/string-matching/images/DNA-4.png)\n\n如果匹配到的为 G，则需要向右移动 5 个单位。\n\n![DNA](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/string-matching/images/DNA-5.png)\n\n这样我们就把所有的情况都讨论完了，为了表示方便，我们可以用一个表`shift table`来表示在不同的匹配情况下探针需要向右移动几个单位。\n\n![table](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/string-matching/images/table.png)\n\n用代码来表示这个过程就是这个样子的：\n\n```\ndef shift_table(p, m):\n    table = {}\n    for i in range(m):\n        table[p[i]] = m\n    for j in range(m-1):\n        table[p[j]] = m-1-j\n    return table\n```\n\nHorspool 算法的核心思想就是构建一张查询表，然后从右往左进行比对，如果比对失败，就根据模式字符串的最后一个字符所对应的字符，查询前面的`shift table`，决定需要移动的单位长度。移动后再从右往左依次比对，直到全部比对成功，或超出右边界，匹配失败。\n\n```\ndef Horspool(text, pattern):\n    n = len(text); m = len(pattern)\n    table = shift_table(pattern, m)\n    i = m - 1\n    while i < n:\n        k = 0\n        while k < m and pattern[m-1-k] == text[i-k]:\n            k += 1\n        if k == m:\n            return i - m + 1\n        else:\n            i += table[text[i]]\n    return -1\n```\n\n我们用一张图来完整地表示一下上述过程。\n\n![DNA](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/string-matching/images/DNA-complete.png)\n\n## 复杂度分析\n\n因为每次比对失败后模式字符串会移动大于等于 1 个单位，所以平均情况下会优于暴力求解算法，达到线性的复杂度 Θ(n)。但是 Horspool 算法在有些极端的情况下甚至比暴力求解还要糟糕。举个最简单的例子，假设文本字符串为“000000000000”，模式字符串为“1000”。由于从左往右和从右往左的区别，加上`shift table`针对 ‘0’ 只移动 1 个单位，所以暴力求解只需要比较 9 次，而 Horspool 算法则需要比较 36 次之多，因此最坏情况下 Horspool 算法的时间复杂度也会达到 Θ(mn)。由于需要额外空间来存储查询表，所以空间复杂度为 Θ(k)，其中 k 代表查询表的长度。"},{"title":"计数排序","url":"/study/algorithm/time-space-tradeoff/counting-sort/","content":"\n## 时空权衡\n\n在前面的章节里，我们知道归并排序是一种高效的排序算法，在任何情况下时间复杂度都为 Θ(nlog n)。但是，它需要用额外的内存空间来暂时储存归并过程中的元素，因此我们可以说归并排序是以牺牲一部分内存空间来换取时间上的高效性。相反，如果存储空间有限，我们就必须以牺牲时间为代价来保证空间不被过度使用。像这样在设计一个算法的过程中同时考虑时间复杂度和空间复杂度，并且在这两者中找到一个平衡点的过程我们把它称作**时空权衡**（**time and space trade-off**）。\n\n![时空权衡](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/counting-sort/images/seesaw.png)\n\n在通常情况下，我们都认为时间更宝贵，而空间相对廉价。因此在大多数情况下，我们都是以牺牲空间的方式来提升时间效率。\n\n## 计数排序\n\n**计数排序**（**counting sort**）就是一种牺牲内存空间来换取低时间复杂度的排序算法，同时它也是一种不基于比较的算法。所谓的不基于比较就是指数组元素之间不存在比较大小的情况，我们知道，用分治法最多也只能使算法的时间复杂度接近 Θ(nlog n)，即基于比较的时间复杂度存在下界 Θ(nlog n)。\n\n![下界](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/counting-sort/images/bound.png)\n\n我们用一种通俗方式来理解计数排序的过程。如下图所示，假设我们用不同大小的小球来表示每一个数组元素的值，我们的目标是让小球从小到大以次排列。\n\n![计数排序](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/counting-sort/images/counting0.png)\n\n首先我们需要知道最大的球和最小的球分别对应的元素，这里最大的对应 8，最小的对应 2。\n\n![计数排序](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/counting-sort/images/counting1.png)\n\n然后我们需要用 8 - 2 + 1 = 7 个计数器来分别统计 2 到 8 之间每种小球的个数（假设 2 到 8 之间的小球对应的元素都为整数），下图中我们用带有标记的桶来表示。\n\n![计数排序](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/counting-sort/images/counting2.png)\n\n接下来，我们遍历所有的小球，将每个值为 i 的小球放入到第 (i - 2 + 1) 个桶中，比方说第一个小球的值为 5，那么我们就需要把它放到标号为 5，也就是从左往右第 4 个桶中。\n\n![计数排序](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/counting-sort/images/counting3.png)\n\n最后，我们从左往右依次将每个桶里的小球取出，按顺序排列，这样小球就是从小到大排列了。\n\n![计数排序](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/counting-sort/images/counting4.png)\n\n我们用一个动画来完整地看一看这个过程吧。\n\n![demo](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/counting-sort/images/counting.mp4)\n\n```\ndef counting_sort(array):\n    largest = max(array); smallest = min(array)  # 获取最大，最小值\n    counter = [0 for i in range(largest-smallest+1)]  # 用于统计个数的空数组\n    idx = 0\n    for i in range(len(array)):\n        counter[array[i]-smallest] += 1\n    for j in range(len(counter)):\n        while counter[j] > 0:\n            array[idx] = j + smallest\n            idx += 1\n            counter[j] -= 1\n    return array\n```\n\n整个过程需要遍历两次数组，一次是遍历长度为 n 的数组，另一次是从计数器（假设有 k 个计数器）中遍历，因此时间复杂度为 Θ(n+k)。而在计数排序的过程中用到了长度为 k 的额外数组，故空间复杂度为 Θ(k)。\n\n我们可以看到计数排序突破了基于比较的排序算法效率的下界，达到了线性的复杂度，是到目前为止我们所接触的最快的算法。我们还是可以通过运行时间来间接反应这一结论，大家可以将下面的结果和前面复杂度为 Θ(nlog n) 的排序算法进行对比。\n\n```\n>>> python \"counting sort.py\"\n>>> 平均比较次数：0\n    平均运行时间：0.0172 s\n```"},{"title":"红黑树","url":"/study/algorithm/transform-and-conquer/R&B-tree/","content":"\n## 介绍\n\n上一节的内容中，我们讨论了 AVL 这种自平衡的树，不知道各位小伙伴有没有真正理解这种神奇的操作呢？哈哈，在这一节中，你将会遇到更神奇，同时也更复杂的算法来帮助我们平衡二叉树。下面就让我们的红黑树浓重登场吧！\n\n![红黑树](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/demo.png)\n\n**红黑树**（**red-black tree**）也是自平衡的二叉树，在介绍它是如何做到自平衡之前，我们首先来了解一下它的性质。\n\n- 性质 1：所有节点必须为红色或黑色\n- 性质 2：根节点为黑色\n- 性质 3：所有叶子节点（NIL）为黑色\n- 性质 4：如果节点为红色，那么其孩子节点（包括 NIL）必须为黑色\n- 性质 5：任意一个节点到每个叶子节点所经过的路径包含相同的数目的黑色节点\n\n红黑树每插入或删除一个节点，都会检查是否违反了上述的性质。如违反，红黑树就做出相应的改变，但和 AVL 树不同的是，红黑树不但继承了 AVL 树的旋转操作，还可以通过节点的颜色变换使树达到平衡，下面我们就分别来介绍一下这两种操作。\n\n## 旋转操作\n\n红黑树的旋转操作与 AVL 树的旋转操作是基本上一样的，唯一的不同是每一个节点都对应自己的父节点，因此涉及指针的操作会有所不同。\n\n因此下面我们定义出 `Node` 类，其中包含了获取祖父节点，叔父节点和兄弟节点的操作。\n\n```\nclass Node(object):\n    def __init__(self, value):\n        self.value = value\n        self.color = 'r'\n        self.left = None\n        self.right = None\n        self.parent = None\n    # 获取祖父节点\n    def grandparent(self):\n        if not self.parent:\n            return None\n        return self.parent.parent\n    # 获取叔父节点\n    def uncle(self):\n        if not self.grandparent():\n            return None\n        if self.parent is self.grandparent().left:\n            return self.grandparent().right\n        else:\n            return self.grandparent().left\n    # 获取兄弟节点\n    def brother(self):\n        if self.parent.left is self:\n            return self.parent.right\n        else:\n            return self.parent.left\n```\n\n### 左旋\n\n如下图所示，c 代表旋转节点，g 代表祖父节点，p 代表父节点，l 和 r 分别代表 c 的左孩子和右孩子。经过一次左旋后，原来的 c 节点成为了 g 的右孩子，原来的 p 节点成为了 c 节点的左孩子，并且成为 c 节点的左孩子的父节点。\n\n![左旋](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/left-rotation.png)\n\n```\ndef left_rotation(self, node):\n    # node 为根节点\n    if not node.parent:\n        self.root = node\n        return\n    grandparent = node.grandparent()\n    parent = node.parent\n    t = node.left\n    parent.right = t\n\n    # 判断 t 是否为叶子节点\n    if t is not self.NIL:\n        t.parent = parent\n    node.left = parent\n    parent.parent = node\n\n    # 判断父节点是否为根节点\n    if parent is self.root:\n        self.root = node\n    node.parent = grandparent\n\n    # 判断祖父节点是否为空\n    if grandparent:\n        # 父节点为祖父节点的左孩子\n        if grandparent.left is parent:\n            grandparent.left = node\n        # 父节点为祖父节点的右孩子\n        else:\n            grandparent.right = node\n```\n\n### 右旋\n\n同理，右旋的操作跟左旋几乎一模一样，只是与左旋互为镜像。下图展示了右旋的全过程。\n\n![右旋](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/right-rotation.png)\n\n```\ndef right_rotation(self, node):\n    # node 为根节点\n    if not node.parent:\n        self.root = node\n        return\n    grandparent = node.grandparent()\n    parent = node.parent\n    t = node.right\n    parent.left = t\n\n    # 判断 t 是否为叶子节点\n    if t is not self.NIL:\n        t.parent = parent\n    node.right = parent\n    parent.parent = node\n\n    # 判断父节点是否为根节点\n    if parent is self.root:\n        self.root = node\n    node.parent = grandparent\n\n    # 判断祖父节点是否为空\n    if grandparent:\n        # 父节点为祖父节点的左孩子\n        if grandparent.left is parent:\n            grandparent.left = node\n        # 父节点为祖父节点的右孩子\n        else:\n            grandparent.right = node\n```\n\n和 AVL 树一样，旋转操作只涉及了指针操作，因此不管是左旋还是右旋都是 Θ(1) 的复杂度。\n\n## 颜色变换\n\n当红黑树插入或删除一个节点时，免不了会违反前面 5 种性质中的一条或多条，在这种情况下，我们就不需要每次都像 AVL 树那样进行麻烦的旋转操作了，而是采取颜色变换的方式，以此来减小因旋转而带来的时间开销。\n\n例如，下图中违反了性质 4，我们只需要调整其中一些节点的颜色就能再次满足该性质。\n\n![颜色变换](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/color-rearrange0.png)\n\n再比如，下图中违反了性质 5，同样也只需调整节点的颜色就能保证再次满足该性质。\n\n![颜色变换](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/color-rearrange1.png)\n\n颜色变换不涉及任何的运算，因此时间复杂度为 Θ(1)。\n\n## 搜索\n\n同二叉搜索树一样，红黑树的搜索也是从根节点出发，比较搜索元素与根节点值的大小，然后决定是直接返回该节点，还是从左(右)子树中继续搜索。如果一直搜索到 NIL 节点都还是搜索失败，那么就返回`None`。\n\n```\ndef search(self, value):\n    # 树的根节点为空就抛出异常\n    if not self.root:\n        raise ValueError(\"The tree is null\")\n    return self.search_node(self.root, value)\ndef search_node(self, root, value):\n    if root is self.NIL:  # 如果是叶子节点 NIL，则返回 None\n        return None\n    if value < root.value:  # 从左子树中查找\n        return self.search_node(root.left, value)\n    elif value > root.value:  # 从右子树中查找\n        return self.search_node(root.right, value)\n    else:\n        return root\n```\n\n由于红黑树是一棵近似的平衡二叉树，故搜索的时间复杂度为 Θ(log n)。\n\n## 插入\n\n插入操作的前序步骤也和二叉搜索树一样，先找到插入节点的位置，创建一个新的节点，最后使该节点指向其父节点，父节点指向新插入的节点。\n\n```\ndef insert(self, value):\n    # 每个新节点均有两个叶子节点 NIL\n    new_node = Node(value)\n    new_node.left = self.NIL\n    new_node.right = self.NIL\n    if not self.root:\n        self.root = new_node\n        new_node.color = 'b'  # 根节点的颜色置为黑色\n    else:\n        self.insert_node(self.root, new_node)\ndef insert_node(self, root, node):\n    # 搜索\n    if node.value < root.value:\n        if root.left is not self.NIL:\n            self.insert_node(root.left, node)\n        else:\n            root.left = node\n            node.parent = root\n            self.insert_case(node)\n    else:\n        if root.right is not self.NIL:\n            self.insert_node(root.right, node)\n        else:\n            root.right = node\n            node.parent = root\n            self.insert_case(node)\n```\n\n完成插入后，我们需要判断新的红黑树有没有违反上面的 5 种性质，如果违反了，就根据不同的情况，做出相应的旋转或变色操作。下面我们就分情况来讨论一下。\n\n- 情况 1：根节点为红色。由于违反了性质 2，所以当根节点的颜色为红色时，我们需要将颜色调整为黑色。\n\n![情况 1](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/insert-case1.png)\n\n- 情况 2：父节点 P 和叔父节点 U 都为红色，祖父节点 G 为黑色，因为新加入的节点 N 为红色，所以违反了性质 4。因此做出的调整是交换父节点和叔父节点的颜色。因为我们不能保证祖父节点的父节点的颜色是否也为红色，所以我们需要在祖父节点 G 上递归地对情况 2 的进行检查。\n\n![情况 2](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/insert-case2.png)\n\n- 情况 3：父节点 P 为红色，叔父节点 U 为黑色或 NIL，且新插入的节点 N 与父节点和祖父节点在同一侧（G 的左(右)孩子为 P，P 的左(右)孩子为 N）。故我们再把情况 3 分为两种情况：\n\n    i) 祖父节点 G 的左孩子为父节点 P，父节点 P 的左孩子为新插入节点 N。此时，我们需要先交换父节点 P 和祖父节点 G 的颜色，然后对 P 进行右旋。\n    \n    ![情况 3A](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/insert-case3A.png)\n\n    ii) 同理，如果祖父节点 G 的右孩子为父节点 P，父节点 P 的右孩子为新插入节点 N。我们还是要先交换父节点 P 和祖父节点 G 的颜色，然后对 P 进行左旋操作。\n    \n    ![情况 3B](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/insert-case3B.png)\n\n- 情况 4：父节点 P 为红色，叔父节点 U 为黑色或 NIL，且新插入的节点 N 与父节点和祖父节点不在同一侧（G 的左(右)孩子为 P，P 的右(左)孩子为 N）。故我们还是把情况 4 分为两种情况：\n\n    i) 祖父节点 G 的左孩子为父节点 P，父节点 P 的右孩子为新插入节点 N。此时，我们先要对新插入节点 N 执行一次左旋操作，这样就变换为了跟情况 3A 一样的情形，接下来我们只需要再调用情况 3A 即可，只是旋转的节点从父节点 P 变为了新插入节点 N。\n    \n    ![情况 4A](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/insert-case4A.png)\n\n    ii) 同样，如果祖父节点 G 的右孩子为父节点 P，父节点 P 的左孩子为新插入节点 N。我们还是先要对新插入节点 N 执行一次旋转操作，只是这里变为了互为镜像的右旋。旋转后就变为了情况 3B，因此接下来我们只需要再调用一次 3B 即可，只是旋转的节点从父节点 P 变为了新插入节点 N。\n    \n    ![情况 4B](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/insert-case4B.png)\n\n我们把所有的情况用一个函数`insert_case`来表示。\n\n```\ndef insert_case(self, node):\n    # 情况 1: 根节点为红色\n    if not node.parent:\n        self.root = node\n        node.color = 'b'\n        return\n    # 情况 2: 父节点和叔父节点为红色，祖父节点为黑色。\n    if node.parent.color == 'r':\n        if node.uncle().color == 'r':\n            node.parent.color = 'b'; node.uncle().color = 'b'\n            node.grandparent().color = 'r'\n            self.insert_case(node.grandparent())  # 对祖父节点递归地调用 insert_case()\n        else:\n            # 情况 3A: 叔父节点为黑色或 NIL，祖父节点的左孩子为父节点，父节点的左孩子为新插入节点\n            if node.parent.left is node and node.parent is node.grandparent().left:\n                node.parent.color = 'b'\n                node.grandparent().color = 'r'\n                self.right_rotation(node.parent)\n            # 情况 3B: 叔父节点为黑色或 NIL，祖父节点的右孩子为父节点，父节点的右孩子为新插入节点\n            elif node.parent.right is node and node.parent is node.grandparent().right:\n                node.parent.color = 'b'\n                node.grandparent().color = 'r'\n                self.left_rotation(node.parent)\n            # 情况 4A: 叔父节点为黑色或 NIL，祖父节点的左孩子为父节点，父节点的右孩子为新插入节点\n            elif node.parent.right is node and node.parent is node.grandparent().left:\n                node.color = 'b'\n                node.grandparent().color = 'r'\n                self.left_rotation(node)\n                self.right_rotation(node)\n            # 情况 4B: 叔父节点为黑色或 NIL，祖父节点的右孩子为父节点，父节点的左孩子为新插入节点\n            elif node.parent.left is node and node.parent is node.grandparent().right:\n                node.color = 'b'\n                node.grandparent().color = 'r'\n                self.right_rotation(node)\n                self.left_rotation(node)\n```\n\n插入只涉及到了搜索，旋转和变色操作，因此时间复杂度为 Θ(log n) + Θ(1) + Θ(1) = Θ(log n)。\n\n## 删除\n\n红黑树的删除操作也和二叉搜索树相似，先找到删除的节点，然后根据待删除节点的孩子个数判断采取相应的删除方式。\n\n我们知道，如果待删除的节点有两个孩子节点，我们需要从它的左子树中找到最大的节点，或者从它的右子树中找到最小的节点，交换两者的值，然后再删除交换后的节点。因此，删除拥有两个孩子的节点实际上等同于删除叶子节点或只有一个孩子的节点。\n\n```\ndef get_max(self, root):\n    if root.right:\n        return self.get_max(root.right)\ndef remove(self, value):\n    # 树的根节点为空就抛出异常\n    if not self.root:\n        raise ValueError(\"The tree is null\")\n    self.remove_node(self.root, value)\ndef remove_node(self, root, value):\n    if root is self.NIL:\n        return\n    # 搜索\n    if value < root.value:\n        self.remove_node(root.left, value)\n    elif value > root.value:\n        self.remove_node(root.right, value)\n    else:\n        # 左右孩子为空\n        if root.left is self.NIL and root.right is self.NIL:\n            self.remove_leaf(root, True)\n        # 只有左孩子或右孩子\n        elif root.left is self.NIL or root.right is self.NIL:\n            self.remove_one_child(root)\n        # 左右孩子都有\n        else:\n            temp = self.get_max(root.left)  # 从左子树中找最大的节点\n            root.value = temp.value\n            self.remove_node(root.left, temp.value)\n```\n\n### 删除只有一个孩子的节点\n\n我们先来看看删除只有一个孩子的节点。如图所示，如果删除的是黑色节点 D，我们只需要将父节点 P 和待删除节点 D 的左(右)孩子相互连接起来即可。因为删除的是黑色节点，所以从父节点 P 出发到左侧任意叶子节点所经过的黑色节点的数目一定不等于到右侧任意叶子节点所经过的黑色节点的数目，也就是说删除后违反了性质 5。为了平衡，我们需要将待删除节点 D 的左(右)孩子的颜色调整为黑色。\n\n![单孩子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-one-child0.png)\n\n删除的节点不可能为红色节点，因为违反了性质 5。如图所示，如果就从该红色节点出发，到达左子树的任意节点所经过的黑色节点的数目一定大于 1，而它的右侧仅为一个 NIL 节点，所以经过黑色节点的数目为 1，所以我们不需要考虑删除红色节点的情况。\n\n![单孩子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-one-child1.png)\n\n用代码来实现这个过程是这样子的。\n\n```\ndef remove_one_child(self, node):\n    if node.left is not self.NIL:\n        node.left.color = 'b'\n        self.fix(node, node.left)\n    else:\n        node.right.color = 'b'\n        self.fix(node, node.right)\n```\n\n最后将待删除的节点删除掉，并用`fix`函数改变父节点和孩子节点的指针。\n\n```\ndef fix(self, p, n):\n    # p.parent 为空\n    if not p.parent:\n        self.root = n\n    elif p.parent.left is p:\n        p.parent.left = n\n    elif p.parent.right is p:\n        p.parent.right = n\n    if n is not self.NIL and n is not None:\n        n.parent = p.parent\n    del p\n```\n\n### 删除叶子节点\n\n下面来讨论一下删除叶子节点的情况，如果删除的是红色节点，由于删除后没有违反红黑树的任何性质，因此我们直接删除即可。\n\n![红色叶子节点](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-leaf-red.png)\n\n接下来是最最麻烦的删除黑色叶子节点的情况。前面看晕了的小伙伴可以先喘口气，下面的情况会更复杂。我们一共把删除黑色叶子节点分为 5 种情况。\n\n- 情况 1：删除的节点为根节点。\n\n    这种情况最简单，直接删除即可，然后将红黑树的根节点`self.root`置为`None`。\n    \n    ![情况 1](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-leaf-black-case1.png)\n    \n- 情况 2：待删除节点 D 的兄弟节点 B 为红色。按照惯例，由于待删除节点 D 可以为父节点 P 的左孩子，也可以为右孩子。所以我们又可以分为两种情况。\n\n    i) D 为左孩子的情况。调整的做法是先交换父节点 P 的和兄弟节点 B 的颜色，然后对兄弟节点执行一次左旋操作。\n    \n    ![情况 2A](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-leaf-black-case2A.png)\n\n    ii) D 为右孩子的情况。调整的做法还是先交换父节点 P 的和兄弟节点 B 的颜色，只是对兄弟节点的旋转操作变为了右旋。\n    \n    ![情况 2B](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-leaf-black-case2B.png)\n    \n    调整后，待删除的节点 D 的兄弟节点 L 的颜色为黑色，所以这就变为了后面要讨论的情况。\n\n- 情况 3：待删除节点 D 的兄弟节点 B 为黑色，且远侄子节点为红色，这里的远侄子节点指的是离 D 较远的节点。同样，待删除节点 D 可能是根节点 P 的左(右)孩子，我们还是把它们分为两种情况：\n\n    i) D 为左孩子的情况，远侄子节点为图中的 R，图中的白色父节点 P 表示可以为任意颜色。调整策略是交换父节点 P 和兄弟节点 B 的颜色，并将远侄子节点 R 的颜色改为黑色，最后再对 B 执行一次左旋操作。\n    \n    ![情况 3A](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-leaf-black-case3A.png)\n\n    ii) D 为右孩子的情况，远侄子节点为图中的 L。调整策略还是先交换父节点 P 和兄弟节点 B 的颜色，将远侄子节点 L 的颜色变为黑色，最后对 B 执行一次右旋操作。\n\n    ![情况 3B](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-leaf-black-case3B.png)\n\n- 情况 4：待删除节点 D 的兄弟节点 B 为黑色，且近侄子节点为红色。相反，近侄子节点指的是离 D 较近的节点。因为待删除节点 D 依然可能是根节点 P 的左(右)孩子，所以我们还是分为两种情况：\n\n    i) D 为左孩子的情况，近侄子节点为图中的 L，图中的白色父节点 P 表示可以为任意颜色。调整方案是先交换兄弟节点 B 和近侄子节点 L 的颜色，然后对 B 执行一次右旋操作，旋转后我们发现情况变为了 3A，因此再调用一次 3A 的操作方法即可。\n    \n    ![情况 4A](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-leaf-black-case4A.png)\n\n    ii) D 为右孩子的情况，近侄子节点为图中的 R。调整方案是先交换兄弟节点 B 和近侄子节点 R 的颜色，然后对 B 执行一次左旋操作，旋转后我们发现情况变为了 3B，因此再调用一次 3B 的操作方法即可。\n    \n    ![情况 4B](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-leaf-black-case4B.png)\n\n- 情况 5：兄弟节点 P 为红色，且它的左右孩子都为黑色。我们根据兄弟节点的颜色进一步分为两种情况。\n\n    i) 父节点 B 为红色，我们只需要交换 P 和 B 的颜色即可。\n    \n    ![情况 5A](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-leaf-black-case5A.png)\n\n    ii) 父节点 B 为黑色，这种情况下，我们需要将 B 的颜色改为红色，由于我们不能判断父节点 P 的父节点和兄弟节点是否都为黑色，所以我们需要在父节点 P 上递归地对情况 5 的进行检查，只是我们不再删除这里的父节点了，像这样一直往上递归，直到到达根节点。\n    \n    ![情况 5B](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/delete-leaf-black-case5B.png)\n\n将上述的情况全部打包成一个函数`remove_leaf`。\n\n```\ndef remove_leaf(self, node, flag):\n    # 删除红色节点不做任何变化\n    if node.color == 'r':\n        self.fix(node, self.NIL)\n        return\n    # 为黑色节点\n    # 情况 1: 删除的节点为根节点\n    if node is self.root:\n        if flag:\n            self.fix(node, None)\n        return\n    else:\n        parent = node.parent\n        brother = node.brother()\n        # 情况 2: 兄弟节点为红色\n        if brother.color == 'r':\n            parent.color = 'r'; brother.color = 'b'\n            # 情况 2A: 待删除节点为父节点的左孩子\n            if node.parent.left is node:\n                self.left_rotation(brother)\n            # 情况 2B: 待删除节点为父节点的右孩子\n            else:\n                self.right_rotation(brother)\n            self.remove_leaf(node, True)\n        else:\n            nephew_left = brother.left\n            nephew_right = brother.right\n            # 情况 3A: 兄弟节点为黑色，右侄子节点为红色，待删除节点为父节点的左孩子\n            if node.parent.left is node and nephew_right.color == 'r':\n                brother.color = parent.color; parent.color = 'b'; nephew_right.color = 'b'\n                self.left_rotation(brother)\n            # 情况 3B: 兄弟节点为黑色，左侄子节点为红色，待删除节点为父节点的右孩子\n            elif node.parent.right is node and nephew_left.color == 'r':\n                brother.color = parent.color; parent.color = 'b'; nephew_left.color = 'b'\n                self.right_rotation(brother)\n            # 情况 4A: 兄弟节点为黑色，左侄子节点为红色，待删除节点为父节点的左孩子\n            elif node.parent.left is node and nephew_left.color == 'r':\n                nephew_left.color = 'b'; brother.color = 'r'\n                self.right_rotation(nephew_left)\n                self.remove_leaf(node, True)\n            # 情况 4B: 兄弟节点为黑色，右侄子节点为红色，待删除节点为父节点的右孩子\n            elif node.parent.right is node and nephew_right.color == 'r':\n                nephew_right.color = 'b'; brother.color = 'r'\n                self.left_rotation(nephew_right)\n                self.remove_leaf(node, True)\n            # 情况 5: 兄弟节点为黑色，并且两个孩子都为黑色节点\n            elif brother.left.color == 'b' and brother.right.color == 'b':\n                # 情况 5A: 父节点为红色\n                if parent.color == 'r':\n                    parent.color = 'b'; brother.color = 'r'\n                # 情况 5B: 父节点为黑色\n                else:\n                    brother.color = 'r'\n                    self.remove_leaf(parent, False)\n    # 决定是否删除节点，flag 为 False 时实用于情况 5B\n    if flag:\n        self.fix(node, self.NIL)\n```\n\n删除操作也包含了搜索，旋转和变色操作，因此时间复杂度也为 Θ(log n) + Θ(1) + Θ(1) = Θ(log n)。\n\n## 例子\n\n让我们来看一个具体的例子，假设从数组`[1, 3, 4, 2, 5, 7, 6]`构建红黑树。\n\n插入节点 1。由于是根节点，所以需要将颜色置为黑色。\n\n![例子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/example1.png)\n\n插入节点 3。没有违反任何性质，不做任何改变。\n\n![例子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/example2.png)\n\n插入节点 4。因为叔父节点为 NIL，祖父节点的右孩子为父节点，父节点的右孩子为新插入节点，为情况 3B。因此我们需要先交换父节点 3 和 其祖父节点 1 的颜色，最后再对节点 3 进行左旋。\n\n![例子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/example3.png)\n\n插入节点 2。其祖父节点为黑色，父节点和叔父节点均为红色，情况 2 满足。所以我们将祖父节点的颜色变为红色，将父节点和叔父节点变为黑色，然后递归地对祖父节点进行检查，发现祖父节点为树的根节点，且为红色，因此满足情况 1，故将颜色变为黑色。\n\n![例子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/example4.png)\n\n插入节点 5。没有违反任何性质，不做任何改变。\n\n![例子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/example5.png)\n\n插入节点 7。因为叔父节点为黑色，祖父节点的右孩子为父节点，父节点的右孩子为新插入节点，也为情况 3B。因此我们需要先交换父节点 5 和 其祖父节点 4 的颜色，最后再对节点 5 进行左旋。\n\n![例子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/example6.png)\n\n最后插入节点 6。发现也满足情况 2，所以需要将祖父节点的颜色变为红色，将父节点和叔父节点变为黑色，然后递归地对祖父节点进行检查，最终得到的红黑树如下图所示。\n\n![例子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/example7.png)\n\n然后我们在从这棵红黑树中依次删除 5, 1, 7。\n\n删除节点 5。节点 5 有两个孩子节点，所以需要在它的左子树中找到最大的节点（节点 4），然后交换两者的值，这样就等同于删除一个黑色的叶子节点。我们发现它的兄弟节点为黑色，左侄子节点为红色，待删除节点为父节点的左孩子，所以满足情况 4A。做出调整是先交换兄弟节点 7 和近侄子节点 6 的颜色，然后对节点 6 执行一次右旋，再次检查待删除节点，发现又满足条件 3A，所以先交换父节点 4 和兄弟节点 6 的颜色，并将远侄子节点 7 的颜色变为黑色，最后再对节点 6 执行一次左旋操作。\n\n![例子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/example8.png)\n\n删除节点 1。由于只有一个孩子节点，因此只需要将节点 1 的右节点 2 与父节点 3 连接起来，然后将节点 2 的颜色变为黑色即可。\n\n![例子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/example9.png)\n\n删除节点 7。节点 7 为黑色叶子节点，且兄弟节点 4 为黑色，父节点 6 为红色，故满足条件 5A。因此我们只需要交换父节点和兄弟节点的颜色即可。\n\n![例子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/images/example10.png)\n\n这样我们就讲完了一个完整例子了。红黑树的操作十分复杂，情况非常多，代码也非常难理解，所以需要大家多花点时间消化本节的内容啊～"},{"title":"AVL树","url":"/study/algorithm/transform-and-conquer/AVL-tree/","content":"\n## 介绍\n\n在二叉搜索树那一节里，我们在最后分析复杂度的时候提到了如果二叉树严重不平衡，那么它的时间复杂度会退化到线性的复杂度，例如从有序数组[1, 2, 3, 4, 5]中构建出的二叉树，就会成为一个线性表，如下图所示。\n\n![二叉树](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/images/binary-tree.png)\n\n因此，我们需要有一棵能够自动平衡的二叉树，使得每次插入、删除和查找的复杂度都接近 Θ(nlog n) 的复杂度。AVL 树就是最早被提出的自平衡二叉搜索树，它的名字来自于两名俄国的科学家 G. M. Adelson-Velsky 和 E. M. Landis。下面，我们就来看看它是如何做到自平衡的吧。\n\n## 平衡因子\n\nAVL 树最核心的思想就是对每个节点计算**平衡因子**（**balance factor**），平衡因子的定义是一个节点的左孩子的高度减去其右孩子的高度。这里所谓的**高度**（**height**）就是指从一个节点出发到达最远叶子节点所经过的最长路径，例如下面的例子中节点 3 到达最远的叶子节点 1 需要经过的路径长度为 2（从节点 3 到节点 2 和 从节点 2 到节点 1）。\n\n![高度](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/images/height.png)\n\n我们规定空节点的高度为 -1，所以求任意一个节点的高度我们就可以分别计算左右孩子的高度，然后取较大者，最后加 1。于是，我们写出以下递归关系式：\n\n![relation](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/images/relation.png)\n\n在写出计算节点高度的代码时，我们首先还是需要构建一个`Node`类。\n\n```\nclass Node(object):\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n        self.height = 0  # 初始化为 0\n```\n\n然后才是我们的`calc_height`函数。\n\n```\ndef calc_height(self, node):\n    if not node:\n        return -1\n    return node.height\n```\n\n接下来计算节点的平衡因子就只需将左孩子的高度减去右孩子的高度即可。\n\n```\ndef calc_balance_factor(self, node):\n    if not node:\n        return 0\n    return self.calc_height(node.left) - self.calc_height(node.right)\n```\n\n例如，下图中的两个例子，根节点的平衡因子分别为 2 和 -2。\n\n![平衡因子](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/images/balance.png)\n\n## 旋转操作\n\n从前面的例子中，我们发现当一个节点的平衡因子的绝对值大于等于 2 时，树已经表现出不平衡。这里，AVL 树提供了一种旋转的机制，使得旋转过后的树不仅能保持二叉搜索树的性质，同时能够使节点的平衡因子的绝对值小于 2，接下来我们就分别来讨论一下不同情况下的旋转操作。\n\n### LL 型\n\n下图表示的是 LL 型的情况，在根节点的左孩子的左侧添加新节点 1 后，平衡因子从原来的 1 变为了 2，这时我们只需要对根节点 3 执行一次右旋操作树就平衡了。\n\n![LL](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/images/LL.png)\n\n### LR 型\n\n如果往根节点的左孩子的右侧添加新节点（这里的节点 2），平衡因子也会从 1 变为 2，但这里我们不能采用单次右旋，而是要先对根节点的左孩子执行一次左旋操作，使之变为 LL 型，然后再对根节点进行右旋。\n\n![LR](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/images/LR.png)\n\n### RR 型\n\nRR 型正好与 LL 型的互为镜像，如果向根节点的右孩子的右侧添加一个新节点（这里的节点 3）后，平衡因子由 -1 变为 -2，导致树的不平衡，这时我们需要对根节点执行一次左旋操作。\n\n![RR](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/images/RR.png)\n\n### RL 型\n\nRL 型与 LR 型也互为镜像，即如果将新节点（这里为节点 2）插入到根节点的右孩子的左侧，平衡因子由 -1 变为 -2。同样，我们不能用一次左旋来使树重新平衡，而是先对根节点的右孩子执行一次右旋，变为 RR 型，然后再对根节点进行左旋。\n\n![RL](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/images/RL.png)\n\n### 一般情况\n\n我们看到，上述的四种情况都是基于两种基本操作：左旋和右旋。然而，我们讨论的并不是一般的情况。举个例子，如果要对下图中的根节点进行右旋，我们会发现根节点的左孩子的右侧不为空，这时该怎么办呢？我们知道，旋转之后的树必须保持二叉树的性质，所以我们可以让多余的右子树作为原根节点的左子树，使得原根节点的值还是大于被调整子树的全部节点的值。\n\n![右旋](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/images/right-rotation.png)\n\n同理，下图也展示了左旋的一般情况，只是与右旋互为镜像操作。\n\n![左旋](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/images/left-rotation.png)\n\n因此，我们写出旋转操作的全部代码。\n\n```\ndef left_rotation(self, node):\n    t1 = node.right\n    t2 = t1.left\n    # 重构树\n    t1.left = node\n    node.right = t2\n    # 更新 node 和 h1 的高度\n    node.height = max(self.calc_height(node.left), self.calc_height(node.right)) + 1\n    t1.height = max(self.calc_height(t1.left), self.calc_height(t1.right)) + 1\n    return t1\ndef right_rotation(self, node):\n    t1 = node.left\n    t2 = t1.right\n    # 重构树\n    t1.right = node\n    node.left = t2\n    # 更新 node 和 h1 的高度\n    node.height = max(self.calc_height(node.left), self.calc_height(node.right)) + 1\n    t1.height = max(self.calc_height(t1.left), self.calc_height(t1.right)) + 1\n    return t1\ndef settle_violation(self, root):\n    balance = self.calc_balance_factor(root)\n    if balance > 1:\n        # 情况 1：LL 型，右旋\n        if self.calc_balance_factor(root.left) >= 0:\n            return self.right_rotation(root)\n        # 情况 2：LR 型，先左旋后右旋\n        else:\n            root.left = self.left_rotation(root.left)\n            return self.right_rotation(root)\n    elif balance < -1:\n        # 情况 3：RR 型，左旋\n        if self.calc_balance_factor(root.right) <= 0:\n            return self.left_rotation(root)\n        # 情况 4：RL 型，先右旋后左旋\n        else:\n            root.right = self.right_rotation(root.right)\n            return self.left_rotation(root)\n    return root\n```\n\n旋转操作由于只涉及到了指针的变动，因而该过程的时间复杂度不随树的节点个数的增加而增加，即为 Θ(1)。\n\n## 搜索\n\n和二叉搜索树一样，AVL 树也是从根节点开始搜索，如果搜索的元素比根节点小，那么就从根节点的左子树中继续搜索，如果比根节点大，那么就从右子树中继续搜索，如果相等，则返回该节点，要是搜索到了叶子节点发现还不是我们要找的元素，则搜索失败，返回`None`。\n\n```\ndef search(self, value):\n    # 树的根节点为空就抛出异常\n    if not self.root:\n        raise ValueError(\"The tree is null\")\n    return self.search_node(self.root, value)\ndef search_node(self, root, value):\n    if not root:  # 如节点为空，则返回 None\n        return None\n    if value < root.value:  # 从左子树查找\n        return self.search_node(root.left, value)\n    elif value > root.value:  # 从右子树查找\n        return self.search_node(root.right, value)\n    else:\n        return root\n```\n\n由于 AVL 树是平衡的，所以查找的时间复杂度接近 Θ(log n)。\n\n## 插入\n\nAVL 树的插入操作也跟二叉搜索树一样，先找到插入的位置，即通过搜索操作找到插入位点，然后创建一个新节点，最后父节点指向该新节点。但由于 AVL 树是一棵自平衡的树，所以每插入一个节点都会更新搜索过程中所经过节点的高度，如发现有节点的高度的绝对值大于等于 2，则采取相应的旋转操作使之保持平衡。这里调用了前面写好的`settle_violation`函数，整个过程均采用递归的方式（这是最难理解的地方，尤其是代码，需要各位小伙伴多花时间研究研究啊～）实现。\n\n```\ndef insert(self, value):\n    node = Node(value)  # 创建新节点\n    self.root = self.insert_node(self.root, node)\ndef insert_node(self, root, node):\n    if not root:\n        return node\n    if node.value < root.value:\n        root.left = self.insert_node(root.left, node)\n    else:\n        root.right = self.insert_node(root.right, node)\n    root.height = max(self.calc_height(root.left), self.calc_height(root.right)) + 1  # 更新根节点的高度\n    return self.settle_violation(root)  # 查看是否满足 AVL 树的性质\n```\n\n## 删除\n\n同样，AVL 树的删除操作也类似于二叉搜索树，找到删除的节点，然后根据待删除节点的孩子的个数，判断出该采取哪一种删除方式，由于它跟二叉搜索树一样，这里就不再赘述了，忘记了的小伙伴可以乘坐这里的[传送门](https://inifnityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/)，找到删除操作的那一部分再看一看。与插入操作一样，删除操作也会检查搜索过程中所经过的所有节点的高度，如果其绝对值大于等于 2，我们也调用`settle_violation`函数来使树保持平衡，整个删除操作也是一个递归的过程（理解代码也是需要一个漫长的过程哦 ◠‿◠）。\n\n```\ndef get_max(self, root):\n    if root.right:\n        return self.get_max(root.right)\n    return root\ndef remove(self, value):\n    # 根节点为空则抛出异常\n    if not self.root:\n        raise ValueError(\"The tree is null!\")\n    self.root = self.remove_node(self.root, value)\ndef remove_node(self, root, value):\n    if not root:\n        return root\n    # 搜索\n    if value < root.value:\n        root.left = self.remove_node(root.left, value)\n    elif value > root.value:\n        root.right = self.remove_node(root.right, value)\n    else:\n        # 左右孩子为空\n        if not root.left and not root.right:\n            del root\n            return None\n        # 只有左孩子或右孩子\n        elif not root.left:\n            temp = root.right\n            del root\n            return temp\n        elif not root.right:\n            temp = root.left\n            del root\n            return temp\n        # 左右孩子都有\n        else:\n            temp = self.get_max(root.left)\n            root.value = temp.value\n            root.left = self.remove_node(root.left, temp.value)\n    root.height = max(self.calc_height(root.left), self.calc_height(root.right)) + 1  # 更新根节点的高度\n    return self.settle_violation(root)  # 查看是否满足 AVL 树的性质\n```\n\n插入和删除操作都是由查找和旋转操作构成，故两者的复杂度均为 Θ(log n) + Θ(1) = Θ(log n)。\n\n## 合并\n\n最后将上面的这些函数都写在一个`AVL_Tree`类里。\n\n```\nclass AVL_Tree(object):\n    def __init__(self):\n        self.root = None\n```\n\n## 演示\n\n要是你觉得上面的代码很难理解，[这里](https://visualgo.net/en/bst)提供了 AVL 树的可视化过程。你可以结合本节的[代码](https://github.com/infinityglow/Algorithm-and-Complexity/tree/master/Transform%20and%20Conquer/AVL%20Tree)将例子输入到可视化的网站中。\n\n"},{"title":"堆与堆排序","url":"/study/algorithm/transform-and-conquer/heap/","content":"\n## 优先队列\n\n乘坐过飞机时你一定清楚这样一个场景，在候机厅排队登机的过程中，拥有头等舱的乘客总是能够有“特权”，不用排队就能直接登机。\n\n![登机](https://infinityglow.github.io/study/algorithm/transform-and-conquer/heap/images/boarding.jpg)\n\n我们看到，坐头等舱的乘客的优先级比坐普通经济舱的乘客要高。如果用计算机里的队列来描述的话，头等舱的乘客在出队时总是先于经济舱的乘客。于是，我们可以定义出一种新的抽象数据类型，**优先队列**（**priority queue**），即所有元素都按照优先级的大小出队。举个例子，假设一个数组为 [34, 12, 56, 98, 49]，优先级的顺序为 [4, 2, 1, 3, 5]，那么出队的顺序就为 56->12->98->34->49。\n\n## 堆\n\n当创建一个优先队列（假设全由数字构成）时，我们需要两个数组，一个用于存储数据，一个用于存储各个元素的优先级。所需要的存储量是一般数组的两倍，显然是不可行的。另外，因为队列的出队操作只能严格在队尾执行，所以每次出队的过程中，出队的元素必须要和队尾元素进行交换，然后再出队，但这样操作后的问题是，存储优先级的数组也要做出相同的变化才能够维持优先队列的性质，非常的麻烦。因此，我们还需要另寻他法，使优先队列的数据结构更简单。\n\n![优先队列](https://infinityglow.github.io/study/algorithm/transform-and-conquer/heap/images/pqueue.png)\n\n我们可以借助二叉树的性质，将优先级越高的元素放置在越靠近根   节点的位置，换句话说就是位于层数越低的地方。**堆**（**heap**）就是将优先队列变换为了二叉树的形式。下面来介绍一下它的性质：\n\n### 完全二叉树\n\n堆是一棵完全二叉树。在将[数据结构](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/)那一节里，我们简单地提到了它。区分完全二叉树和非完全二叉树就是，完全二叉树除了最后一层的节点没有排满以外，其他所有层均已排满。并且在构建完全二叉树的时候，新加入的节点在最后一层从左往右依次排列，直至排满。如当前层已排满就从下一层开始排。\n\n![二叉树](https://infinityglow.github.io/study/algorithm/transform-and-conquer/heap/images/tree.png)\n\n### 堆的构建\n\n下面来说一说堆的第二个性质，这里我们以数字为例，数字越大的优先级越高。由于优先级越高的元素放置在越靠近根节点的位置，所以我们定义，所有父节点的值均大于等于其孩子节点的值，这样根节点的值一定是堆中最大的。例如下图中根节点 95 是最大值，它大于它的两个孩子节点 86 和 74，而节点 86 又大于节点 47 和 52。\n\n![堆](https://infinityglow.github.io/study/algorithm/transform-and-conquer/heap/images/heap.png)\n\n有了堆的第二个性质，我们就可以构建出一个堆了。首先将数组元素以堆的形式来表示，然后调整堆内元素的排列，直到满足堆的第二个性质。\n\n构建堆的具体过程在下面的视频中已经显示出来。首先从最后一个具有孩子节点的父节点（节点 36）开始，与孩子节点中值较大的节点（节点 72）比较，如果该节点比父节点要大，就交换这两个节点的值，然后被交换的孩子节点又与它的子节点进行比较，直到没有子节点为止。对所有的父节点，我们都执行上述的操作，最后就能构建出一个堆了。\n\n![堆](https://infinityglow.github.io/study/algorithm/transform-and-conquer/heap/images/build.mp4)\n\n如果用数组来体现这一性质，那么数组元素就应该满足：e<sub>i</sub> ≥ e<sub>2i+1</sub> 并且 e<sub>i</sub> ≥ e<sub>2i+2</sub>。其中 e<sub>i</sub> 代表第 i 个元素的值。至于循环中 i 的取值，假设有 n 个元素，i 就从 ⌊(n-2)/2⌋ 开始逐渐递减到 0。\n\n因此我们写出代码：\n\n```\ndef heaplify(array, temp_idx, size):\n    temp_val = array[temp_idx]\n    heap = False\n    while not heap and 2 * temp_idx + 1 < size:\n        j = 2 * temp_idx + 1  # 左孩子的索引\n        # 右孩子存在\n        if j < size - 1:\n            # 比较两个孩子的值\n            if array[j] < array[j+1]:\n                j = j + 1\n        # 判断是否满足堆的性质\n        if array[j] <= temp_val:\n            heap = True\n        else:\n            array[temp_idx] = array[j]\n            temp_idx = j  # 更新 temp_idx\n    array[temp_idx] = temp_val\n    return array\n```\n\n在主程序中用一个`for`循环来控制调整堆的次数。\n\n```\nfor i in range((len(array)-2)//2, -1, -1):\n    arrary = heaplify(array, i, len(array))\n```\n\n最后来分析一下构建堆的复杂度。假设堆有 h 层，且每一层均已填满。在构建时，我们需要从第 h - 1 层开始调整堆的结构，直至调整到第 1 层。在最坏的情况下，第 i 层的元素最多需要比较 2(h - i) 次，而每一层最多有 2<sup>i-1</sup> 个节点，所以用一个求和公式来表示就是这样的：\n\n![derivation](https://infinityglow.github.io/study/algorithm/transform-and-conquer/heap/images/derivation0.png)\n\n最后求出复杂度为 Θ(n)。\n\n### 堆的删除\n\n与队列一样，堆的删除只能在堆顶处执行。为了删除堆顶后仍能保持堆的性质，我们实际的操作是将堆的最后一个元素的值赋给堆顶元素，然后堆顶元素从上而下调整，如遇到父节点比孩子节点大就停止。\n\n![堆](https://infinityglow.github.io/study/algorithm/transform-and-conquer/heap/images/delete.mp4)\n\n上面的动画展示了删除堆顶元素 95 的全过程。首先需要与最后一个节点的元素 60 交换位置，然后删除 95，最后从上到下调整节点 60 的位置。\n\n在最坏的情况下，堆删除的复杂度取决于堆的层数 h。如果堆内有 n 个元素，那么堆的层数 h = ⌊log<sub>2</sub>n⌋ + 1，故复杂度为 Θ(log n)。\n\n## 堆排序\n\n**堆排序**（**heap sort**）是基于堆的一种排序，整个过程分为四步：\n\n- 构建堆\n- 输出并删除堆顶元素，用堆中最后一个元素替换堆顶元素\n- 调整堆结构\n- 重复上述过程，直至堆内没有任何元素\n\n事实上，我们利用了堆顶元素总是最大这一性质，然后依次取出最大的元素，同时调整堆的结构使得下一个堆顶元素又是堆中剩下元素中的最大值，如果一直重复这个过程，数组就从大到小依次排列啦。\n\n```\ndef heap_sort(array):\n    for i in range((len(array)-2)//2, -1, -1):\n        array = heaplify(array, i, len(array))\n    for i in range(len(array)-1, -1, -1):\n        array[0], array[i] = array[i], array[0]  # 堆顶元素和堆的最后一个元素交换\n        array = heaplify(array, 0, i)\n    return array\n```\n\n我们再用一个动画来直观地感受一下这个过程。\n\n![堆](https://infinityglow.github.io/study/algorithm/transform-and-conquer/heap/images/heap.mp4)\n\n堆排序结合了构建堆和删除堆顶元素两个过程。对于构建堆，复杂度即为 Θ(n)；而删除堆的操作需要执行 n 次，每次删除的复杂度均取决于当前堆的层数，所以我们列出下列的关系式：\n\n![derivation](https://infinityglow.github.io/study/algorithm/transform-and-conquer/heap/images/derivation1.png)\n\n计算出堆的删除操作在最坏的情况下不会高于 Θ(nlog n) 的复杂度。\n\n所以我们可以得到堆排序的复杂度，即将两者的复杂度相加 Θ(n) + Θ(nlog n) = Θ(nlog n)。\n\n堆排序在任何情况下复杂度都保持在 Θ(nlog n)，并且不需要额外的存储空间。但计算的时候我们发现堆排序的比较次数前面有一个常数 2，所以在通常情况下会稍逊色于快速排序和归并排序。如果你分别运行 GitHub 里我写的这三种排序的代码，你会发现堆排序的平均比较次数会多于快速排序和归并排序，所消耗的时间也比这两者要长。\n\n```\n>>> python \"heap sort.py\"\n>>> 平均比较次数：235346\n    平均运行时间：0.0616 s\n\n>>> python \"merge sort.py\"\n>>> 平均比较次数：120467\n    平均运行时间：0.0569 s\n\n>>> python \"quick sort.py\"\n>>> 平均比较次数：156098\n    平均运行时间：0.0348 s\n```"},{"title":"霍纳法则","url":"/study/algorithm/transform-and-conquer/horners-rule/","content":"\n## 多项式计算\n\n在计算机科学里，有时会遇到很多关于计算多项式的问题，例如 2x<sup>4</sup> - 3x<sup>3</sup> + 5x<sup>2</sup> + x - 7。我们首先能够想到的方法就是求出每一项的值，然后把它们全部加起来。如果多项式的阶数不高，这种方法完全可行，甚至更容易理解，可是如果把这个问题推广到 n 阶，即计算 a<sub>n</sub>x<sup>n</sup> + a<sub>n-1</sub>x<sup>n-1</sup> + ··· + a<sub>2</sub>x<sup>2</sup> + a<sub>1</sub>x + a<sub>0</sub> 的值，并且当 n 很大时，这种算法就显得力不从心了。\n\n这里以 x = 4 为例来计算多项式 2x<sup>4</sup> - 3x<sup>3</sup> + 5x<sup>2</sup> + x - 7 的值。\n\n```\ndef poly_bf(coeffi_list, x):\n    degree = len(coeffi_list) - 1  # 最高次项\n    result = 0\n    for i in range(degree+1):\n        coeffi = coeffi_list[i]; poly = 1\n        for j in range(degree-i-1, -1, -1):\n            poly *= x  # 计算 x^i\n        result += coeffi * poly\n    return result \n```\n\n直接求解的方法的复杂度等于多少呢？我们知道，计算机在计算乘法的时候的时间开销要大于加减法的时间开销，所以这里的复杂度大致可以看做执行乘法运算的次数。\n\n![derivation](https://infinityglow.github.io/study/algorithm/transform-and-conquer/horners-rule/images/derivation.png)\n\n最后得到时间复杂度为 Θ(n<sup>2</sup>)。\n\n## 霍纳法则\n\n**霍纳法则**（**Horner's rule**）是将一个一般的多项式转化为下面的形式：\n\n![transformation](https://infinityglow.github.io/study/algorithm/transform-and-conquer/horners-rule/images/transformation.png)\n\n我们可以用一张表格，来使我们能够更好地理解计算的过程。假设还是计算当 x = 4 时 2x<sup>4</sup> - 3x<sup>3</sup> + 5x<sup>2</sup> + x - 7 的值。我们需要先将其转换为 x(x(x(2x - 3) + 5) + 1) - 7 的形式。\n\n![table](https://infinityglow.github.io/study/algorithm/transform-and-conquer/horners-rule/images/table.png)\n\n代码实现非常容易，只需要一个循环即可。\n\n```\ndef poly_horner(coeffi_list, x):\n    degree = len(coeffi_list) - 1  # 最高次项\n    result = coeffi_list[0]\n    for i in range(1, degree+1):\n        result = result * x + coeffi_list[i]\n    return result\n```\n\n经过霍纳法则变换的多项式只需要执行 n 次乘法运算便可以得到 n 阶多项式的值，所以复杂度自然就为 Θ(n)，跟直接求解相比有了明显的提升。\n"},{"title":"预排序","url":"/study/algorithm/transform-and-conquer/presorting/","content":"\n## 变治\n\n**变治**（**transform and conquer**）也是一类解决问题的方法。前面的减治和分治都是针对问题本身进行求解，并没有对问题的实例做任何的变换。变治法就是将实例从一种形式转换为另外一种形式的方法。这样听起来还是太抽象了，咱们还是拿一个具体的例子吧。\n\n不知道大家有没有玩过一个叫做熄灯（英文叫 lights out）的游戏。规则很简单，以下面的图片（来源：[维基百科](https://en.wikipedia.org/wiki/Lights_Out_(game)#/media/File:LightsOutIllustration.svg)）为例，面板上分布着 5 × 5 的格子，每个格子的灯都是亮着的，且灯只有亮着和熄灭两种状态，每按一次格子，都会影响到该格子以及上、下、左、右四个相邻格子的灯的状态，即原来亮着的灯会熄灭，原来熄灭的灯会打开。游戏的目标是用最少的次数将面板上所有的灯都熄灭。\n\n![lights out](https://infinityglow.github.io/study/algorithm/transform-and-conquer/presorting/images/lightsout.png)\n\n我们可以把面板上这些灯的状态抽象为一个 5 × 5 的矩阵，其中 1 代表灯是亮着的，0 代表灯是熄灭的。现在，游戏的目标变为了怎样使全为 1 的矩阵变为全为 0 的矩阵。我们知道，对于每一个格子来讲，都有按下（1）和不按下（0）两种可能，所以我们可以用一个方程来表示它们：x<sub>1</sub> ⊕ x<sub>2</sub> ⊕ ··· ⊕ x<sub>25</sub> ⊕ 1 = 0。方程的含义是怎样按这 25 个格子最后使得其中一个格子的状态为熄灭（0）。其中 ‘⊕’ 表示异或，它是一种逻辑运算，只有当 A 和 B 不相同时为 1，相同时为 0。这正好满足了游戏的规则。\n\n| A | B | A ⊕ B |\n| :----: | :----: | :----: |\n| 0 | 0 | 0 |\n| 0 | 1 | 1 |\n| 1 | 0 | 1 |\n| 1 | 1 | 0 |\n\n因为每一个格子都会收到相邻格子的影响，拿第一行第一列的格子来讲，它只会受到三个格子的影响，而其他格子不会对它造成任何影响，因此我们可以在每个 x<sub>i</sub> 的前面加一个系数 a<sub>i</sub>，受影响的为 1，不受影响的为 0，因此对于不受影响的格子来讲，无论 x 怎么变，其结果都为 0。如果对面板中每一个格子都列出相应方程式，我们就可以得到一个 25 元 1 次方程组，用一个矩阵来表示就是这样子的：\n\n![矩阵](https://infinityglow.github.io/study/algorithm/transform-and-conquer/presorting/images/matrix.png)\n\n学过线性代数的你一定想到了用高斯消元法来解这样的线性方程组。的确如此，只是这里的 + 换成了 ⊕。我们将不再深究具体怎么去实现这一过程了。只想说明一点的是：原来的熄灯问题经过变换之后，变为了用高斯消元法解线性方程组的问题，这就是变治法的思想，也是这一章要重点讨论的问题。\n\n## 判断数组元素是否重复\n\n先来举一个最简单的例子，判断数组元素是否有重复数。我们可以用最简单的方式：取数组的每一个元素，然后在剩余的数组元素里查看有没有重复。\n\n```\ndef is_unique_bf(array):\n    length = len(array)\n    for i in range(length-1):\n        element = array[i]\n        for j in range(i+1, length):\n            if array[j] == element:\n                return False\n    return True\n```\n\n这样做效率很低，复杂度为 Θ(n<sup>2</sup>)。假设我们先将数组进行排序，因为对于一个有序数组，我们只需要查看一个元素跟它的下一个元素是否相等即可。\n\n```\ndef is_unique_presort(array):\n    length = len(array)\n    array.sort()  # 排序\n    for i in range(length-1):\n        if array[i] == array[i+1]:\n            return False\n    return True\n```\n\n由于算法由两部分构成（排序和主循环），所以该其复杂度也应该为两部分：Θ(nlog n) + Θ(n) = Θ(nlog n)。类似于这样的处理方式我们称为**预排序**（**presorting**）。\n\n## 寻找众数\n\n**众数**（**mode**）是一组数据中出现次数最多的数。如果用暴力求解的算法寻找众数，我们需要统计数据中所有数字出现的次数，然后选择出现频率最高的数。\n\n````\ndef compute_mode_bf(array):\n    counter = {}\n    for i in range(len(array)):\n        counter[array[i]] = counter.get(array[i], 0) + 1\n    freq = 0\n    for key, value in counter.items():\n        if value > freq:\n            freq = value\n            mode = key\n    return mode\n````\n\n在最坏的情况下，数组里每一个元素出现的次数都为 1，所以第 i 个元素需要跟字典里的 i - 1 个元素进行比较，复杂度就为 Θ(n<sup>2</sup>)。\n\n![derivation](https://infinityglow.github.io/study/algorithm/transform-and-conquer/presorting/images/derivation.png)\n\n如果对数组进行预排序，我们就只需要统计数组中临近的相同元素的个数即可，因为相同的元素在有序数组里一定是相邻的。\n\n```\ndef compute_mode_presort(array):\n    array.sort()  # 排序\n    i = 0; freq = 0\n    while i < len(array):\n        temp_freq = 1; temp_mode = array[i]\n        while i + temp_freq < len(array) and array[i+temp_freq] == temp_mode:\n            temp_freq += 1\n        if temp_freq > freq:\n            freq = temp_freq; mode = temp_mode\n        i += temp_freq\n    return mode\n```\n\n上述的算法只需要遍历一次数组，再加上排序算法，故复杂度为 Θ(nlog n) + Θ(n) = Θ(nlog n)。"},{"title":"最近点对与凸包问题（DC）","url":"/study/algorithm/divide-and-conquer/clo-pair-con-hull/","content":"\n## 最近点对问题\n\n在[前面](https://infinityglow.github.io/study/algorithm/brute-force/clo-pair-con-hull/)的章节里，我们分别讨论了最近点对和凸包问题，但采用的都是列举其所有情况的暴力解法，效率非常低。这节的内容，我们会用到两种更为高效的方法。\n\n既然这章都在讲分治法，在解决最近点对问题的时候自然也离不开将这些散点进行分组。所以这里我们用一条垂直分界线 L，将散点分成两部分，S<sub>L</sub> 和 S<sub>R</sub>。\n\n![最近点对](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/images/clo-pair0.png)\n\n其中，S<sub>L</sub> 包含 ⌈n/2⌉ 个点，而 S<sub>R</sub> 包含 ⌊n/2⌋ 个点。\n\n接下来，我们要分别计算出左右两部分的最近点对距离 d<sub>L</sub> 和 d<sub>R</sub>。\n\n![最近点对](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/images/clo-pair1.png)\n\n然后我们取其最小值 d = min{d<sub>L</sub>, d<sub>R</sub>}。但是这里的 d 不一定就是最后的最小值，因为分别位于分界线 L 左右两边的点可能存在更短的点对。这时，我们需要在分界线左右宽度均为 d 的区域内寻找有没有更短的点对。\n\n![最近点对](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/images/clo-pair2.png)\n\n在这个区域中，我们只需要把所有的点对全部列举出来，然后选择最小的，最后与前边计算得到的 d 进行比较取较小者即可。但为了计算方便，我们可以先对这些区域内的点的 y 坐标进行排序，然后再逐个查看每个点对。这样做的好处是一旦我们发现 y<sub>i</sub> - y<sub>j</sub> 的值大于 d，我们就查看下一个点，以减少不必要的比较次数。\n\n左右两半部分各自最近点对要怎样计算出来呢？我们可以利用前面提到的方法，将这些散点再分为左右两部分，像这样一直递归下去，当散点个数小于等于 3 个时，我们再利用直接求解的方式将最近点对计算出来。\n\n![最近点对](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/images/clo-pair3.png)\n\n代码实现：\n\n当 n <= 3 时：\n\n```\ndef brute_force(x, y):\n    dmin = np.inf  # 初始化 dmin 为无穷\n    for i in range(len(x)):\n        for j in range(i + 1, len(x)):\n            if (x[i] - x[j]) ** 2 + (y[i] - y[j]) ** 2 < dmin:\n                dmin = (x[i] - x[j]) ** 2 + (y[i] - y[j]) ** 2\n                coordinates = np.array([[x[i], y[i]], [x[j], y[j]]])\n    return dmin, coordinates\n```\n\n主函数：\n\n```\ndef closest_pair_dc(P, Q, n):\n    if n <= 3:\n        return brute_force(P, Q)\n    else:\n        left = (len(P) + 1) // 2; right = len(P) - left  # 左右两部分的散点数\n        P_left = P[:left]; P_right = P[left:]\n        Q_left = Q[:left]; Q_right = Q[left:]\n        d_left, pair_left = closest_pair_dc(P_left, Q_left, left)  # 从左半部分递归地寻找最近点对\n        d_right, pair_right = closest_pair_dc(P_right, Q_right, right)  # 从右半部分递归地寻找最近点对\n        # 选择较短者\n        if d_left < d_right:\n            d = d_left; temp_pair = pair_left\n        else:\n            d = d_right; temp_pair = pair_right\n        m = P[(len(P)) // 2]\n        S = []  # 存储区域 |x - m| < d 内所有的点\n        # copy\n        for key in P:\n            if abs(key - m) < d:\n                S.append(key)\n        for i in range(len(S) - 1):\n            j = i + 1\n            while j < len(S) and (XY_pair[S[i]] - XY_pair[S[j]]) ** 2 < d:\n                if (S[i] - S[j]) ** 2 + (XY_pair[S[i]] - XY_pair[S[j]]) ** 2 < d:\n                    d = (S[i] - S[j]) ** 2 + (XY_pair[S[i]] - XY_pair[S[j]]) ** 2  # 更新 d\n                    temp_pair = np.array([[S[i], XY_pair[S[i]]], [S[j], XY_pair[S[j]]]])  # 更新点对\n                j += 1\n    return d, temp_pair\n```\n\n根据前面的讲解和代码，我们可以写出下面的递归关系式：\n\n![relation](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/images/relation0.png)\n\n最后得到复杂度为 Θ(nlog n)。这可比暴力求解算法的 Θ(n<sup>2</sup>) 有了明显的改善呐。\n\n## 凸包问题\n\n跟前面一样，用分治法解决凸包问题也需要将所有散点分为两部分，然后分别从这两部分中找出构成凸多边形的点。\n\n![凸包](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/images/con-hull0.png)\n\n具体步骤是取横坐标最小和最大的两个点 p<sub>1</sub> 和 p<sub>n</sub>，我们很容易证明 p<sub>1</sub> 和 p<sub>n</sub> 就是构成凸多边形的点，于是将这两个点连接起来构成向量 p<sub>1</sub>p<sub>n</sub>。现在问题就变为了从向量左右两侧的散点中分别找出构成凸包的点。\n\n![凸包](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/images/con-hull1.png)\n\n接下来我们需要从左右两侧的点中找出距离向量 p<sub>1</sub>p<sub>n</sub> 最远的点 p<sub>max</sub>，该点即可作为构成凸多边形的点。\n\n![凸包](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/images/con-hull2.png)\n\n找出距离最远点其实就是使 △p<sub>1</sub>p<sub>max</sub>p<sub>n</sub> 的面积最大，如果你熟悉线性代数，计算三角形的面积即计算下面行列式的值。\n\n![determinant](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/images/determinant.png)\n\n其中 (x<sub>1</sub>, y<sub>1</sub>), (x<sub>2</sub>, y<sub>2</sub>), (x<sub>3</sub>, y<sub>3</sub>) 分别表示 p<sub>1</sub>, p<sub>n</sub> 和 p<sub>max</sub> 的横纵坐标。如果行列式为正，代表 p<sub>max</sub> 的值在向量 p<sub>1</sub>p<sub>n</sub> 的左侧；如果为负值，则代表 p<sub>max</sub> 在向量 p<sub>1</sub>p<sub>n</sub>。于是可以写出代码： \n\n```\ndef compute_area(x1, y1, x2, y2, x3, y3):\n    area = x1*y2 + x3*y1 + x2*y3 - x3*y2 - x2*y1 - x1*y3        \n    return area\n```\n\n如果存在距离相同的点，我们选择使 ∠p<sub>max</sub>p<sub>1</sub>p<sub>n</sub> 最大的 p<sub>max</sub>。\n\n```\ndef compute_cos(x1, y1, x2, y2, x3, y3):\n    # 利用余弦定理计算角度\n    a = np.sqrt((y3 - y2)**2 + (x3 - x2)**2)\n    b = np.sqrt((y3 - y1)**2 + (x3 - x1)**2)\n    c = np.sqrt((y2 - y1)**2 + (x2 - x1)**2)\n    return -(b**2 + c**2 - a**2) / (2 * b * c)\n```\n\n寻找 p<sub>max</sub> 的主函数：\n\n```\ndef find_p_max(subset, p_left, p_right):\n    x1 = p_left[0]; y1 = p_left[1]\n    x2 = p_right[0]; y2 = p_right[1]\n    max_area = 0\n    for i in range(len(subset)):\n        x3 = subset[i][0]; y3 = subset[i][1]\n        area = compute_area(x1, y1, x2, y2, x3, y3)  \n        if area > max_area:\n            max_area = area\n            angle = compute_cos(x1, y1, x2, y2, x3, y3)\n            idx = i\n        elif area == max_area:\n            # 如果面积相同，则选择使∠p_maxp1pn最大的p_max\n            cur_angle = compute_cos(x1, y1, x2, y2, x3, y3)\n            if cur_angle > angle:\n                angle = cur_angle\n                idx = i\n    return subset[idx]\n```\n\n我们从点集合 `subset` 中寻找距离向量最远的点。其中 `subset` 表示向量 p1pn 划分出的两部分点集合，我们用下面两个函数来生成它们：\n\n```\ndef is_in_set(p, p_left, p_right):\n    # 去掉 p1 and pn\n    if p[0] == p_left[0] and p[1] == p_left[1] or p[0] == p_right[0] and p[1] == p_right[1]:\n        return False\n    return True\ndef generate_subset(totalset, p_left, p_right):\n    subset = np.array([[]])\n    x1 = p_left[0]; y1 = p_left[1]\n    x2 = p_right[0]; y2 = p_right[1]\n    for i in range(len(totalset)):\n        x3 = totalset[i][0]; y3 = totalset[i][1]\n        area = compute_area(x1, y1, x2, y2, x3, y3)\n        if area > 0 and is_in_set(totalset[i], p_left, p_right):\n            if subset.shape[1] == 0:\n                subset = np.append(subset, totalset[i][np.newaxis, :], axis=1)\n            else:\n                subset = np.append(subset, totalset[i][np.newaxis, :], axis=0)\n    return subset\n```\n\n找到 p<sub>max</sub> 后，我们连接 p<sub>1</sub>p<sub>max</sub> 和 p<sub>max</sub>p<sub>n</sub>（这里以上半部分的 p<sub>max</sub> 为例）。此时，我们再从 p<sub>1</sub>p<sub>max</sub> 和 p<sub>max</sub>p<sub>n</sub> 的左侧中分别寻找距离最远的点，来作为构成凸包的点。如果我们递归地执行上述的操作，所有构成凸包的点就都能够找到啦～\n\n![凸包](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/images/con-hull3.png)\n\n因此，我们写出主函数 `quick_hull`。\n \n ```\ndef quick_hull(subset, p_left, p_right):\n    # subset 为空\n    if subset.shape[1] == 0:\n        return\n    p_max = find_p_max(subset, p_left, p_right)\n    global pairs  # 用于存储构成凸包的点\n    pairs = np.append(pairs, p_max[np.newaxis, :], axis=0)  # 增加一个维度\n    upper_subset = generate_subset(subset, p_left, p_max)\n    lower_subset = generate_subset(subset, p_max, p_right)\n    quick_hull(upper_subset, p_left, p_max)\n    quick_hull(lower_subset, p_max, p_right)\n```\n\n最后是我们的画图代码：\n\n```\ndef plotting(hull_points, scatter_points):\n    upper_points = np.array(generate_subset(hull_points, hull_points[0], hull_points[1]))\n    lower_points = np.array(generate_subset(hull_points, hull_points[1], hull_points[0]))\n    x_upper = sorted(upper_points[:, 0])\n    x_lower = sorted(lower_points[:, 0], reverse=True)\n    x = [hull_points[0][0]]; x.extend(x_upper); x.append(hull_points[1][0]); x.extend(x_lower); x.append(hull_points[0][0])\n    y = []\n    for key in x:\n        y.append(XY_pair[key])\n    plt.scatter(scatter_points[:, 0], scatter_points[:, 1], s=15)\n    plt.plot(x, y, c='r')\n    plt.grid()\n    plt.show()\n```\n\n假设散点是随机分布在平面上，每次都要把点集划分为相等的两部分，所以每一次问题的规模都会减半。划分好之后，我们需要遍历整个点集，以寻找距离最远的点，这个过程是一个线性的复杂度。因而，我们可以写出下面的递归关系式：\n\n![relation](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/images/relation1.png)\n\n故平均复杂度为 Θ(nlog n)。\n\n相比暴力求解的 Θ(n<sup>3</sup>) 复杂度，分治法解决凸包问题在效率上已经有了明显的进步。"},{"title":"二叉树的遍历","url":"/study/algorithm/divide-and-conquer/bt-traversal/","content":"\n## 引入\n\n前面讲到了二叉搜索树以及它的一些基本操作（插入，搜索，删除），可是我们要怎样知道这些操作是被正确执行的呢？这就需要借助遍历来间接“打印”出树的结构。\n\n和图的遍历一样，我们也需要逐个访问每一个节点，但跟图不同的是，二叉树的遍历是从根节点开始的，而非任意节点。根据二叉树的特点，每颗二叉树都是由三部分组成：根节点，左子树和右子树。而根节点的左右子树也是由这三部分组成，所以二叉树的遍历问题也是可以用分治来解决的，即遍历一颗二叉树 = 访问根节点 + 遍历根节点的左子树 + 遍历根节点的右子树。\n\n![二叉树](https://infinityglow.github.io/study/algorithm/divide-and-conquer/bt-traversal/images/binary-tree.png)\n\n于是我们可以写出下面递归关系式：\n\n![relation](https://infinityglow.github.io/study/algorithm/divide-and-conquer/bt-traversal/images/relation.png)\n\n其中 n(T) 代表的是二叉树 T 的节点个数。因为当 T 为空树时，我们不执行任何操作，所以这里的 S(0) = 0，即递归的出口。\n\n## 三种遍历法\n\n根据左子树、右子树和根节点的遍历顺序，我们可以把遍历方式分为前序、中序和后序。接下来我们就来分别看看吧。\n\n### 前序遍历\n\n前序遍历是先访问根节点，然后遍历左子树，最后遍历右子树。下面这张动图展示前序遍历的整个过程。\n\n![前序](https://infinityglow.github.io/study/algorithm/divide-and-conquer/bt-traversal/images/pre-order.gif)\n\n代码实现：\n\n```\ndef pre_order(T):\n    print(T.value, end=' ')\n    if T.left:\n        pre_order(T.left)\n    if T.right:\n        pre_order(T.right)\n```\n\n### 中序遍历\n\n中序遍历是先遍历左子树，然后访问根节点，最后遍历右子树。下面这张动图展示中序遍历的整个过程。\n\n![中序](https://infinityglow.github.io/study/algorithm/divide-and-conquer/bt-traversal/images/in-order.gif)\n\n代码实现：\n\n```\ndef in_order(T):\n    if T.left:\n        in_order(T.left)\n    print(T.value, end=' ')\n    if T.right:\n        in_order(T.right)\n```\n\n### 后序遍历\n\n后序遍历是先遍历左子树，然后遍历右子树，最后访问根节点。下面这张动图展示后序遍历的整个过程。\n\n![后序](https://infinityglow.github.io/study/algorithm/divide-and-conquer/bt-traversal/images/post-order.gif)\n\n代码实现：\n\n```\ndef post_order(T):\n    if T.left:\n        post_order(T.left)\n    if T.right:\n        post_order(T.right)\n    print(T.value, end=' ')\n```\n\n由于遍历需要访问的节点数为 n，所以无论是哪种遍历方式，其复杂度均为 Θ(n)。"},{"title":"快速排序","url":"/study/algorithm/divide-and-conquer/quick-sort/","content":"\n## Lomuto 划分\n\n前面我们学习了分治的思想，将一个大问题划分为若干个小问题，然后逐个解决小问题，最后将小问题的解合并成大问题的解。这一节的内容，我们还会用到这种方法来解决排序问题，但这次我们更强调“分”。\n\n在对一个无序的数组进行排序的过程中，我们总是想要将小的元素放在数组的左侧，而将大的元素放在数组的右侧。Lomuto 划分就是一种将数组分为两部分的算法，其中较小部分的所有元素均小于某个中间值，较大部分均大于等于某个中间值，我们把这个中间值叫做**中轴**（**pivot**）。\n\n![中轴](https://infinityglow.github.io/study/algorithm/divide-and-conquer/quick-sort/images/pivot.png)\n\n那么具体该怎样实现呢？让我们以下面的数组为例吧。首先，我们需要选择一个值当做中轴，这里选择数组的第一个元素。\n\n![array](https://infinityglow.github.io/study/algorithm/divide-and-conquer/quick-sort/images/array.png)\n\n选好中轴之后，我们需要把中轴放到相应的位置，使得中轴左边的元素小于中轴，中轴右边的元素大于或等于中轴。这里我们要用到两个变量 `i` 和 `s`，`i` 从第二个元素开始递增。在每次循环中，如果 `array[i]` 的值小于中轴，`s` 向右移动一个单位，然后交换 `array[s]` 和 `array[i]` 的值，使得比中轴小的值尽可能放在数组的左边。最后，我们交换 `pivot` 和 `array[s]` 的值，这样中轴就放置到了相应的位置了。\n\n![动画](https://infinityglow.github.io/study/algorithm/divide-and-conquer/quick-sort/images/animation.gif)\n\n代码实现：\n\n```\ndef partition(a):\n    l = 0; r = len(array) - 1\n    pivot = a[l]\n    s = l\n    for i in range(l+1, r+1):\n        if a[i] < p:\n            s += 1\n            a[s], a[i] = a[i], a[s]  # 交换 a[s] 和 a[i] 的值\n    a[l], a[s] = a[s], a[l]  # 交换 a[l] 和 a[s] 的值\n    return s\n```\n\n由于代码中只有一个 `for` 循环，基本操作执行了 n - 1 次，所以 Lomuto 划分的时间复杂度为 Θ(n)。\n\n## 快速排序\n\n我们知道，执行一次 Lomuto 划分后数组分为了两部分。要想让整个数组有序，我们只需要使左半部分和右半部分分别有序即可。假设中轴插入的位置恰好是数组的正中间，我们可以用一个关系式来表达快速排序中 Lomuto 划分的过程。\n\n![relation](https://infinityglow.github.io/study/algorithm/divide-and-conquer/quick-sort/images/relation0.png)\n\n如果数组的长度小于等于 1 就直接 `return`，也就是我们这里的递归出口。快速排序就是递归地执行划分数组的一个过程。\n\n![relation](https://infinityglow.github.io/study/algorithm/divide-and-conquer/quick-sort/images/relation1.png)\n\n因此，我们就可以写出快速排序的 Python 代码：\n\n```\ndef quick_sort(array):\n    # 数组长度至少大于 1\n    if len(array) > 1:\n        s = partition(array)\n        array[0: s] = quick_sort(array[0: s])  # 左半部分\n        array[s+1: len(array)] = quick_sort(array[s+1: len(array)])  # 右半部分\n    return array\n```\n\n最后，让我们用一个动画（来源于[网络](https://visualgo.net/en/sorting)）来直观地感受一下吧！\n\n![demo](https://infinityglow.github.io/study/algorithm/divide-and-conquer/quick-sort/images/avg.mp4)\n\n## 复杂度分析\n\n下面来分析一下快速排序的复杂度。前面已经提到过，如果中轴恰好将一个数组对等地分为了两部分，再根据递归关系式：t(n) = 2t(n/2) + n - 1。我们就可以知道，在最好情况下，其复杂度为 Θ(nlog n)。\n\n如果数组正好是升序排列的，因为每个元素都小于等于它右侧的元素，所以在每次划分中，中轴并没有将数组对半分，而是将长度为 n 的数组分成了长度为 1 和长度 n - 1 的数组。因此，递归关系式应该这样写：t(n) = t(n-1) + n - 1，最后推导出复杂度为 Θ(n<sup>2</sup>)，即最坏情况下的复杂度。\n\n让我们用一个动画（来源于[网络](https://visualgo.net/en/sorting)）来直观地感受一下这种情况下的快速排序。\n\n![demo](https://infinityglow.github.io/study/algorithm/divide-and-conquer/quick-sort/images/worst.mp4)\n\n从动画中可以看出，我们对中轴的选择尤为重要，除了简单粗暴地选择数组的第一个元素之外，我们还可以从数组中随机挑选一个元素，也可以取第一个元素，中间元素和最后一个元素中的中位数，来尽可能地保证中轴能够对半分数组，用以达到分治的效果。\n\n我在这节的[代码]()中还模拟了平均情况和最坏情况下比较次数和运行时间，最后的结果如下：\n\n```\n平均情况：\n        平均比较次数： 157748\n        平均运行时间：0.0347 s\n最坏情况：\n        平均比较次数： 49995000\n        平均运行时间：6.5079 s\n```\n"},{"title":"归并排序","url":"/study/algorithm/divide-and-conquer/merge-sort/","content":"\n## 分治\n\n**分治**（**divide and conquer**）是算法里面的一种经典思想。与减治的通过缩小问题规模来解决问题而有所不同的是，分治采用的是，将一个大问题分成若干个小问题，然后逐一解决小问题，最后将小问题的解合并为原来大问题的解。听起来很抽象？没关系，下面的一个例子你马上就能明白。\n\n小的时候我们一定都玩过类似乐高的玩具，当你花了几天的时间拼出一件作品时，你一定是满满的成就感吧。其实，我们在整个拼接的过程中都用到了分治的思想。要想拼出一栋房子，我们肯定不会一个零件一个零件地拼，而是先将这些零件组合成一个个的小部件（例如屋顶，窗户等），然后把这些小部件拼接起来，组成更大的部件，然后这些大部件再相互拼接，最后拼出一件完整的作品。\n\n![玩具](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/toy.png)\n\n假设整个作品由 n 个零件组成，它由 b 个规模相等的小部件拼接而成，拼接需要 a 次完成，一次拼接的时间为 f(n)。所以，根据这个关系，我们可以写出下面的关系式：\n\n![formula](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/formula0.png)\n\n其中，f(n) 是关于 n 的一个函数，这里我们可以把它视作一次拼接所需要的时间复杂度，Θ(n<sup>d</sup>)，d 代表复杂度的阶数。我们根据 a, b, d 的取值，再借助下面的公式，就可以计算出不同情况下的复杂度了：\n\n![formula](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/formula1.png)\n\n公式具体的证明过程在 [Levitin](https://doc.lagout.org/science/0_Computer%20Science/2_Algorithms/Introduction%20to%20the%20Design%20and%20Analysis%20of%20Algorithms%20%283rd%20ed.%29%20%5BLevitin%202011-10-09%5D.pdf) 算法书的附录里，大家感兴趣可以去自己研究研究，后面的例子在计算复杂度的过程中会直接套用这里的公式。\n\n## 有序数组的合并\n\n将两个有序的数组合并成一个有序的数组就类似于将两个小部件的拼接过程，那具体该怎么实现呢？让我们来举一个例子吧。\n\n假设下面两个数组 a 和 b 要合并，我们需要一个空数组 c 来存放 a 和 b 中的元素。\n\n![array](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/array0.png)\n\n具体步骤如下：\n\n- 第一步：用三个变量 i, j, k 来分别记录数组 a，数组 b 和空数组 c 的索引值。\n\n![array](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/array1.png)\n\n- 第二步：比较 a[i] 和 b[j] 的大小，将小的元素存入到空数组 c 中。\n\n![array](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/array2.png)\n\n- 第三步：较小元素的数组索引值和数组 c 的索引值加 1。\n\n![array](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/array3.png)\n\n- 第四步：如果数组 a 和 b 的其中一个还未完全比较，执行第二步和第三步，否则执行第五步。\n\n![array](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/array4.png)\n\n- 第五步：将剩余数组的元素全部存入到数组 c 中。\n\n![array](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/array5.png)\n\n假设数组 a 和 b 分别有 n / 2 个元素，最好的情况要比较 n / 2 次，最坏的情况要比较 n - 1 次，所以合并有序数组的时间复杂度为 Θ(n)。\n\n## 归并排序\n\n我们知道，只有一个元素的数组一定是有序的，所以归并排序的第一步操作就是将数组对半分割，直到分割到只有一个元素为止。然后再将这些元素两两归并，最后使整个数组有序。因此，归并排序可以像下面这张图那样描述：\n\n![归并排序](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/merge-sort.png)\n\n代码由两个函数实现，一个是我们前面提到的合并有序数组。\n\n```\ndef merge(a, b):\n    i, j, k = 0, 0, 0  # 初始化变量\n    p, q = len(a), len(b)\n    c = [0 for i in range(p+q)]  # 建立空数组\n    while i < p and j < q:\n        if a[i] <= b[j]:\n            c[k] = a[i]; i += 1\n        else:\n            c[k] = b[j]; j += 1\n        k += 1\n    while i < p:\n        c[k] = a[i]\n        i += 1; k += 1\n    while j < q:\n        c[k] = b[j]\n        j += 1; k += 1\n    return c\n```\n\n另一个是我们的主函数 `merge_sort`。\n\n```\ndef merge_sort(array):\n    if len(array) <= 1:\n        return array\n    m = len(array) // 2  # 中间位置\n    A = merge_sort(array[: m])\n    B = merge_sort(array[m:])\n    C = merge(A, B)\n    return C\n```\n\n![demo](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/merge.mp4)\n\n最后来分析一下归并排序的复杂度。前面我们知道归并的复杂度为 Θ(n)，由于分割过程只执行一次基本操作，所以分割的复杂度为 Θ(1)，可以忽略不计。故我们可以得出下面的递归关系式：\n\n![relation](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/images/relation.png)\n\n这个关系式的含义是，对长度为 n 的数组进行排序的时间复杂度等于对两个长度为 n / 2 的数组排序的时间复杂度，再加上分割和归并的时间复杂度 Θ(n)。根据前面的公式，我们可以得到归并排序的时间复杂度为 Θ(nlog n)。由于归并过程需要长度为 n 的空数组，所以归并排序的空间复杂度为 Θ(n)。\n\n归并排序的时间复杂度已经超越了前面讲到的所有排序算法，但它需要牺牲存储空间来实现。后面要讲到的[计数排序](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/counting-sort/)也体现了这种思想。\n\n"},{"title":"插值查找","url":"/study/algorithm/decrease-and-conquer/interpolation-search/","content":"\n## 查字典\n\n如果要在字典里查找 “algorithm” 这个单词，我们肯定不会傻傻地像二分法那样从中间开始查找。相反，我们会从字母 A 开始查找，然后再根据第二个字母在字母表中的位置，找到相应的位置继续查找，这样反复这个过程，直到查到这个单词。\n\n在这一节的内容里，我们会实现类似于查字典的算法。\n\n## 插值查找\n\n**插值查找**（**interpolation search**）是二分查找的改良版。假设有这样一个数组 [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]，我们可以发现，每个相邻元素的差均为 10，呈现出均匀分布。如果要查找元素 70，我们首先可以计算出在数组中小于等于 70 的元素占所有元素的比例的期望值为 p = (70 - 0) / (90 - 0) = 7 / 9，而数组的长度 n 我们知道等于 10，所以我们期望查找的索引值就为 ⌊n × p⌋ = 7，对应的元素为 70，恰好就是我们要找的元素。\n\n这里，我们用一个公式来表示每次查找的期望索引值： \n\n![formula](https://infinityglow.github.io/study/algorithm/decease-and-conquer/interpolation-search/images/formula.png)\n\n其中，l 和 r 分别代表数组的第一个和最后一个索引，key 代表待查找的元素。\n\n跟二分查找一样，如果一次查找失败，数组的长度就相应地减小，再代入上面的公式继续查找，直到查找成功或失败。\n\n```\ndef formula(l, r, key, array):\n    p = (key - array[l]) / (array[r] - array[l])\n    n = r - l\n    idx = int(n * p)\n    return idx\n\ndef interpolation_search(array, key):\n    l = 0; r = len(array) - 1\n    while l <= r:\n        m = l + formula(l, r, key, array)\n        if array[m] == key:\n            return m\n        elif array[m] < key:\n            l = m + 1\n        else:\n            r = m - 1\n    return -1\n```\n\n## 复杂度分析\n\n插值查找的平均复杂度为 Θ(log log n)，但证明过程相当的复杂，这篇[论文](http://www.cs.technion.ac.il/~itai/publications/Algorithms/p550-perl.pdf)给出了详细的证明过程，感兴趣的同学可以自己去看看，这里我们就不再讨论了。\n\n要是数组不是均匀分布的，插值查找的复杂度会退化到线性的复杂度 Θ(n)。举一个极端的例子，假设数组为 [0, 99, 100, 100, 100]，我们要查找元素 99。第一轮查找我们计算出索引值为 3，第二轮为 2，第三轮为 1，这样我们查找了三次。推广到含有 n 个元素的数组就需要查找 n - 2 次，所以复杂度就为 Θ(n)。\n\n因此，插值查找的高效性只针对均匀分布的数组，而对于分布不均匀的数组，插值搜索就不再适用了。"},{"title":"二分查找与二叉树","url":"/study/algorithm/decrease-and-conquer/binary-search-tree/","content":"\n## 有序表的查找\n\n我们已经讨论过在一个数组中找到相应元素的算法，我们采用的是最简单，最直接的顺序搜索的方式，即从第一个元素开始，从左往右依次搜索，直到查找到该元素或查找失败。对于复杂度而言，我们也讨论过它的平均复杂度为 Θ(n)。那么还有没有更快的算法呢？这一节我们就来讨论一下吧～\n\n如果一个数组本身是有序的，查找一个元素就类似于在前面提到的[猜数问题](https://infinityglow.github.io/study/algorithm/brute-force/bubble-selection-sort/)，只是这里查找的元素变成了要猜的数。如果我们从数组的中间位置开始查找，在每轮查找过后，我们只需要在剩下一半的数组中再继续查找，这样以此类推，直到查找到该元素，或查找失败。\n\n下面来举一个具体的例子，假设我们的数组长这样：\n\n![数组](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/array0.png)\n\n下面查找元素 61，经过一轮的搜索过后，待搜索数组的长度变为原来的一半：\n\n![数组](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/array1.png)\n\n然后反复这个过程，最终找到 61：\n\n![数组](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/array2.png)\n\n我们只需要十行左右的代码就能轻松实现：\n\n```\ndef binary_search(array, key):\n    l = 0; r = len(array) - 1\n    while l <= r:\n        m = (l + r) // 2  # 向下取整\n        if key == array[m]:\n            return m\n        elif key < array[m]:\n            r = m - 1\n        else:\n            l = m + 1\n    return -1 \n```\n\n最后来分析一下时间复杂度。我们知道，每次查找操作，问题的规模都变为原来的一半。所以我们可以写出以下递归关系式。\n\n![derivation](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/derivation0.png)\n\n怎样来解这个关系式呢？这里我们不妨设 n = 2<sup>k</sup>，这样关系式就变为了 t(2<sup>k</sup>) = t(2<sup>k-1</sup>) + 1，然后再用回代的方式把它解出来。\n\n![derivation](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/derivation1.png)\n\n最后用 n 替换掉 k，得到复杂度为 Θ(log n)。\n\n## 二叉搜索树\n\n### 概念\n\n如果像下面这幅图一样，把一个有序数组变为二叉树的结构，然后用中间的元素当做树的根节点，左半部分的中间元素当做根节点的左孩子，右半部分的中间元素当做根节点的右孩子，这样以此类推，树的层数就会越来越多，这样我们就构建好了一颗二叉树。\n\n![conversion](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/conversion.png)\n\n我们可以看到，对于每个节点，左孩子都小于它的父节点，右孩子都大于等于它的父节点。所以我们把这样的二叉树叫做**二叉搜索树**（**binary search tree**）。\n\n### 搜索\n\n如果要搜索二叉树里面的元素（这里我们搜索节点 61），我们首先从根节点开始访问。\n\n![二叉树](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/tree0.png)\n\n假如根节点的元素就是我们要找的，那么就直接返回该节点；如果查找的元素比根节点大，我们就查找它的右子树。在这个例子里，61 比 48 要大，所以我们从 48 的右子树中搜索 61，这样问题就变成了从以 73 为根节点的二叉树中查找 61 这个元素。\n\n![二叉树](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/tree1.png)\n\n同理，如果查找的元素比根节点小，我们就查找它的左子树，这里 61 比 73 要小，所以问题又减小为了从以 61 为根节点的二叉树中查找 61 这个元素。\n\n![二叉树](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/tree2.png)\n\n这个过程，我们用递归就能够轻松地实现。在这之前，我们先要构建 `Node` 类和 `BST` 类：\n\n```\nclass Node(object):\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n```\n\n```\nclass BST(object):\n    def __init__(self):\n        self.root = None\n```\n\n然后用两个函数来实现：\n\n```\ndef search(self, value):\n    # 树的根节点为空就抛出异常\n    if not self.root:\n        raise ValueError(\"The tree is null\")\n    return self.search_node(self.root, value)\ndef search_node(self, root, value):\n    if not root:  # 如节点为空，则返回 None\n        return None\n    if value < root.value:  # 从左子树查找\n        return self.search_node(root.left, value)\n    elif value > root.value:  # 从右子树查找\n        return self.search_node(root.right, value)\n    else:\n        return root\n```\n\n除了搜索，二叉树还有插入和删除节点的操作。\n\n### 插入\n\n对于插入节点，我们首先要进行的操作是先找到插入的位置，即通过搜索操作找到插入位点。举一个例子，如果要插入节点 42，首先我们要先找到插入的位置（节点 34），因为 42 比 34 要大，所以 42 作为 34 的右节点。\n\n![二叉树](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/insert.gif)\n\n代码实现：\n\n```\ndef insert(self, value):\n    new_node = Node(value)  # 创建一个新节点\n    # 查看根节点是否为空\n    if not self.root:\n        self.root = new_node\n    else:\n        self.insert_node(self.root, new_node)\ndef insert_node(self, root, node):\n    if node.value < root.value:\n        if root.left:  # 从左子树查找\n            self.insert_node(root.left, node)\n        else:\n            root.left = node\n    else:\n        if root.right:  # 从右子树查找\n            self.insert_node(root.right, node)\n        else:\n            root.right = node\n```\n\n### 删除\n\n删除节点相对来说就要复杂一些，除了要找到删除的节点之外，我们还要对不同情况的节点执行不同的操作。\n\n- 情况 1：删除的为叶子节点\n\n    这种情况最简单，只需要找到该节点，然后删除即可，例如删除这里的 10。\n    \n![二叉树](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/delete1.gif)\n    \n- 情况 2：删除的节点只有左孩子或只有右孩子\n\n    这种情况下，只需要将原本指向它的节点指向它的子节点即可。像下面这幅图这样，我们要删除 34 这个节点，这时我们只需要将节点 25 的右孩子指向 34 的右孩子（节点 42）即可，最后删除掉节点 34 。\n    \n![二叉树](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/delete2.gif)\n    \n- 情况 3：删除的节点既有左孩子又有右孩子\n\n    这种情况是最麻烦的，删除节点后，要想保持原有二叉搜索树的性质，我们首先要从待删除节点的左子树中找到最大的节点，或者从右子树中找到最小的节点。这里以左子树为例，假设我们要删除节点 48，首先从它的左子树中找到最大节点 42，然后交换它们的值，这样 42 就成为了根节点，原先的 42 变为了这里的 48，最后我们删除节点 48。\n    \n![二叉树](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/delete3.gif)\n\n代码实现：\n\n`max` 函数\n\n```\ndef max(self, root):\n    if root.right:\n        return self.max(root.right)\n    return root\n```\n\n```\ndef remove(self, value):\n    # 根节点为空抛出异常\n    if not self.root:  \n        raise ValueError(\"树为空树\")\n    self.root = self.remove_node(self.root, value)\ndef remove_node(self, root, value):\n    if not root:\n        return None\n    # 查找\n    if value < root.value:\n        root.left = self.remove_node(root.left, value)\n    elif value > root.value:\n        root.right = self.remove_node(root.right, value)\n    else:\n        # 左右孩子为空\n        if not root.left and not root.right:\n            del root\n            return None\n        # 只有左孩子或右孩子\n        elif not root.left:\n            temp = root.right\n            del root\n            return temp\n        elif not root.right:\n            temp = root.left\n            del root\n            return temp\n        # 左右孩子都有\n        else:\n            temp = self.max(root.left)  # 找到待删除节点的左子树的最大节点\n            root.value = temp.value\n            root.left = self.remove_node(root.left, temp.value)\n    return root\n```\n\n### 复杂度分析\n\n二叉搜索树的复杂度取决于二叉树的结构。如果像上面的例子那样，复杂度就跟二分查找一样，为 Θ(log n)。假设二叉树长得像下面这张图的右边的树一样，我们会觉得它显得特别地不“平衡”，跟普通的链表没有什么区别，所以复杂度就退化为线性的复杂度 Θ(n)。\n\n![对比](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/images/comparison.png)\n\n因此，二叉树的平衡性显得就尤为重要，后面讲到的[AVL树](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/)和[红黑树](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/)会采用各自的机理，使树尽可能变得“平衡”。"},{"title":"拓扑排序","url":"/study/algorithm/decrease-and-conquer/topo-sorting/","content":"\n##  定义\n\n在大学里，每当到了期末的时候，你一定会头疼于各种选课给你带来的困扰。其中一项就是先修课，每次你高高兴兴地选了自己心仪的选修课，却发现自己不满足先修课的要求，只好默默地退掉。这一次，我们就来讨论一下这个问题。\n\n![选课](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/topo-sorting/images/enrol.jpg)\n\n假设大学里开设了这些课程：高等数学，线性代数，概率论，数据结构，机器学习和计算机视觉。它们存在这样的先修关系：线性代数的先修课为高等数学；概率论的先修课是线性代数；机器学习的先修课为线性代数、概率论和数据结构；最后计算机视觉的先修课是机器学习。\n\n下面我们需要把这些课程和关系做一个抽象化的处理，说白了就是能够让计算机读懂这些关系。这里我们用到了图这种数据结构，把每门课程当做图的节点，两门课的先修关系抽象为图的边，并且还是有向边。要是把它画出来就是这样子的：\n\n![图](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/topo-sorting/images/graph0.png)\n\n这里，我们对上图进行类似“排序”的操作，也就是怎样安排这些课程而不会因为没有修满先修课而不能选上，我们把这种操作叫做**拓扑排序**（**topological sorting**）。\n\n## 卡恩算法\n\n下面就是来解决这个问题了，它跟我们前面提到的减治有什么关系呢？假如我们要在某一个学期修计算机视觉这门课，我们首先要保证修了机器学习这门课，而要达到修机器学习这门课的条件，我们又要修三门前置课，这样问题的规模在逐渐减小，直到这门课没有任何先修条件，也就是我们可以在最开始选择的课。\n\n卡恩算法为我们提供了具体的步骤。\n\n- 步骤 1：在图中找到没有被其它节点所指向的节点，即入度（在图论中，入度指的是所有进入该节点的节点个数）为 0 的节点。\n\n![图](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/topo-sorting/images/graph1.png)\n\n- 步骤 2：删除该节点以及对应的边。\n\n![图](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/topo-sorting/images/graph2.png)\n\n- 步骤 3：重复步骤 1 和步骤 2，直到图中没有入度为 0 的节点。\n\n![图](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/topo-sorting/images/graph-demo.gif)\n\n最后我们用代码来实现一下：\n\n首先还是构建 Vertex 类：\n\n```\nclass Vertex(object):\n    def __init__(self, value):\n        self.value = value\n```\n\n然后找到入度为 0 的节点：\n\n```\ndef find_source(G):\n    source = None\n    V = G.keys(); E = G.values()\n    for v in V:\n        for neighbour in E:\n            for vertex in neighbour:\n                # 检查每条边\n                if vertex is v:\n                    break\n            else:\n                continue\n            break\n        else:\n            source = v\n            break\n    return source\n```\n\n最后是我们的主函数：\n\n```\ndef topo_main(G):\n    order = []\n    while len(G) != 0:\n        s = find_source(G)\n        if s is None:\n            return \"不存在拓扑排序.\"\n        order.append(s.value)\n        G.pop(s)\n    return order\n```\n\n在主程序中，我们先构建一个图 `G`，然后执行 `topo_main` 函数，最后得到排序后的结果：\n\n```\n>>> print(topo_main(G))\n>>> 排序结果为 [\"高等数学\", \"线性代数\", \"概率论\", \"数据结构\", \"机器学习\", \"计算机视觉\"]\n```\n\n其实，除了卡恩算法之外，我们还可以用 DFS 的方法。由于它跟减治的关系不是很大，这里就不做深入的讨论了，感兴趣的同学可以去[网上](https://www.geeksforgeeks.org/topological-sorting/)搜一搜这方面的内容。\n\n"},{"title":"插入排序","url":"/study/algorithm/decrease-and-conquer/insertion-sort/","content":"\n## 减治\n\n在这一章里面，我们会讨论一种解决问题的新方案。**减治**（**decrease and conquer**），全称叫做减而治之，是通过逐步缩小问题规模的方式来解决一个问题。可能你还不是很明白，别急，我们先上一个例子：假设 9 枚金币中有一枚是假的（假币的重量会比真币轻），现在给你一个天平，你需要用最少的次数找出假币，该怎么做呢？\n\n![天平](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/images/balance.jpg)\n\n你可以一枚一枚地称，但最多你可能要称 8 次才能称出来，不是很高效。其实我们可以将这些金币对半分，即 9 枚金币分为 4 枚和 5 枚。因为天平两边的金币数要是相同的我们才能判断哪边轻，所以这里需要从 5 枚里取出 1 枚。这样如果拿出的那一枚不是假币的话，我们只需要从较轻的 4 枚中找到假币，于是又可以对半分，称量之后，再从其中较轻的两枚中选择。这样以此类推，直到找出假币。\n\n我们可以看到，原问题是从 9 枚金币中找出假币，经过一次称量之后，问题变为了从 4 枚金币中找出假币，再次称量后问题的规模变为了 2，最后变为 1，即找到假币。这就是减治法的精髓了：一个大问题被一次又一次地拆分成了更小的问题，直到我们能够直接解决小问题。\n\n## 插入排序\n\n小时候你一定玩过扑克牌，当你在整理那些乱序的牌的时候，你脑海里想到的第一种方法一定是将这些新摸到的牌插入到对应的位置，以此来达到手牌有序。\n \n![扑克](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/images/poker.png)\n\n让我们以数组为例，假设有下面的数组：\n\n![数组](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/images/array0.png)\n\n我们可以看到，前面 3 个元素已经有序，后面 7 个元素还是无序状态，这时候我们需要将第 4 个元素插入到相应的位置，这样有序序列就从 3 增加到 4，无序序列就从 7 减小到 6。\n\n![数组](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/images/array1.png)\n\n如果对于一个含有 n 个元素的完全无序的数组，我们可以把第一个元素视作有序，其余 n - 1 个元素视为无序，因为只有一个元素的数组肯定是有序的，所以每次将无序序列的第一个元素插入到相应位置后，问题的规模就减少 1。这样我们重复 n - 1 次，数组就从无序变为有序了。\n\n最后我们用代码实现一下插入排序。\n\n```\ndef insertion_sort(array):\n    for i in range(1, len(array)):\n        v = array[i]\n        j = i - 1\n        while j >= 0 and array[j] > v:\n            array[j+1] = array[j]  # 用前一个元素覆盖当前元素\n            j = j - 1\n            cnt += 1\n        array[j+1] = v  # 循环结束后，将 v 插入到相应位置\n    return array\n```\n\n## 复杂度分析\n\n### 最好情况\n\n插入排序的复杂度分析不同于之前讲到冒泡和选择排序，我们需要考虑不同实例下的数组。假设一个数组已经有序，在每次的 `for` 循环里，每个待插入的元素只需要跟自己前面的元素比较后就完成一次插入操作，即每个元素只比较一次。所以，对于一个含有 n 个元素的数组来讲，我们只需要比较 n - 1 次。\n\n![推导](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/images/derivation0.png)\n\n所以复杂度就为 Θ(n)。我们用一个动画来直观地感受一下。\n\n![demo](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/images/insertion-best.gif)\n\n### 最坏情况\n\n如果数组是倒序排列的，那么每次待插入的元素都要插入到第一个位置（假设数组每个元素是唯一的）。也就是说，`for` 循环每执行一次，需要比较的次数就为 `i` 次，所以根据这个关系我们可以得出下面的式子：\n\n![推导](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/images/derivation1.png)\n\n复杂度为 Θ(n<sup>2</sup>)，从动画时间的长短我们就可以看出，这种情况下算法的效率是比较低的。\n\n![demo](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/images/insertion-avg.gif)\n\n### 平均情况\n\n平均情况的复杂度分析相对来说就要稍微困难一点了，这里我们用基于概率的方式计算，对于第 `i` 个元素，它能够插入的位点有 `i` 处，每一处需要比较的次数分别为 1, 2, ..., i。像下面这张图一样：\n\n![数组](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/images/array2.png)\n\n假设每个位点插入的概率相同，我们就可以得到下面的式子：\n\n![推导](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/images/derivation2.png)\n\n这说明了对于一个随机数组，第 `i` 个元素插入到相应位置所需要的平均比较次数为 (i+1)/2 次。对于整个数组而言，待插入的元素是从第 2 个开始，一直遍历到 n，所以只要把它们全部加起来，就能得到平均复杂度了。\n\n![推导](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/images/derivation3.png)\n\n我们看到，虽然时间复杂度也为 Θ(n<sup>2</sup>)，但是前面的系数变为了 1/4，说明平均情况的时间开销要低于最坏情况。为了证明这一结论，我在[本节的代码](https://github.com/infinityglow/Algorithm-and-Complexity/tree/master/Decrease%20and%20Conquer/Insertion%20Sort)里分别模拟了最好、最坏和平均情况所需要的比较次数和运行时间。下面给出部分结果：\n\n```\n最好情况：\n    平均比较次数： 9999\n    平均运行时间：0.0024 s\n最坏情况：\n    平均比较次数： 24927061\n    平均运行时间：4.4334 s\n平均情况：\n    平均比较次数： 49999962\n    平均运行时间：8.6132 s\n```\n\n从上面的结果来看，不论是比较次数还是运行时间，最好情况都要远远优于后面两种情况。所以如果数组是有序的或几乎有序的，采用插入排序会大大降低排序的时间，甚至比后面要讲到的排序算法还要快。\n\n"},{"title":"暴力搜索","url":"/study/algorithm/brute-force/exhaustive-search/","content":"\n## 回顾\n\n在[图的遍历](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/)那一节里面我们知道，不管是深度优先遍历还是广度优先遍历，本质上都是对图中的每一个节点执行一次遍历算法，即穷尽所有的情况。这次要讲到的**暴力搜索**（**exhaustive search**）就是专门为了解决这类问题而产生的。除了图的遍历，暴力搜索还以解决[组合问题](https://infinityglow.github.io/study/algorithm/basic-knowledge/problem-types/)为主。下面我们一起来看看吧～\n\n## 背包问题\n\n背包问题是一道典型的组合问题，问题可以这样来描述它：给定一些物品，这些物品都有它自己的重量 w<sub>i</sub> 和价格 v<sub>i</sub> ，并且每个物品只能选择一次。现在有一个能装 W 重量的背包，你需要尽可能多的使背包内物品的价格最大。\n\n![背包问题](https://infinityglow.github.io/study/algorithm/brute-force/exhaustive-search/images/knapsack.png)\n\n暴力搜索的方式就是将这个问题的全部组合一一列举出来，即穷举集合 {w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>i</sub>, ..., w<sub>n</sub> } 中所有的子集，然后判断重量是否超过了背包上限，如果没有，就分别计算它们的价格，最后选出最高价格的组合。\n\n下面来举一个具体的例子。假设这里的 W = 10，w<sub>1</sub> = 7, v<sub>1</sub> = 42; w<sub>2</sub> = 3, v<sub>2</sub> = 12; w<sub>3</sub> = 4, v<sub>3</sub> = 40; w<sub>4</sub> = 5, v<sub>4</sub> = 25。那么我们根据实例，列举出所有的情况：\n\n| 子集 | 总重量 | 总价格 |\n| :----: | :----: | :----: |\n| ∅ | 0 | 0 |\n| {1} | 7 | 42 |\n| {2} | 3 | 12 |\n| {3} | 4 | 40 |\n| {4} | 5 | 25 |\n| {1, 2} | 10 | 54 |\n| {1, 3} | 11 | 不符合要求 |\n| {1, 4} | 12 | 不符合要求 |\n| {2, 3} | 7 | 52 |\n| {2, 4} | 8 | 37 |\n| {3, 4} | 9 | 65 |\n| {1, 2, 3} | 14 | 不符合要求 |\n| {1, 2, 4} | 15 | 不符合要求 |\n| {1, 3, 4} | 16 | 不符合要求 |\n| {2, 3, 4} | 12 | 不符合要求 |\n| {1, 2, 3, 4} | 19 | 不符合要求 |\n\n总共有 2<sup>4</sup> = 16 种情况，如果我们取出那些符合要求的情况，选择价格最大的一种，问题就解决了，最后的结果为：{w<sub>3</sub>, w<sub>4</sub>} 这种组合，其价格为 65。\n\n如果要写成代码，我们首先要构建一个从集合 {w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>, w<sub>4</sub>} 中产生超集（集合的全部子集所构成的集合）的函数 `power_set_gen`。\n\n```\ndef power_set_gen(w):\n    p_set = [set()]\n    for item in w:\n        temp_set = []\n        for subset in p_set:\n            subset = subset | {item}  # 将新元素加入到原有的子集中\n            temp_set.append(subset)\n        p_set.extend(temp_set)\n    return p_set\n```\n\n构建好超集后，就可以在 `knapsack` 函数里面枚举所有的情况啦。\n\n```\ndef knapsack(w_v, p_set, W):\n    w_combination = None; max_value = 0  # 分别用来记录最优情况的组合和最大值\n    for instance in p_set:\n        weight, value = 0, 0  # 累加器\n        for item in instance:\n            weight += item; value += w_v[item]\n        if weight <= W and value > max_value:\n            w_combination = instance\n            max_value = value\n    return w_combination, max_value\n```\n\n最后求得的结果跟上面的一模一样。\n\n```\n>>> python knapsack.py \n>>> 最佳组合为物品 3 和物品 4，其价格为 65。\n```\n\n关于复杂度的问题，只要你知道一个含有 n 个元素的集合的子集数为 2<sup>n</sup>，就能轻松得出复杂度为 Θ(2<sup>n</sup>)了。\n\n## 分配问题\n\n分配问题是另一道经典的组合问题，问题的描述为一家公司有 n 名员工，现在有 n 件任务需要分配给这些员工，一件任务只能分配给一名员工，每名员工完成每件任务都有对应的时间，现在要我们来分配这些任务，最后使总时间最短。\n\n![分配问题](https://infinityglow.github.io/study/algorithm/brute-force/exhaustive-search/images/assignment.png)\n\n我们将上面的图用一个表格来表示：\n\n|  | Task 1 | Task 2 | Task 3 | Task 4|   \n| :----: | :----: | :----: | :----: | :----: |\n| Staff 1 | 9 | 2 | 7 | 8 |\n| Staff 2 | 6 | 4 | 3 | 7 |\n| Staff 3 | 5 | 8 | 1 | 8 |\n| Staff 4 | 7 | 6 | 9 | 4 |\n\n下面我们看看具体该如何来分配。任务 1 可以分配给 4 名员工中的任意一位，当任务 1 分配给一名员工之后，任务 2 就只能分配给剩下的 3 名员工中的一位了，然后任务 3 分配给剩下两名中的一位，最后任务 4 交给最后未分配的员工。所以我们可以根据前边的描述，穷尽所有的情况，将最优情况用“暴力”的手段搜索出来。于是我们可以写出相应的代码。\n\n首先需要有一个生成全排列的函数，来指定每个员工对应的任务序号。\n\n```\ndef perm_gen(source):\n    permutation = [[source[0]]]\n    for i in range(1, len(source)):\n        temp = [] \n        for p in permutation:\n            for j in range(len(p), -1, -1):\n                p.insert(j, source[i])\n                temp.append(p.copy())\n                p.pop(j)\n        permutation = temp.copy()\n    return permutation\n```\n\n```\n>>> print(perm_gen([1, 2, 3]))\n>>> [[1, 2, 3], [1, 3, 2], [3, 1, 2], [2, 1, 3], [2, 3, 1], [3, 2, 1]]\n```\n\n然后是我们的主函数：\n\n```\ndef assignment_bf(perm, table):\n    combination = None; min_cost = np.inf  # 分别用来记录最有情况的组合和最小值\n    for i in range(len(perm)):\n        temp_cost = 0  # 累加器\n        for j, k in enumerate(perm[i]):\n            temp_cost += table[j][k-1]\n        if temp_cost < min_cost:\n            combination = perm[i]\n            min_cost = temp_cost\n    return combination, min_cost\n```\n\n最后输出结果。\n\n```\n>>> python assignment.py\n>>> 最佳分配方案为：\n>>> 员工 1 做任务 2，员工 2 做任务 1，员工 3 做任务 3，员工 1 做任务 4。\n>>> 最短时间为 13\n```\n\n如果我们把上面问题的规模扩大到 n，按照上面的方式，我们就很容易得出一共有 n! 种情况，所以暴力求解分配问题的复杂度自然就为 Θ(n!) 了。\n\n我们可以看到，用暴力搜索的方式解决这两类问题，复杂度分别达到了指数级和阶乘级。要知道，就算当问题的规模很小（n = 20）时，计算机都不能在有限的时间内将它们计算出来，尤其是阶乘级的复杂度，因为 20! 就已经达到了 2.4 × 10<sup>18</sup> 这么大，更何况在现实中，n 都是 10<sup>6</sup> 起步的。所以，用暴力搜索解决组合问题显然是不切实际的。后面的[动态规划](https://infinityglow.github.io/study/algorithm/dynamic-programming/knapsack-problem/)和[匈牙利算法](https://infinityglow.github.io/study/algorithm/hungarian-algorithm/)会给出更为优质的算法来解决这类问题。"},{"title":"最近点对与凸包问题（BF）","url":"/study/algorithm/brute-force/clo-pair-con-hull/","content":"\n## 最近点对问题\n\n这一节内容，我们来看看两个经典的几何学问题。首先是**最近点对问题**（**closest-pair problem**）。问题的描述是在一个二维平面上有一些随机分布的散点，要求我们在这些散点里找到两个点，使得它们的距离最小。\n\n![最近点对](https://infinityglow.github.io/study/algorithm/brute-force/clo-pair-con-hull/images/clo-pair0.png)\n\n有解决这个问题很简单，只需要把所有的情况都列举出来，即每一个点与其他所有节点的距离都计算出来，然后取最小的值。所以，这里我们再次用到了暴力求解的方法。\n\n```\ndef closest_pair_bf(X, Y):\n    d = np.inf  # 初始化 d 为无穷\n    for i in range(len(X)-1):\n        for j in range(i+1, len(X)):\n            d = min((X[i] - X[j])**2 + (Y[i] - Y[j])**2)\n    return d\n```\n\n其中参数 `X` 和 `Y` 分别表示所有散点的横纵坐标的集合。在主程序中，我们用 [`numpy.random.normal`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html) 方法生成随机数，并且用[`np.column_stack`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html)方法将它们打包。\n\n```\nnp.random.seed(13)  # 为了代码的可复现性\nX = np.random.normal(2, 0.5, size=[16, 1])\nY = np.random.normal(2, 0.5, size=[16, 1])\nXY_pair = np.column_stack((X, Y))\n```\n\n如果平面上有 n 个点，那么要考虑的点对数就为 n(n-1)/2。比如这里一共有 16 个点，我们就要考虑 120 种情况。\n\n![最近点对](https://infinityglow.github.io/study/algorithm/brute-force/clo-pair-con-hull/images/clo-pair1.png)\n\n关于算法的复杂度，我们很容易就能推导得出为 Θ(n<sup>2</sup>)。最后我们标注出最近点对在图中的位置。\n\n![最近点对](https://infinityglow.github.io/study/algorithm/brute-force/clo-pair-con-hull/images/clo-pair2.png)\n\n## 凸包问题\n\n**凸包问题**（**convex-hull problem**）是这里要讨论的第二个问题。首先我们先要弄明白什么是凸包，在生活中，你一定见过在一个扎满钉子的木头上套上一圈橡皮筋的场景，像下面这张图那样。哈哈，实际上在这里橡皮筋围成的多边形就是一个凸包了。\n\n![橡皮筋](https://infinityglow.github.io/study/algorithm/brute-force/clo-pair-con-hull/images/rubber.png)\n\n从严格意义上来讲，凸包指的是由数个点构成的凸多边形（每个内角都小于180度），并且这个多边形能够把这个平面上所有的点都囊括在内。就像上面那张图中被橡皮筋围住的钉子一样。所以，我们怎么来解决这个问题呢？答案当然还是采用最“暴力”的手段。穷举所有的情况，找到我们所需要的多边形。\n\n从形成的凸包多边形我们可以观察到，对于多边形的边所形成的直线，其他所有的节点都在这条直线的同一侧。于是我们可以根据这个特点写出代码：\n\n```\ndef convex_hull_bf(XY_pair):\n    pairs = []  # 用于存储满足条件的点\n    for i in range(len(XY_pair) - 1):\n        for j in range(i+1, len(XY_pair)):\n            a = XY_pair[j][1] - XY_pair[i][1]\n            b = XY_pair[i][0] - XY_pair[j][0]\n            c = XY_pair[i][0] * XY_pair[j][1] - XY_pair[i][1] * XY_pair[j][0]\n            k = 0; pos = 0; neg = 0  # pos 和 neg 用于统计直线左右两侧点的个数          \n            while k < len(XY_pair):\n                if k != i and k != j:\n                    if a * XY_pair[k][0] + b * XY_pair[k][1] - c > 0:\n                        pos += 1\n                    else:\n                        neg += 1\n                    if pos != 0 and neg != 0:\n                        break\n                k += 1\n            else:\n                pairs.append((XY_pair[i], XY_pair[j]))\n    return pairs\n```\n\n对于判断一个点是在一条直线的左侧还是右侧，我们会用到了下面的公式：\n\n![公式](https://infinityglow.github.io/study/algorithm/brute-force/clo-pair-con-hull/images/formula.png)\n\n其中 a = y<sub>2</sub> - y<sub>1</sub>, b = x<sub>1</sub> - x<sub>2</sub>, c = x<sub>1</sub>y<sub>2</sub> - y<sub>1</sub>x<sub>2</sub>，x 和 y 分别为需代入的点。如果值大于 0，那么该点在直线的左侧；如果小于 0，那么该点在直线的右侧。\n\n最后我们将满足条件的点通过收尾相连的方式显示出来，这样我们就能画出多边形了。\n\n![凸包](https://infinityglow.github.io/study/algorithm/brute-force/clo-pair-con-hull/images/con-hull.png)\n\n作图的代码如下：\n\n```\ndef plotting(xy_pair, polygon):\n    plt.scatter(xy_pair[:, 0], xy_pair[:, 1], s=15)\n    for i in range(len(polygon)):\n        np_point_stack = np.stack(polygon[i], axis=1)  # 将每一个点对打包成 (xi, yi) 的形式\n        plt.plot(np_point_stack[0], np_point_stack[1], c='r')\n    plt.show()\n```\n\n最后来分析一下复杂度。我们知道，如果平面上有 n 个点，那么一共就有 n(n-1)/2 个点对，也就是直线的条数。对于每一条直线，又要从剩下 n - 2 个点中判断是否在直线的同一侧。这样一来，只要我们列出求和公式，再经过推导就能够得到复杂度为 Θ(n<sup>3</sup>)。这里我就偷个懒，不详细写出来了～\n\n## 总结\n\n我们可以看到，用暴力求解的办法解决几何问题依然是那么简单、直接，但效率也是依然的低。所以这两种算法也只能解决小规模的问题。面对 n > 10<sup>6</sup> 这种大规模的问题时，我们就要求救[其他算法](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/)了。\n\n\n\n"},{"title":"图的两种遍历","url":"/study/algorithm/brute-force/graph-traversal/","content":"\n## 邻接矩阵和邻接表\n\n在讲[数据结构](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/)的那一节里我们提到了图这种数据结构，并且介绍了两种表示方法：邻接矩阵和邻接表。接下来我们来分别看看这两种方法是如何表示一张图的吧。\n\n和树这种数据结构不同，图的节点之间的连接是自由的，即一个节点可以连接其他任意节点（包括自身），并且是具有方向性的。所以我们可以根据这样的特性，用 0 和 1 来表示一个节点 u 是否能够到达另一个节点 v。假设一个图由 n 个节点组成，那么我们就有 n<sup>2</sup> 种可能的连接。为了方便，我们用一个 n × n 的矩阵来表示：\n\n![邻接矩阵](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/matrix.png)\n\n根据上面的矩阵，我们可以得知节点 a 可以到达节点 c，而节点 d 却不能到达节点 c。如果我们考虑矩阵中的每一项，根据其连接关系，我们就可以画出这个图：\n\n![图](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/graph0.png)\n\n如果我们用一个链表的形式来表示一个节点到与之相连的节点，那么我们就可以得到一个由多个链表构成的数组。举个例子，上面的图中，节点 b 对应的链表中含有节点 c 和 f，所以节点 b 是可以直接到达节点 c 和节点 f 的。同样，由于我们的图是一个无向图，所以节点 c 和节点 f 分别对应的链表也是含有节点 b 的。根据整个邻接表，我们也可以画出和上面相同的图。\n\n![邻接表](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/list.png)\n\n## 深度优先遍历\n\n在了解两种遍历算法之前，我们先要知道什么是遍历？**遍历**（**traversal**）是对树（图）这种相对复杂的数据结构中的每一个节点进行逐一访问的过程。换句话说就是我们能够找到一条路，将所有节点都“走”过一遍。\n\n**深度优先遍历**（**depth-first search**）是图的一种遍历方式。既然是深度优先遍历当然是跟深度有关系啦。具体过程：假设从一个节点出发，我们需要尽可能远地探索其他分支，直到遇到“**死胡同**”（**dead end**）。例如下面的图，我们从节点 a 开始遍历，由于算法是尽可能远地探索那些节点，所以当我们探索到“死胡同”的时候，走过的节点就为 a-b-d-e (分支节点按照字母表顺序进行优先选择)。\n\n![图](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/graph1.png)\n\n节点 e 被遍历后，我们需要退回到节点 d，再尽可能地往更远的地方走，直到再次遇到“死胡同”。所以节点 c 和节点 f 是下一次被遍历的两个节点。\n\n![图](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/graph2.png)\n\n然后重复这个过程，直到退回到了节点 a。显然，这个过程需要用递归或者栈来实现的（因为后来的节点先离开），要实现这个代码，首先我们先要用一个类来表示图中的节点。\n\n```\nclass Vertex(object):\n    def __init__(self, value, visited):\n        self.value = value\n        self.visited = False\n        self.neighbours = []  # 记录所有与之相邻的节点\n```\n\n然后实现一下深度优先遍历的过程：\n\n```\ndef dfs(v):\n    v.visited = True\n    for w in v.neighbours:\n        if not w.visited:\n            w.visited = True\n            dfs(w)\n```\n\n我们看到，由于节点 g 和节点 h 没有与左边的图连通，所以我们需要再构建一个函数用来遍历所有节点是否被访问过，如果没有，我们就从该节点开始再进行遍历。\n\n```\ndef DFS(G):\n    V = G.keys()\n    for v in V:\n        if not v.visited:\n            dfs(v)\n```\n\n最后我们遍历完全部的节点。\n\n![图](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/graph3.png)\n\n## 广度优先遍历\n\n和深度优先遍历一样，**广度优先遍历**（**breadth-first search**）也是从一个节点出发，遍历所有节点。但不一样的是，我们这次是尽可能广地遍历其他邻近的节点。还是拿刚刚的图来做例子，当节点 a 被访问后，节点 b 和节点 c 会紧接着被访问。\n\n此时节点 a 的所有邻近节点全部被访问了，所以这里只需要考虑节点 b 和节点 c 的邻近节点，因此下一次被访问的节点就会有 d 和 f，然后我们反复这个过程，最终遍历全部节点。\n\n这种先进来的节点先离开的形式我们可以用一个队列来实现：\n\n```\ndef bfs(v):\n    v.visted = True\n    queue = []  # 初始化一个空队列\n    queue.append(v)  # v 入队\n    while queue:\n        for w in v.neighbours:\n            if not w.visited:\n                w.visited = True\n                queue.append(w)\n        queue.pop(0)  # v 出队\n```\n\n同样，对于那些没有相互连接的图，我们处理的方式是一样的：\n\n```\ndef BFS(G):\n    V = G.keys()\n    for v in V:\n        if not v.visited:\n            bfs(v)\n```\n\n完整的图解如下：\n\n节点 a 入队\n\n![图](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/graph4.png)\n\n节点 b, c 入队，节点 a 出队\n\n![图](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/graph5.png)\n\n节点 d, f 入队，节点 b, c 出队\n\n![图](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/graph6.png)\n\n节点 e 入队，节点 d, f 出队\n\n![图](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/graph7.png)\n\n节点 g 入队\n\n![图](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/graph8.png)\n\n节点 h 入队，节点 g 出队。再经过一次 `while` 循环后节点 h 出队，最后遍历完成。\n\n![图](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/images/graph9.png)\n\n那么图的遍历跟暴力求解有什么关系呢？事实上，不管是深度优先遍历还是广度优先遍历，我们可以发现，其实每一个节点在被访问的过程中都执行了一次遍历算法，我们正是通过这种方式，才保证了图的每一个节点能够被访问，达到遍历的效果。后面我们讲到的[暴力搜索](https://infinityglow.github.io/study/algorithm/brute-force/exhausitive-search/)也是运用到了这么一点性质。\n\n图的复杂度怎么计算呢？我们知道图可以由邻接矩阵或邻接表来表示，所以我们要分别来讨论：\n\n如果我们用邻接矩阵来表示，在执行遍历算法的过程中，由于每个节点都要查看其他节点是否与自己相连，即矩阵中每一行的 0 和 1，所以对于一个有 |V| 个节点的图来讲，就要查看 |V|<sup>2</sup> 次，所以用邻接矩阵来表示的图，其两种遍历的复杂度均为 Θ(|V|<sup>2</sup>)。\n\n邻接表由于只记录了每个节点和与之相连节点的信息，即每条边的信息。所以在遍历的过程中，我们需要 |V| + |E| 次，其中 |V| 表示判断节点是否已经被访问过的次数，|E| 表示一个图里边的条数（具有两个方向的边算作两次）。所以用邻接表表示的图，其两种遍历的复杂度均为 Θ(|V|+|E|)。\n"},{"title":"字符串匹配","url":"/study/algorithm/brute-force/string-matching/","content":"\n## 回顾\n\n前面我们学习了两种低效的排序算法，这次我们来看看一个关于搜索的问题：**字符串匹配**（**string matching**）。在[算法的问题类型](https://infinityglow.github.io/study/algorithm/basic-knowledge/problem-types/)那一节里面我们提到了这个问题，这好比你在浏览网页或者在看电子书的时候按下“Ctrl+F”键，然后你输入一个单词或一段话，它就在原文中自动帮你找到那些匹配的文字。\n\n![matching](https://infinityglow.github.io/study/algorithm/brute-force/string-matching/images/matching.png)\n\n上图截自[《算法导论》](http://kddlab.zjgsu.edu.cn:7200/students/lipengcheng/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%EF%BC%88%E8%8B%B1%E6%96%87%E7%AC%AC%E4%B8%89%E7%89%88%EF%BC%89.pdf)的某一页，我在我自己的电脑里试了一下，搜索的是单词“algorithm”，几秒钟就全部搜出来了，可以说速度是相当快的，要知道这本书可是有 1000 多页的呀，这得益于一种高效的算法帮我们能够在短时间内完成复杂的工作。\n\n这类问题似乎很简单，但这也是困扰了科学家们多年，直到现在对于这种算法的探究都还没有停止，因为解决这个问题并不难，我们只需要用最“暴力”的办法就可以解决掉它，但要找到一种高效的算法却是难上加难。我们将在[后面](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/string-matching/)的章节里简单介绍它们。\n\n\n\n## 一个例子\n\n这里我们举一个更加有意思的例子。懂一点生物的小伙伴们肯定知道，DNA 探针（一段短的 DNA 序列）在与一段长的 DNA 分子杂交的时候，它们的碱基一定会遵循配对原则，即：A 跟 T 配对，C 跟 G 配对。对于这个问题，我们可以抽象成一个算法问题，也就是从一个含有“ATCG”的字符串中查找一个**模式**（**patten**）字符串，比如“GAATTC”。\n\n如果我们用暴力求解的方式，也就是从字符串的起始位置开始，与模式字符串的第一个字符对齐，进行比对。如果比对成功，就再比较下一个字符，直到全部比对成功。如果失败，模式字符串就向右移动一个单位，再从头与字符串比对，直到超出右边边界。用一张图来描述是这样子的：\n\n![DNA](https://infinityglow.github.io/study/algorithm/brute-force/string-matching/images/DNA.png)\n\n将这个过程写成代码的形式：\n\n```\ndef string_matching_bf(text, pattern):\n    n = len(text); m = len(pattern)\n    for i in range(n-m+1):\n        j = 0  # 用于记录成功配对的字符个数\n        while j < m and pattern[j] == text[i+j]:\n            j += 1\n        if j == m:\n            return i\n    return -1\n```\n\n其中字符串的长度为 `n`，配对字符串长度为 `m`。接下来我们分析一下复杂度。先来看看最好的情况，最好的情况当然就是一次就成功啦，所以只需要比较 m 次，时间复杂度就为 Θ(m)。最坏的的情况就是，每当模式字符串向右移动一个单位，都需要比较 m 次，并且第 m 次的时候配对失败，所以最坏的情况总共需要比较 m(n-m+1) 次，时间复杂度就为 Θ(mn)。\n\n在自然语言中，通常第一个字符比对失败的概率很大，每次模式字符串移动的一个单位时候所需要比较的次数一般接近于 1，所以平均情况可以视为一个线性的复杂度：Θ(n)。\n\n"},{"title":"冒泡排序与选择排序","url":"/study/algorithm/brute-force/bubble-selection-sort/","content":"\n## 暴力求解\n\n从这一章开始，我们就要正式地进入具体的算法问题了。别忘了，我们没有按照类别将这些算法问题分类（排序，搜索等），而是按照不同的解决方法来划分的。在我们还没有了解那些高(ling)深(ren)莫(tou)测(teng)的算法之前，我们先来学习一种最简单、最直接的解决方式：**暴力求解**（**brute-force**）。\n\n![step](https://infinityglow.github.io/study/algorithm/brute-force/bubble-selection-sort/images/step.jpg)\n\n所谓的暴力求解就是指：根据问题具体的描述直接写出相应的算法，从而忽略我们观察和分析问题的过程。举两个例子，前面提到的扔鸡蛋的问题，假如你从第一层开始一层一层地试，直到试出在哪一层鸡蛋刚好会碎，这种方法就是最简单，最直接的方法，也是你一开始就能够想到的方法。另一个例子就是假设让你猜一个 1-100 之间的整数，每次猜完之后会告诉你大了还是小了，你可能会想到从 1 开始一个一个递增地往上猜，但这样显然是最笨的方法，聪明的你肯定会从两个数的中间开始猜，像这样的方法我们在[后面](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/)还会再次提及。\n\n从两个简单的例子我们似乎可以提前得出结论：暴力求解的效率一定不会很高，但我们可以很方便的 coding。所以，对于一些规模很小的问题我们可以采用这种方式，但对于规模较大的问题，我们计算机的 CPU 可能就吃不消了。\n\n![sick](https://infinityglow.github.io/study/algorithm/brute-force/bubble-selection-sort/images/sick.jpg)\n\n## 冒泡排序\n\n下面我们就从两种最简单的排序算法开始介绍。首先是**冒泡排序**（**bubble sort**），当你一听到这个名字的时候，你就可能认为它跟冒泡泡有一定关系，事实上的确如此，我们可以先从一个动画（来源于[网络](https://visualgo.net/en/sorting)）直观地感受一下：\n\n![demo](https://infinityglow.github.io/study/algorithm/brute-force/bubble-selection-sort/images/bubble.gif)\n\n可以看到的是，每次循环，左边总有一些大的数会向右“浮动”到相应的位置，如果把它旋转 90 度，这些数就好像在“冒泡”了。实际上，冒泡排序是基于数组中相邻两个元素的比较，如果乱序就交换位置，一直重复这个过程直到数组有序。因为在每次循环中，那些较大的数总是要跟较小的数做交换，所以在直观上就好像这些较大的数在往右“浮动”了。\n\n冒泡排序的实现非常简单，几行代码就能够搞定：\n\n```\ndef bubble_sort(array):\n    n = len(array)\n    for i in range(n-1):\n        for j in range(n-1-i):\n            if array[j] > array[j+1]:\n                array[j], array[j+1] = array[j+1], array[j]\n    return array\n```\n\n其中 n 为数组的长度。那冒泡排序的效率如何呢？根据我们前面所学的[复杂度分析](https://infinityglow.github.io/study/algorithm/complexity-analysis/non-recursive/)的内容，我们可以得到并推导出下面的式子：\n\n![推导](https://infinityglow.github.io/study/algorithm/brute-force/bubble-selection-sort/images/derivation0.png)\n\n最后我们推出冒泡排序的复杂度为 Θ(n<sup>2</sup>)。\n\n## 选择排序\n\n**选择排序**（**selection sort**）是用暴力求解来解决排序问题的第二种方法，同样它也是一种简单、直接的方法，让我们先从一个动画（来源于[网络](https://visualgo.net/en/sorting)）演示中来一探究竟吧。\n\n![demo](https://infinityglow.github.io/study/algorithm/brute-force/bubble-selection-sort/images/selection.gif)\n\n从动画中可以看到，每次循环都会从右边未排序的序列中选择一个最小的数，与未排序的序列中的第一个元素交换，然后反复这个过程，知道循环结束。\n\n选择排序用 Python 实现也很容易，也只需要几行代码：\n\n```\ndef selection_sort(array):\n    n = len(array)\n    for i in range(n-1):\n        min = i\n        for j in range(i+1, n):\n            if array[min] > array[j]:\n                min = j\n            array[i], array[min] = array[min], array[i]\n    return array\n```\n\n因为要暂存那些最小值的索引值，所以这里用到了一个变量`min`。最后来分析一下选择排序的复杂度，跟冒泡排序一样，根据循环结构，可以推导出最终的复杂度也为 Θ(n<sup>2</sup>)\n\n![推导](https://infinityglow.github.io/study/algorithm/brute-force/bubble-selection-sort/images/derivation1.png)\n\n从时间复杂度我们可以看出，这两种排序算法的效率都不高。为了间接的证明这个结论，我在这一节的[完整代码]()里用到了每次排序所需要的比较次数和排序时间这两个指标来间接反映这一结论。后面我们还会接触到一些更为高效的算法，大家也可以通过这两个指标来判断一个排序算法快慢与否。\n"},{"title":"复杂度分析（递归）","url":"/study/algorithm/complexity-analysis/recursive/","content":"\n## 什么是递归\n\n前面我们对一些算法的复杂度进行了分析，但这些都是基于循环和迭代的，这一节我们会针对递归的算法进行复杂度分析。首先要需要知道什么是递归，**递归**（**recursion**）是函数调用自身的一个过程。举个例子，假设你是一个英语水平有限的人，你在读一段英文材料中遇到了某个生词，你需要查字典去了解这个单词的意思，但是字典只提供了英英字典，意味着你找到的单词下方的释义也有可能是你不认识的单词，于是你又继续查找那些单词的意思，而那些单词的释义有可能又出现你不认识的单词，你又继续查找直到你能够全部看懂为止。这样一个过程我们可以把它看做递归，因为查字典这个动作在不停地调用自身。\n\n![字典](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/images/dictionary.jpg)\n\n试想如果你的英语水平很糟糕，你可能查了很久的字典还是没有弄明白这个生词是什么意思，甚至出现单词 A 用到了单词 B 做释义，单词 B 用到了单词 A 做释义这种情况，即无限循环。那么一次递归结束的条件是什么呢？显然，如果一个单词的释义你全都能看懂，当前递归就结束了，这种条件叫作**初始条件**（**initial condition**），也叫作递归的出口。\n\n## 阶乘的计算\n\n阶乘的计算大家应该都不陌生，一个数的阶乘 n ! = n × (n-1) × (n-2) × ... × 2 × 1，所以我们自然而然想到的计算方法是从 1 到 n 把它们乘起来，但只要我们稍微观察一下便可以得到 n ! = n × ( n - 1) ! 这样的关系式。这个关系式也不是一直递归下去的，同样需要一个出口，根据定义，0 的阶乘等于 1，所以当 n = 0 时递归停止。我们可以用几行代码轻松地实现它：\n\n```\ndef factorial(n):\n    if n == 0:\n        return 1\n    return n * factorial(n-1)\n```\n\n如果用一个数学关系式是来表达是这样子的：\n\n![关系式](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/images/relation0.png)\n\n我们观察可以得到这样的规律：问题的规模 n 在经过一次乘法运算 n × F(n - 1) 后变为了 n - 1，并且每经过一次乘法运算，问题的规模都减少了 1，所以要求这个算法的复杂度，我们可以推出如下的关系式：\n\n![关系式](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/images/relation1.png)\n\n其中“+ 1”代表问题的规模每减少 1 就执行了一次乘法运算。这样的关系式叫作**递归关系式**（**recurrence relation**），要解出这个关系式，我们需要一步一步地推导，具体来讲就是先将 n - 1 代入式子中得到一个新的递归关系式，再将它代入到原式子中就可以得到 t(n) = t(n - 2) + 2，如此往复，直到出现递归出口。\n\n![推导](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/images/derivation0.png)\n\n于是我们得出计算阶乘的时间复杂度为 O(n)。\n\n## 汉诺塔\n\n汉诺塔大家肯定都玩过，规则也很简单：有 A, B, C 三根柱子，上面穿有从大到小排列的圆盘，现在你需要借助 B 柱把 A 柱上的圆盘挪到 C 柱上，一次只能挪一个，但大的圆盘不能放在小的圆盘之上。\n\n ![演示](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/images/demo.gif)\n\n那么它跟递归有什么关系呢？如果你是一个细心观察的人，你就会发现，要想把 n 个圆盘从 A 柱挪到 C 柱上，你首先要通过某种方法把 n - 1 个柱子从 A 挪到 B 柱上，给 A 柱上最大的圆盘“腾个位置”出来，然后把最大的圆盘移到 C 柱上，最后在通过某种方法把 B 柱上 n - 1 个圆盘挪到 C 柱上。至于怎么移动这 n - 1 个柱子，我们可以再用某种方法移动 n - 2 个柱子，给第二大的圆盘“腾位置”，再将 n - 2 个柱子移动到相应的柱子上。这样我们便可以一直递归下去，直到只有一个圆盘，也就是这里的递归出口了。\n\n我们用几张图来直观地理解一下，假设这里有 4 个圆盘，我们可以分为三个步骤：\n\n ![状态0](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/images/state0.png)\n\n- 步骤一：将 3 个圆盘从 A 柱移动到 B 柱\n\n![状态1](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/images/state1.png)\n\n- 步骤二：将 A 柱剩下的圆盘移动到 C 柱\n\n![状态2](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/images/state2.png)\n\n- 步骤三：将 3 个圆盘从 B 柱移动到 C 柱\n\n![状态3](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/images/state3.png)\n\n那么怎样来写递归关系式呢？我们看到，要解决 n 阶汉诺塔的问题，我们先要解决两个 n - 1 阶同样的问题，外加一个单次移动。所以我们的递归关系式应该这样写：\n\n![关系式](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/images/relation2.png)\n\n我们还是采用回代的方式解出这个关系式：\n\n![推导](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/images/derivation1.png)\n\n复杂度为 O(2<sup>n</sup>)，最后我们用代码来实现一下吧。\n\n```\ndef hanoi(a, b, c, n):\n    if n == 1:\n        print(\"{} -> {}\".format(a, c))\n        return\n    hanoi(a, c, b, n-1)  # c 柱为枢纽，将 a 柱中 n - 1 个圆盘移到 b 柱上\n    hanoi(a, b, c, 1)  # 将待移动的圆盘数设为 1\n    hanoi(b, a, c, n-1)  # a 柱为枢纽，将 b 柱中 n - 1 个圆盘移到 c 柱上\n```\n\n让我们试试 `n = 3` 时输出的结果：\n```\na -> c\na -> b\nc -> b\na -> c\nb -> a\nb -> c\na -> c\n```\n\n类似的例子还有很多，但它们的方法都是一样的。"},{"title":"复杂度分析（非递归）","url":"/study/algorithm/complexity-analysis/non-recursive/","content":"\n## 最好，最坏和平均情况\n\n一个算法的好坏并不是一成不变的，它会在不同的情况下有着不同的表现。先来看看一个生活中的场景，假设买彩票中奖的概率是1 / 10,000，那么在最好，最坏和平均的情况下需要买几注彩票才能够中奖呢？显然，如果你的运气够好，第一注你就直接中了，这是最好的情况；相反，如果你的手气够差，你买的彩票永远都中不了奖，这是最坏的情况；平均来讲，你需要买最少10,000注才能够中奖，其实就是这里的数学期望。\n\n![彩票](https://infinityglow.github.io/study/algorithm/complexity-analysis/non-recursive/images/lottery.jpg)\n\n这里的类比并不是想说明算法跟概率学有任何的关系，只是想告诉大家，算法的复杂度确实是有“运气”的成分，它取决于一个（类）问题的**实例**（**instance**）。这里的实例可以这样来理解：如果排序是我们要解决的问题，也就是对任意无序数组的有序化，那么一个实例就是一个具体的数组，比如 [3, 1, 2, 5, 4] 或 [1, 2, 3, 4, 5]。\n\n我们可以看到，后者的数组已经有序了，不需要再额外执行任何操作。对于有些排序算法来讲就可以利用这一点性质，减少执行的次数，让算法更有“远见”。像这样能使一个算法执行次数最少的情况我们把它称作**最好情况**（**best case**）。相反，使一个算法执行次数最多的情况叫做**最坏情况**（**worst case**）。最后剩下的**平均情况**（**average case**）指的是通常状态下算法的复杂度，它并不是等于（最好情况 + 最坏情况）/ 2 这么简单，而是要把所有的 instance 考虑在内，计算复杂度，然后再求一个平均值，后面的例子会详细讨论这个问题。\n\n## 顺序查找\n\n如果我们要查找数组里的某个元素，我们可以通过顺序查找的方式进行，也就是从左往右依次查找。代码如下：\n\n```\ndef find(array, element):\n    length = len(array)\n    for i in range(length):\n        if array[i] == element:\n            return i\n    return -1\n```\n\n我们用一个`for`循环遍历整个数组，如果找到了`element`就返回相应的索引值，否则返回 -1。判断这一条件是否成立的语句显然是 `if array[i] == element`，并且它是被执行的最多的语句，我们将这种执行次数最多，耗时最长的操作叫做**基本操作**（**basic operation**）。一般这种操作出现在最内层的循环，或者出现[递归](https://infinityglow/study/algorithm/complexity-analysis/recursive/)的地方。\n\n然后，我们来分析一下最好和最坏情况的时间复杂度。首先，最好的情况是循环的第一个元素就是我们要找的，比如我们要从数组 [3, 1, 2, 5, 4] 中找元素 3，那么时间复杂度显然就为 O(1)；最坏的情况是最后一个元素才是我们要找的或者查找失败，比如像这样的数组 [1, 2, 4, 5, 3] 需要执行 5 次基本操作。一般的情况，如果数组的长度为 n，则需要 n - 1 次，所以复杂度为 O(n)。\n\n最后我们来说一说平均复杂度，如果我们考虑了所有的情况，即查找的元素 `element` 在数组 `array` 的每一个位置都会出现，且出现的概率是相等的，那么我们可以通过这样一个式子来计算：\n\n![公式](https://infinityglow.github.io/study/algorithm/complexity-analysis/non-recursive/images/formula0.png)\n\n其中分子项为每一种情况需要查找的次数，最后得到的复杂度也为 O(n)，表明了在一般情况下该算法的时间复杂度也为线性复杂度。\n\n## 矩阵乘法\n\n让我们再来看一个例子，相信学过线性代数的小伙伴对矩阵的乘法应该不陌生，假设有两个矩阵 A (m × n) 和 B (n × p)，最后得到的矩阵 C (m × p) 中每一项都是 A 中每一行的 n 个元素和 B中每一列的 n 个元素乘积之和。下面是一个动图的演示（摘自[3Blue1Brown](https://www.youtube.com/watch?v=XkY2DOUCWMU)里的视频）：\n\n![矩阵乘法](https://infinityglow.github.io/study/algorithm/complexity-analysis/non-recursive/images/matmul.gif)\n\n如果写成 Python 的代码就是这样子的：\n\n```\ndef matmul(A, B):\n    n = len(A)\n    C = [[0 for i in range(n)] for j in range(n)]\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                C[i][j] = C[i][j] + A[i][k] * B[k][j]\n    return C\n```\n\n这里为了计算方便，两个矩阵都采用了方阵的形式。很显然，这里的基本操作是 `C[i][j] + A[i][k] ∗ B[k][j]`。准确地来说，由于在计算机里做乘法比做加法所耗费的时间要大，所以 `A[i][k] ∗ B[k][j]` 才是我们的基本操作。\n\n分析完了基本操作，下面就来计算复杂度了。因为不管什么样的矩阵，它们执行的基本操作都是一样的，所以就不存在最好，最坏和平均情况的讨论了。于是根据循环我们可以得到：\n\n![公式](https://infinityglow.github.io/study/algorithm/complexity-analysis/non-recursive/images/formula1.png)\n\n其复杂度为 O(n<sup>3</sup>)。是不是很简单呢？类似的例子还有很多，但它们方法都是一样的。\n"},{"title":"三种表示方法：O, Ω, Θ","url":"/study/algorithm/complexity-analysis/three-notations/","content":"\n## 引入\n\n前面我们讲到了求最大公约数的算法：欧几里得算法。我们先举一个具体的实例：24 和 16 的公约数。根据其公式 `gcd(m, n) = gcd(n, m mod n)`，我们可以得到：`gcd(24, 16) = gcd(16, 8) = gcd(8, 0)`，所以最大公约数是 8。除了这种方法以外，我们还可以从 16 开始一个一个地递减，如果存在两数都能够被整除，那么这个数就是最大公约数。写成代码的形式就是：\n\n```\ndef gcd(m, n):\n    r = min(m, n)  # 选较小者\n    while r != 0:\n        if m % r == 0 and n % r == 0:\n            return r\n        r -= 1 \n``` \n\n我们可以计算得出，该算法的 if 语句被执行了 9 次，而前面的方法只执行了 3 次，我们看两者出现了差异，换句话说就是算法的效率出现了不同。下面就引出**时间复杂度**（**time complexity**）的概念：当一个问题的规模 n 趋向于无穷大的时候，算法所需要的时间 t(n) 。当然，如果一个算法需要很大的运行空间，那么**空间复杂度**（**space complexity**）就是当 n 趋向于无穷大的时候，算法所需要的空间 s(n) 。\n\n## 用于约束的 O, Ω, Θ\n\n那么问题来了，t(n) 的单位是什么呢？如果是秒的话，不同的计算机，不同的运行环境，计算出的结果肯定是不一样的，难以达到统一，这就需要我们定性的去描述这个问题，所以就有了用另一个函数来约束 t(n)。其中大 O 符号约束了 t(n) 的**上界**（**upper bound**）。换句话说，当 n 在趋近无穷大的时候，t(n) 的大小总是小于等于某个函数。举个例子，前面用迭代法求最大公约数所耗费的时间跟问题的规模明显呈现的是一个线性关系，那么我们可以这样表示：t(n) ∈ O(g(n))，其中 g(n) = n。当然，t(n) 也满足 t(n) ∈ O(n<sup>2</sup>)。这里为了更好的理解，没有采用严格的数学定义，感兴趣的同学可以去[维基百科](https://zh.wikipedia.org/wiki/%E5%A4%A7O%E7%AC%A6%E5%8F%B7)一探究竟，这里我们就不细讲了。\n\n同样，大 Ω 符号约束了 t(n) 的**下界**（**lower bound**），即 n 在趋近无穷大的时候，t(n) 总是大于等于某个函数。还是拿刚才的例子，t(n) 我们可以表示为 t(n) ∈ Ω(n)，代表其时间复杂度不会低于线性的复杂度，当然也不会低于常数的复杂度（不随 n 的变化而变化），所以也有：t(n) ∈ Ω(1)。\n\n剩下的 Θ 符号就代表的是 t(n) 的**确界**（**tight**）了，就是说 n 在趋近无穷大的时候跟它时间复杂度一样的一个函数。所以前面的例子就有：t(n) ∈ Θ(n)。最后用三张图来直观地感受一下吧！\n\n![illustration](https://infinityglow.github.io/study/algorithm/complexity-analysis/three-notations/images/illustration.png)\n\n## 复杂度的比较\n\n如果给定两个时间复杂度 O(f(n)) 和 O(g(n))，怎样来判断哪个时间复杂度比较大呢？根据复杂度的定义我们知道，一个函数在趋向于无穷大时的变化情况决定了复杂度的大小，而要比较两者的大小，我们可以通过做商的方式，像这样：![limit0](https://infinityglow.github.io/study/algorithm/complexity-analysis/three-notations/images/limit0.png) 这个极限可能会出现好几种情况，让我们分别来看看吧。\n\n- 情况1 极限为 0\n\n   假设 f(n) = n，g(n) = n<sup>2</sup>。那么我们就可以得出极限为 0 ![limit1](https://infinityglow.github.io/study/algorithm/complexity-analysis/three-notations/images/limit1.png) 这样我们可以认为 f(n) 的增长慢于 g(n) 的增长，即 f(n) 的复杂度低于 g(n) 的复杂度。\n\n- 情况2 极限为 ∞\n   \n     假设 f(n) = n，g(n) = log n。由于极限是无穷比无穷的未定式，所以我们可以借助洛必达法则，最后得出极限为 ∞ ![limit2](https://infinityglow.github.io/study/algorithm/complexity-analysis/three-notations/images/limit2.png) 这样我们可以认为 f(n) 的增长快于 g(n) 的增长，即 f(n) 的复杂度高于 g(n) 的复杂度。\n     \n- 情况3 极限为 c\n   \n     假设 f(n) = 3n<sup>2</sup> + 5，g(n) = 7n<sup>2</sup> + log n。同样借助洛必达法则，最后得出极限为一常数 c ![limit3](https://infinityglow.github.io/study/algorithm/complexity-analysis/three-notations/images/limit3.png) 这样我们可以认为 f(n) 的增长快慢和 g(n) 相同，即 f(n) 的复杂度和 g(n) 的复杂度相同。\n    \n从情况 3 我们可以观察出，两个函数的复杂度都由前面的部分决定（3n 和 7n），这间接地说明了前面的部分占据了主要的地位，于是我们可以把一些常见的复杂度排个序，并用一张图展示出来。\n\n![comparison](https://infinityglow.github.io/study/algorithm/complexity-analysis/three-notations/images/comparison.png)\n\n从左上到右下，其大小排列依次是：O(2<sup>n</sup>) > O(n<sup>3</sup>) > O(n<sup>2</sup>) > O(nlog n) > O(n) > O(log n) > O(1)\n\n\n"},{"title":"算法的问题类型","url":"/study/algorithm/basic-knowledge/problem-types/","content":"\n## 概述\n\n前面讲到了很多关于数据结构的概念，后面的内容就要用这些数据结构和算法来解决具体的问题了。那么首先我们是不是要对这些问题归一个类呢？废话不多说，直接先列举出来，主要分为五大类：\n\n- 排序问题\n- 搜索问题\n-  图的问题\n-  几何问题\n- 组合问题\n\n## 排序问题\n\n排序问题是计算机科学里最基础，也是最重要的问题，很多情况下直接决定了你设计的程序的效率，因为大部分的程序都会用到排序。在实际生活中排序也是用处多多。一个最简单的例子就是国外大学的招生办将申请者的 GPA 全部导入到 Excel 中，然后将它们从高到低依次排序来筛选他们想要的学生。\n\n排序所要实现的功能也很简单，我们的输入可以是一串无序的数字或者字母，输出就是从大到小或从小到大排列的有序数组，像这样：\n\n![排序](https://infinityglow.github.io/study/algorithm/basic-knowledge/problem-types/images/sorting.png)\n\n## 搜索问题\n\n搜索也是我们无时不刻都会用到的一个功能。Google 和百度的服务器每天都要从互联网上爬取海量的数据，然后将它们放在搜索树中以便于用户的查找。试想，如果这个搜索的效率很低很低，要几十秒甚至是几分钟才搜索出结果，那这两家公司可能也不会有今天的成就了吧。\n\n所以这就是我们要讨论的第二个问题：搜索问题。它可以再分为两个类，一个是用于单一键的搜索，例如下面这颗二叉树，我们要搜索 `Key = 35` 的节点是否存在：\n\n![搜索](https://infinityglow.github.io/study/algorithm/basic-knowledge/problem-types/images/search.png)\n\n第二个问题是关于**字符串匹配**（**string matching**）的。假设我们有一段英文：“A fall into a pit, a gain in your wit.”，现在要从里面查找“pit”这个单词以及它所处的位置。跟第一种不同的是，我们要查找的不再是一个单一的键而是一个字符串。这样就和我们从网页或电子书里搜索一段话没什么区别了。\n\n## 图的问题\n\n图的问题可谓是应用面最广的问题类型之一了。当我们开车迷路时，我们的手机里的高德地图总是给我们规划好距离最短的路线，或是耗时最短的路线，引领着我们到家。再就是小的时候玩过的“一笔画”游戏，其抽象成图的问题就为：给定一些图的节点和边 `<V, E>` ，总是存在一条路径包含了所有的边，且每条边只能被访问一次。下图就分别列举出了存在和不存在的情况。这些通通都要用到图的算法来解决。\n\n![一笔画](https://infinityglow.github.io/study/algorithm/basic-knowledge/problem-types/images/E-path.png)\n\n## 几何问题\n\n几何问题当然就是讨论关于点、线、面的问题啦。这里有一个最经典的问题：邮局选址问题。问题的描述是一条笔直的马路两侧分布着一些村庄，现在要在这些村庄之间修建一个邮局，使得所有村庄离这个邮局的距离之和最短。后面我们还会讨论**最短点对问题**（**closest-pair problem**）和**凸包问题**（**convex-hull problem**）。\n\n![邮局](https://infinityglow.github.io/study/algorithm/basic-knowledge/problem-types/images/post-office.jpg)\n\n## 组合问题\n\n前面提到的“一笔画”问题不仅是关于图的问题，同样它也是一个组合问题，这样的问题试图从排列组合中找到答案。我们知道，排列组合问题的规模一般都非常大，所以组合问题也是算法里面最耗时，最难解决的一类问题。后面我们还会讨论[背包问题](https://infinityglow.github.io/study/algorithm/dynamic-programming/knapsack-problem/)等一系列组合问题。\n\n![背包问题](https://infinityglow.github.io/study/algorithm/basic-knowledge/problem-types/images/knapsack.png)\n\n"},{"title":"基本数据结构","url":"/study/algorithm/basic-knowledge/data-structure/","content":"\n## 概念\n\n如果说程序员是用代码编织这个世界的一群人，那么**数据结构**（**data structure**）就是编织所用到的各种工具了，它将计算机中各种各样的数据组织在一起。而我们的算法就是利用这些现有的工具将它们拼接成风格、功能各不相同的程序了。简单来说：算法 + 数据结构 = 程序。这一节就让我们来了解一些基本的数据结构吧。\n\n## 数组与链表\n\n我们从最简单的一维数据结构开始。数组大家应该都不陌生，它是将一组相同数据类型的数据存储在一起的数据结构。\n\n![数组](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/array0.png)\n\n这里我们分析一下最基本的三大操作：查找、插入和删除。\n\n拿上面的图为例，因为数组中每一个**元素**（**element**）都有自己的一个下标，称为**索引**（**index**），它们被存储在计算机的RAM中，所以查找的速度非常快。一般查找的时间不随数组长度而增加。\n\n插入和删除就要稍微麻烦一些。因为数组占用的是计算机内存的连续地址空间，所以在插入一个新元素时，所有在它后面的元素都要向后移动一个单位，为新元素“腾出”位置。同理，为了保持数组的完整性，删除一个元素后，所有在它后面的元素也要向前移动一个单位，以此来“填充”空位。\n\n以这里的数组为例，如果我们要删除 `index = 5` 的元素，那么在它后面的 `15`，`34` 和 `80` 都要向前移动一个单位，如下图中所示。\n\n![数组](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/array1.png)\n\n让我们用Python代码来实现一下。\n\n```\n# 查找\ndef search(array, element):\n    length = len(array)\n    for i in range(length):\n        if array[i] == element:\n            return i\n    return -1\n\n# 插入\ndef insert(array, idx, element):\n    new_array = array.copy() + [None]  # 复制原数组到一个新数组\n    length = len(new_array); i = length - 1\n    # 所有idx之后的元素向右移动一个单位\n    while i > idx:\n        new_array[i] = new_array[i-1]\n        i -= 1\n    new_array[idx] = element\n    return new_array\n\n# 删除\ndef remove(array, element):\n    length = len(array)\n    idx = search(array, element)  # 找到对应索引\n    if idx != -1:\n        # 如果查找成功，所有在idx之前的元素向左移动一个单位\n        while idx < length - 1:\n            array[idx] = array[idx+1]\n            idx += 1\n        array = array[: -1]\n    return array\n```\n\n再来说一说链表。与数组采取的顺序存储方式不同，链表中所有的元素的存储地址都是分散的，也就是说，它是靠一条链条（指针）把这些元素联系在一起的。所以我们把链表中每一个存储单元称为一个**节点**（**node**），在每个节点里有两个域：一个用来存储数据，一个是指针指向下一个节点，像这样：\n\n![链表](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/linked-list0.png)\n\n在Python中我们用一个类来表示：\n\n```\nclass Node(object):\n    def __init__ (self, value, next):\n        self.value = value\n        self.next = None\n```\n\n如果将它们串联在一起，将会是这样：\n\n![链表](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/linked-list1.png)\n\n其中的`header`指的是头结点，它一般记录第一个节点的地址和链表的长度，我们在查找的时候就是从头结点开始的，代码如下：\n\n```\nclass Linked_List(object):\n    def __init__ (self):\n        self.length = 0\n        self.header = None\n    def search(self, value):\n        p = self.header  # 获取第一个节点\n        while p != None:\n            if p.value == value:\n                return p\n            p = p.next\n        return None\n```\n\n和数组相比，链表的插入和删除就非常的简单，只需要改变一下指针即可。下面还是以插入元素`61`为例：\n\n首先需要新建一个节点p\n\n![链表](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/linked-list2.png)\n\n然后，p的指针指向头结点所指的节点\n\n![链表](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/linked-list3.png)\n \n最后，改变头结点的指针，使其指向p，同时更新链表的长度length。\n\n![链表](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/linked-list4.png)\n\n插入和删除的Python实现：\n\n```\nclass Linked_List(object):\n    def __init__ (self):\n        self.length = 0\n        self.header = None\n    def insert(self, value):\n        # 新建一个节点\n        p = Node(value)\n        p.next = self.header\n        self.header = p\n        self.length += 1\n    def remove(self):\n        p = self.header  # 获取第一个节点\n        if p is not None:\n            self.header = p.next\n            self.length -= 1\n            del p  # 删除节点 p\n```\n\n## ADT\n\n**抽象数据类型**（**Abstract Data Type**, 简称**ADT**）是数据结构中非常重要的一个部分。用最简单的理解方式就是：不仅限于编程语言中已经实现的一些数据类型，例如 Python 中 list，set，tuple，dictionary 等等。我们可以进一步的定义出属于我们自己数据结构，比如说增删只能在一侧进行，具有层状、环状的结构等等。那就让我们来看看几个最为常见的例子吧～\n\n### 栈\n\n**栈**（**stack**）就是前面提到的只能在一侧进行元素的增加和删除的数据结构，这种过程分别称为**入栈**(**push**)和**出栈**(**pop**)。生活中最简单的类比就是叠盘子，新洗好的盘子总是放在最上面，而拿走的时候总是从最上面一个拿走。这种后进先出（**last in, first out**）的方式就体现了这种思想。\n\n![栈](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/stack.jpeg)\n\n### 队列\n\n和栈相反，**队列**（**queue**）是遵循**先进先出**（**first in, first out**）的方式实现元素的增减，其过程分别叫做**入队**（**enqueue**）和**出队**（**dequeue**）。生活中的例子就是去超市收银台排队的时候，排到队伍后面的人要先等前面的人都结完账了自己才能结账，所以这就是先进先出啦。\n\n![队列](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/queue.png)\n\n因为和前面的数组和链表有相似性，所以这里就不再用Python代码做演示了，你可以去我的[GitHub](https://github.com/infinityglow/Algorithm-and-Complexity/tree/master/Basic%20Knowledge/Data%20Structure)主页查看本节的完整代码。\n\n### 树\n\n与前面的数据结构不同，**树**（**tree**）是典型的层状数据结构。直观上来看，它就像倒挂着的一颗树，每一个节点连接着多个子节点。这里为了方便，我们只讨论**二叉树**（**binary tree**），也就是每一个节点最多只有两个子节点的情况。\n\n![树](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/tree0.png)\n\n下面来简单介绍一下二叉树的性质。和所有树状结构一样，二叉树在非空的情况下一定有一个**根节点**（**root node**），如图中的节点26。它的两个子节点我们分别把它们叫做该节点的**左孩子**（**left child**）和**右孩子**（**right child**），而对于那些没有孩子的节点，我们把它称做**叶子节点**（**leaf node**）。前面只是针对节点的讨论，而对于树本身，我们要了解的是满二叉树和完全二叉树。\n\n满二叉树是指每一个节点孩子的个数只能为0或2，如下图所示，左边是一颗满二叉树，但右边由于节点48只有一个孩子，所以就不是满二叉树。\n\n![树](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/tree1.png)\n\n再来看完全二叉树，它的性质是除了最后一层的节点没有排满以外，其他层的节点均已排满，并且最后一层节点都是从左往右依次排列。下图的两个例子分别代表了完全二叉树和非完全二叉树。后面的[堆](https://infinityglow.github.io/study/algorithm/transform-and-conquer/heap/)也会用到这种性质。\n\n![树](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/tree2.png)\n\n在Python中我们可以用一个类来表示树的节点：\n\n```\nclass Node(object):\n    def __init__ (self, value):\n        self.value = value\n        self.left = None  # 左孩子\n        self.right = None  # 右孩子\n```\n\n### 图\n\n**图**（**graph**）是 ADT 中用到的最多的结构，生活中的方方面面都会用到图。例如，高德地图就是把所有城市，所有道路通通抽象成了图这种数据结构来帮助我们导航的。哈哈，厉害吧！那我们就首先来了解一下图的基本结构。\n\n图也是由一个一个的**节点**（**vertex**）组成的。但与树不同的是，图的节点之间相连的叫做**边**（**edge**），所以我们用 G = <V, E> 来表示一张图，其中的 V 表示由 vertex 构成的集合，而 E 则表示由 edge 构成的集合。\n\n图的分类一般有好几种，如果一个节点到另一个节点是双向的我们把它叫做**无向图**（**undirected graph**），如果是单向的就叫**有向图**（**undirected graph**），下图中就分别代表了无向图和有向图。\n\n![图](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/graph0.png)\n\n假如从一个节点到另一个节点是要付出“代价”的，那么我们就可以在每一条边上把这种“代价”体现出来，称之为**权重**（**weight**），而构成这样的图当然就叫做**有权图**（**weighted graph**）了。\n\n![图](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/graph1.png)\n\n我们可以用两种方式来表示一张图，一种叫做**邻接矩阵**（**adjacency matrix**）和**邻接表**（**adjacency list**），后面的内容会详细讨论，先上图。\n\n![图](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/images/graph2.png)\n\n这样我们就完成所有基本数据结构的学习了，是不是对它们有了清晰的认识呢？\n\n[本节全部代码](https://github.com/infinityglow/Algorithm-and-Complexity/tree/master/Basic%20Knowledge/Data%20Structure)\n"},{"title":"算法与复杂度","url":"/study/algorithm/overview/","content":"\n## 前言\n\n这个系列的博文会逐个介绍计算机科学里面最基础、也是最重要的一部分内容：**算法**(**algorithm**)。提到它，这可能是你最擅长的部分，亦或是你学生生涯的噩梦。不管怎么样，对于学计算机的小伙伴来讲，它始终是不可回避的一个话题。不论是学生时代的你还是已经踏上了工作的岗位，算法都会一直陪伴着你。\n\n为什么要做这个系列呢？因为网上对于这一块的内容实在是太多，甚至是太杂，而很少有把算法的知识体系整合起来形成一个系列的教学博客。于是乎想尽自己的微薄之力，让更多的人能够更好地理解算法，在未来求职的面试中不再因为它而与自己理想的公司失之交臂。\n\n我将与国内的教学方式和教学内容有所差别。形式上不再是只针对如何解决这个问题，因为只会解决问题并不代表真正理解这个问题。我会花一些篇幅着重介绍一些概念性的内容，这也是国内的教学最欠缺的部分。国内的课堂不会告诉你自然对数e与自然界生长的规律有关；学完了线性代数，你可能光学会了如何解行列式，却忽视了行列式也是有几何意义的。内容上不再按照“排序算法”、“搜索算法”等方式分类，而采用了问题解决的不同方式来划分，比如“暴力求解”、“分治法”、“动态规划”等等。当然，我也是参考了Levitin编写的教材[Introduction to The Design and Analysis of Algorithms, 3rd Edition](https://doc.lagout.org/science/0_Computer%20Science/2_Algorithms/Introduction%20to%20the%20Design%20and%20Analysis%20of%20Algorithms%20%283rd%20ed.%29%20%5BLevitin%202011-10-09%5D.pdf)。要是你觉得这本书讲得太基础，你也可以参考MIT的[《算法导论》](http://kddlab.zjgsu.edu.cn:7200/students/lipengcheng/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%EF%BC%88%E8%8B%B1%E6%96%87%E7%AC%AC%E4%B8%89%E7%89%88%EF%BC%89.pdf)。\n\n编程语言我会采用Python，因为Python是最接近这些英文书里用于讲解的伪代码，理解起来会更加方便。对于刚入门的小白来讲，Python的简洁不会让你因为不能理解编程语言本身而最终放弃了这门课程的学习。我会把每一节的内容的完整代码都放在我的[GitHub](https://github.com/infinityglow/Algorithm-and-Complexity)的 Algorithm 仓库里，方便学习后用代码真正实现它们。\n\n最后，要是讲解有任何疑问，欢迎在评论区留言。如果发现了有任何错误和表述不规范的地方，希望各位大佬轻喷，毕竟我也是第一次写博客，有错误也是在所难免的。\n\n## 总览\n\n### 介绍 (Introduction)\n\n&emsp;[1.1 什么是算法](https://infinityglow.github.io/study/algorithm/introduction/)\n\n### 基本知识 (Basic Knowledge)\n\n&emsp;[2.1 基本数据结构](https://infinityglow.github.io/study/algorithm/basic-knowledge/data-structure/)\n&emsp;[2.2 算法的问题类型](https://infinityglow.github.io/study/algorithm/basic-knowledge/problem-types/)\n\n### 复杂度分析 (Complexity Analysis)\n\n&emsp;[3.1 三种表示方法：O, Ω, Θ](https://infinityglow.github.io/study/algorithm/complexity-analysis/three-notations/)\n&emsp;[3.2 复杂度分析（非递归）](https://infinityglow.github.io/study/algorithm/complexity-analysis/non-recursive/)\n&emsp;[3.3 复杂度分析（递归）](https://infinityglow.github.io/study/algorithm/complexity-analysis/recursive/)\n\n### 暴力求解 (Brute Force)\n\n&emsp;[4.1 冒泡排序与选择排序](https://infinityglow.github.io/study/algorithm/brute-force/bubble-selection-sort/)\n&emsp;[4.2 顺序查找与字符串匹配（BF）](https://infinityglow.github.io/study/algorithm/brute-force/string-matching/)\n&emsp;[4.3 图的两种遍历](https://infinityglow.github.io/study/algorithm/brute-force/graph-traversal/)\n&emsp;[4.4 最近点对与凸包问题（BF）](https://infinityglow.github.io/study/algorithm/brute-force/clo-pair-con-hull/)\n&emsp;[4.5 暴力搜索](https://infinityglow.github.io/study/algorithm/brute-force/exhaustive-search/)\n\n### 减治法（Decrease and Conquer）\n\n&emsp;[5.1 插入排序](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/insertion-sort/)\n&emsp;[5.2 拓扑排序](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/topo-sorting/)\n&emsp;[5.3 二分查找与二叉树](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/binary-search-tree/)\n&emsp;[5.4 插值查找](https://infinityglow.github.io/study/algorithm/decrease-and-conquer/interpolation-search/)\n\n### 分治法（Divide and Conquer）\n\n&emsp;[6.1 归并排序](https://infinityglow.github.io/study/algorithm/divide-and-conquer/merge-sort/)\n&emsp;[6.2 快速排序](https://infinityglow.github.io/study/algorithm/divide-and-conquer/quick-sort/)\n&emsp;[6.3 二叉树的遍历](https://infinityglow.github.io/study/algorithm/divide-and-conquer/bt-traversal/)\n&emsp;[6.4 最近点对与凸包问题（DC）](https://infinityglow.github.io/study/algorithm/divide-and-conquer/clo-pair-con-hull/)\n\n### 变治法（Transform and Conquer）\n\n&emsp;[7.1 预排序](https://infinityglow.github.io/study/algorithm/transform-and-conquer/presorting/)\n&emsp;[7.2 霍纳法则](https://infinityglow.github.io/study/algorithm/transform-and-conquer/horners-rule/)\n&emsp;[7.3 堆与堆排序](https://infinityglow.github.io/study/algorithm/transform-and-conquer/heap/)\n&emsp;[7.4 AVL树](https://infinityglow.github.io/study/algorithm/transform-and-conquer/AVL-tree/)\n&emsp;[7.5 红黑树](https://infinityglow.github.io/study/algorithm/transform-and-conquer/R&B-tree/)\n&emsp;[7.6 2-3树](https://infinityglow.github.io/study/algorithm/transform-and-conquer/two-three-tree/)\n\n### 时空权衡（Time Space Tradeoff）\n\n&emsp;[8.1 计数排序](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/counting-sort/)\n&emsp;[8.2 字符串匹配（TST）](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/string-matching/)\n&emsp;[8.3 哈希](https://infinityglow.github.io/study/algorithm/time-space-tradeoff/hashing/)\n\n### 动态规划（Dynamic Programming）\n\n&emsp;[9.1 关于钱的两个经典问题](https://infinityglow.github.io/study/algorithm/dynamic-programming/classic-problems/)\n&emsp;[9.2 背包问题（DP）](https://infinityglow.github.io/study/algorithm/dynamic-programming/knapsack-problem/)\n&emsp;[9.3 弗洛伊德算法](https://infinityglow.github.io/study/algorithm/dynamic-programming/Floyd-Warshall/)\n  \n### 贪心算法（Greedy Algorithm）\n\n&emsp;[10.1 普林姆算法](https://infinityglow.github.io/study/algorithm/greedy-algorithm/Prim/)\n&emsp;[10.2 克鲁斯卡尔算法](https://infinityglow.github.io/study/algorithm/greedy-algorithm/Kruskal/)\n&emsp;[10.3 迪克斯特拉算法](https://infinityglow.github.io/study/algorithm/greedy-algorithm/Dijkstra/)\n&emsp;[10.4 哈弗曼树](https://infinityglow.github.io/study/algorithm/greedy-algorithm/Huffman-tree/)\n\n### *高阶算法（Advanced Algorithm）\n\n&emsp;[*回溯](https://infinityglow.github.io/study/algorithm/advanced-algorithm/backtracking/)\n&emsp;[*分支限界](https://infinityglow.github.io/study/algorithm/advanced-algorithm/branch-and-bound/)\n&emsp;[*线性规划（单纯形法）](https://infinityglow.github.io/study/algorithm/advanced-algorithm/linear-programming/)\n&emsp;[*最大流问题](https://infinityglow.github.io/study/algorithm/advanced-algorithm/maximum-flow/)\n&emsp;[*匈牙利算法](https://infinityglow.github.io/study/algorithm/advanced-algorithm/hungarian-algorithm/)\n&emsp;[*千禧问题：P = NP ?](https://infinityglow.github.io/study/algorithm/advanced-algorithm/p-np/)\n\n"},{"title":"手把手教你用VPS搭建SS+bbr实现科学上网","url":"/others/ss_v2ray/ss/","content":"\n## 写在前面\n\n  首先先声明一下，本文不带有任何商业性质，单纯地是想方便海内外学生党使用Google、YouTube、Wikipedia等工具来查阅资料，作为一个Google和Wikipedia的重度使用者，有时候离开了这两样东西真的就没办法生活，所以做一个这方面的教程是非常有必要的。\n\n  如果你对访问外网的频率和带宽要求不高，用一般的免费vpn或者一些在线代理就足够了，市面上这些vpn哪个好用，哪个免费相信你们比我更清楚。如果要经常看YouTube高清视频或者连外服打游戏的小伙伴呢，这些免费但又不稳定的vpn自然就不能满足你们的需要了，所以得拥有一个自己专属的服务器来实现科学上网。\n\n## 基本原理\n\n![Shadowsocks基本原理](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/principle.png \"基本原理示意图\")\n\n  简单介绍一下ss的基本原理，其实ss就是隧道通信的一种，就是说从中国大陆直接访问Google、Facebook等黑名单网站是会被防火墙检测到并直接中断我们与这些服务器的连接的。这时候ss横空出世，作为一个中间者把我们将要发送给Google、Facebook服务器的请求经过本地(sslocal)加密，再将它们封装一下，与我们后面要部署的服务器达成通信协议。\n\n  当服务器(ssserver)端收到来自sslocal的数据之后，先解密，再将解密过后的数据发送给Google、Facebook服务器。当这些服务器把数据传回给ssserver过后，ssserver会将这些数据加密并打包发送给sslocal，sslocal拿到数据之后再进行解密，最后以网页的形式呈现出来。整个过程中，由于数据是进行加密的，所以防火墙是不知道我们在访问Google等外网的。\n\n  但据说最近ss已经能够被防火墙识别出来其特征，安全性存在一些漏洞，如果有空了可以再做一期v2ray的教程，其安全性和稳定性都要比ss要高。\n\n## 部署VPS\n\n  第一步需要部署我们的云服务器，这里先推荐一下[谷歌云](https://cloud.google.com)，可以免费试用一年的时间，并且还比较稳定，但有点麻烦的是需要你绑定信用卡，而且还必须是美国的。。。\n\n  这里我们演示的是一些主流服务商提供的VPS，话不多说，先推荐一波：\n\n  [搬瓦工](https://bandwagonhost.com)\n  [vultr](https://www.vultr.com)\n  [hostwind](https://www.hostwinds.com)\n  [time4vps](https://www.time4vps.com)\n\n  有的可能在国内被墙掉了，访问不了的只能自己想办法了。\n\n  这里以vultr为例，先进入官网。\n\n![vultr官网](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s1.png)\n  如果你是新用户，则需要先注册一下，这里点击“Sign up”。\n  点击之后，用你的邮箱作为用户名，然后自己创建一个密码。\n\n![vultr注册界面](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s2.png)\n  注册完以后登录即可。\n\n![vultr登录界面](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s3.png)\n  进入如下页面之后，选择“Product”，再点击右边的加号部署服务器。\n\n![部署服务器](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s4.png)\n![部署服务器](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s5.png)\n![部署服务器](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s6.png)\n\n  这里我们选择纽约的节点(因为最便宜)，然后系统选择Ubuntu 18.04，服务器配置根据自己的需求来选择。选完以后，点击下方的“Deploy Now”。付完款以后，将会出现如下的界面。\n![成功部署](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s7.png)\n\n## Shadowsocks服务器端配置\n\n  成功部署好VPS后就要开始服务器端的配置了。首先我们进入到服务器的信息页面(如下图所示)，我们需要记住这里的IP地址以及ssh远程登录时的密码。\n![服务器信息](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s10.png)\n\n  拿到IP地址以后可以先打开终端ping一下以确保服务器是否可用。我这里以Ubuntu系统作为演示，通信一切正常。\n![ping](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s9.png)\n\n  下一步就要用ssh命令远程登录我们的服务器了。Mac用户和Linux用户直接在终端使用ssh命令即可，Windows用户呢就要稍微麻烦一点，得先在网上下载PuTTY，然后再远程登录。\n\n``` bash\n$ ssh root@45.77.108.240\n```\n\n  按下回车后会叫你输入密码，也就是前面服务器信息里的密码。登录成功后就会出现下面的界面。\n\n![登录成功](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s12.png)\n\n  成功登录后，为了确保兼容性，需要更新一下软件包。\n\n``` bash\n$ sudo apt update\n$ sudo apt upgrade -y\n```\n\n  安装python和pip\n\n``` bash\n$ sudo apt install python\n$ sudo apt install python-pip\n```\n\n  安装shadowsocks\n\n``` bash\n$ pip install shadowsocks\n```\n  \n  在/etc/目录下创建shadowsocks.json配置文件\n\n``` bash\n$ vim /etc/shadowsocks.json\n```\n  \n  进入到vim界面之后，按i进入插入模式，然后按照下面的模板将服务器的配置信息写好(只需将端口号和密码改成自己想要的即可)，最后按”Esc“+“:wq”保存并退出\n\n```{\n    \"server\":\"0.0.0.0\",\n    \"server_port\": \"YOUR_PORT\",\n    \"local_address\":\"127.0.0.1\",\n    \"local_port\":\"1080\",\n    \"password\":\"YOUR_PASSWD\",\n    \"timeout\":\"500\",\n    \"method\":\"aes-256-cfb\"\n}```\n\n  配置文件写好以后就要启动我们的server了，在这里我们需要从配置文件中读取相应参数，因此ssserver命令需要加入-c参数。\n\n``` bash\n$ ssserver -c /etc/shadowsocks.json\n```\n\n  不出意外的话，第一次启动应该会报出“AttributeError: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1: undefined symbol: EVP_CIPHER_CTX_cleanup”这样的错，这篇[博客](https://blog.csdn.net/St_Louis/article/details/103171781)给出了完美的解决方案，只需要修改一个文件中的两行数据就行，这里我就不一一演示了。\n\n  如果想要在后台运行，只需要在最后加入“-d start”即可。\n\n``` bash\n$ ssserver -c /etc/shadowsocks.json -d start\n```\n\n## Shadowsocks客户端配置\n\n  客户端比服务器端就要好配置多啦，三大主流操作系统都支持[图形化界面](https://github.com/shadowsocks/shadowsocks-gui)，Linux甚至支持还命令行的客户端。这里以Linux的GUI为例，只需要把服务器端的IP地址、端口号、密码、加密方式按照你服务器端配置的方式填写正确即可，剩下的本地地址、本地端口、协议类型就按照下面的方式填写就行了。\n\n![客户端配置](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s26.png)\n\n## 浏览器配置\n\n  虽然我们的服务器端和客户端都已经配置好了，但要在浏览器里实现外网的访问还需要几个简单的步骤。\n\n  首先需要在[chrome网上应用店](https://chrome.google.com/webstore/category/extensions?hl=zh-CN)下载一个叫做SwitchyOmega的插件。下载完成后在你的浏览器右上方会有一个小圆圈，点击后选择Option。\n![插件](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s15.png)\n\n  进入后选择左边的proxy，Protocol选择Socks5，Sever选择127.0.0.1，Port为1080。\n![Proxy](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s16.png)\n\n  保存退出后，就能访问Google啦～\n\n  但这样有一个问题，所有的流量都走了代理服务器，而平时我们访问一些国内的网站是不需要走代理的，并且用代理服务器来访问国内的网站会非常的慢。可不可以实现自动分流呢？哈哈，SwitchyOmega给我们了这样一个“贴心”的服务：auto switch\n\n  选择左边的auto switch，再选择“Add a rule list”\n\n![auto switch](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s17.png)\n\n  在\"Rule list rules\"选择\"proxy，Rule List Format\"选择\"AutoProxy\"\n\n![auto switch](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s21.png)\n\n  我们可以自定义规则，哪些网址走代理，哪些直接访问。但这样一个一个输入实在太麻烦，我们直接从GitHub上搜索gfwlist即可。\n\n  选择第一项\n![github gfwlist](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s18.png)\n\n  进入后点击gfwlist.txt，复制URL到SwitchyOmega中的Rule List URL，再点击Download Profile Now\n![auto switch](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s19.png)\n\n  更改应用后，你会发现下方的Rule List Text出现了各种网址，这说明已经添加成功了。\n![auto switch](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s23.png)\n\n  这样当我们把SwitchyOmega的模式选择成auto switch后就能实现智能分流啦。\n\n## bbr加速\n\n  最后，为了让我们的网速飞起来，看YouTube视频不会卡顿，需要借助一个叫做bbr加速的东西。操作非常简单，只需要几个步骤就好。这里需要再次登录我们的VPS，在VPS里进行操作。\n\n  修改系统变量并保存\n``` bash\n$ echo \"net.core.default_qdisc=fq\" >> /etc/sysctl.conf\n$ echo \"net.ipv4.tcp_congestion_control=bbr\" >> /etc/sysctl.conf\n$ sysctl -p\n\n```\n\n  查看是否开启\n``` bash\n$ sysctl net.ipv4.tcp_available_congestion_control\n```\n\n  如果显示如下内容表示已经开启\n``` bash\n$ net.ipv4.tcp_available_congestion_control = reno cubic bbr\n```\n\n  查看BBR是否启动\n``` bash\n$ lsmod | grep bbr\n```\n\n  如果显示以下内容表示已经开启\n``` bash\n$ tcp_bbr                20480  25\n```\n\n  开启了bbr加速以后网速快的飞起，以我家的100M带宽为例，看YouTube 4k视频一点都不卡顿，网速也是相当给力的。\n![YouTube](https://infinityglow.github.io/others/ss_v2ray/screenshot_ss/s25.png)\n\n"},{"title":"什么是算法","url":"/study/algorithm/introduction/","content":"\n## 概念\n\n什么是算法？英文维基百科给出了这样的定义：“An algorithm is a finite sequence of well-defined, computer-implementable instructions, typically to solve a class of problems or to perform a computation.”翻译过来的意思就是一系列有限的、清晰定义的、可实现的计算机指令，并用以解决一类问题或进行计算。\n\n是不是被这么官方的定义搞懵了？在生活中一个最简单的类比就是烹饪。如果把要烧一道菜比作这里要解决的问题，那么菜谱中每一道工序，每一个步骤就是这道菜的“算法”了。但这样的类比也存在一些问题：我们的算法要求每一步都是要明确定义，也就是说不能有任何歧义的。还是拿做菜来讲，如果菜谱上是这样定义的：\n\n- 将油倒入锅中加热至七成热\n- 把鸡腿放入油锅中炸至金黄\n- 放入少许盐\n\n![炸鸡腿](https://infinityglow.github.io/study/algorithm/introduction/images/chicken-leg.jpg)\n\n以上的步骤虽然前后有逻辑性，最终能够做出炸鸡腿，但是这样的定义是不符合算法的规范的，因为这样的定义只具备了**经验性**（**empirical**），不具备**系统性**（**systematical**），计算机是读不懂人类的这些经验的。所以，正确的姿势应该是这样的：\n\n- 将200ml的色拉油放入铁锅中，打开电炉，调至第9档，当油温达到200°C后停止加热\n- 把碗里的鸡腿依次放入油锅中煎炸，直至鸡腿表面呈现出 #FF9900 (金黄色RGB的十六进制表示法)\n- 根据鸡腿的重量（g）和一个映射函数 f(x) 计算出需要放置的盐的重量（g），将其放入锅中\n\n这样我们的计算机就好理解多啦～\n\n![happy](https://infinityglow.github.io/study/algorithm/introduction/images/happy.jpg)\n\n## 解决的问题\n\n了解完了算法的概念，那么算法是要用来干什么的呢？前面提到了是用来解决一类问题，这样的问题可以是[排序](https://infinityglow.github.io/tags/sorting/), [查找](https://infinityglow.github.io/tags/search/)，最优化问题等等。总之，这些问题大部分都是有确定解的，我们只需要用其中的一种算法找到它们即可。\n\n但是，仅仅找到了算法来解决这些问题是远远不够的。我们还需要考虑这个算法的效率如何，也就是对其进行[复杂度](https://infinityglow.github.io/study/algorithm/complexity-analysis/three-notations)分析。这里暂时不做讨论，先举一个经典的算法问题：扔鸡蛋问题。假设楼高为100层，给你2个鸡蛋，你需要试出来从第几层往下扔鸡蛋刚好会碎。你可能首先想到的是从第一层开始一层一层地扔直到鸡蛋摔碎，但是这样你可能最多扔99次（假设从100层往下扔鸡蛋才碎），而且你没有利用上第二个鸡蛋。一些聪明的同学可能又想到了用一个鸡蛋十层十层地往上扔，如果碎了就在两个十层之间一层一层地往上扔。这样最多只要扔19次，已经比之前的方法有了明显的进步，这就是算法要考虑的效率问题。事实上，这个问题的最优解是最多扔14次，需要后面要讲到的[动态规划](https://infinityglow.github.io/tag/dp/)。\n\n![扔鸡蛋](https://infinityglow.github.io/study/algorithm/introduction/images/egg.png)\n\n## 例子\n\n让我们看一道完整的例子，求两个数的最大公约数。相信在中学阶段老师就告诉过你们把两个数的因子提取出来，然后把所有共同的因子相乘得到结果。这里我们对这种方法不做讨论，让我们先看看一种高效的解法：**欧几里得算法**（**Euclid's algorithm**）。\n\n欧几里得算法又叫辗转取余法，其原理是两个数的最大公约数等于较小数与两数取余的最大公约数，用数学语言来描述的话就是 `gcd(m, n) = gcd(n, m mod n)`。但这只是其中的一步，什么时候才返回我们想要的结果呢？这就需要用算法语言来描述这个过程，这种语言可以是自然语言，也可以是数学语言（伪代码）。让我们先以自然语言为例吧。\n\n- 第一步：如果 n = 0，返回m；否则执行第二步。\n- 第二步：m 除以 n，将余数赋给变量r。\n- 第三步：把 n 赋给 m，r 赋给 n。执行第一步。\n\n如果用伪代码来表示的话就是这样的：\n\n![伪代码](https://infinityglow.github.io/study/algorithm/introduction/images/Euclid.png)\n\n最后，让我们用Python来实现这个函数吧。\n\n```\ndef gcd(m, n):\n    while n != 0:\n        r = m\n        m = n\n        n = r % n\n        gcd(m, n)\n    return m\n```\n\n[本节全部代码](https://github.com/infinityglow/Algorithm-and-Complexity/tree/master/Introduction)\n\n"}]